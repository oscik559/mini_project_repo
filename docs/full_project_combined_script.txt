# === command_processor.py ===

# modalities/command_processor.py


import atexit
import json
import logging
import os
from collections import defaultdict, deque
from typing import Dict, List, Tuple

import ollama
import psycopg2
from psycopg2 import Error as Psycopg2Error
from psycopg2 import sql
from psycopg2.extras import DictCursor

from config.app_config import setup_logging
from mini_project.database.connection import get_connection
from mini_project.modalities.prompt_utils import PromptBuilder

# === Logging Setup ===
debug_mode = os.getenv("DEBUG", "0") in ["1", "true", "True"]
log_level = os.getenv("LOG_LEVEL", "DEBUG" if debug_mode else "INFO").upper()
setup_logging(level=getattr(logging, log_level))
logger = logging.getLogger("CommandProcessor")

# models: "llama3.2:1b", "deepseek-r1:1.5b", "mistral:latest", "deepseek-r1:32b"
OLLAMA_MODEL = "mistral:latest"


class CommandProcessor:

    def __init__(self, llm_model: str = OLLAMA_MODEL):
        self.conn = get_connection()
        self.cursor = self.conn.cursor(cursor_factory=DictCursor)
        self.logger = logger
        self.llm_model = llm_model

        # Cache available sequences and objects from database for validation purposes
        self.available_sequences = self.get_available_sequences()
        self.available_objects = self.get_available_objects()

    def get_available_sequences(self) -> List[str]:
        """Fetch available sequence names from sequence_library in database"""
        self.cursor.execute("SELECT sequence_name FROM sequence_library")
        available_sequences = [row[0] for row in self.cursor.fetchall()]
        logger.info(f"Available sequences: {available_sequences}")
        return available_sequences

    def fetch_column(self, table: str, column: str) -> list:
        try:
            query = sql.SQL("SELECT {field} FROM {tbl}").format(
                field=sql.Identifier(column), tbl=sql.Identifier(table)
            )
            self.cursor.execute(query)
            return [row[0] for row in self.cursor.fetchall()]
        except Psycopg2Error as e:
            logger.error("Database error in fetch_column: %s", str(e), exc_info=True)
            raise

    def get_available_objects(self) -> List[str]:
        self.cursor.execute("SELECT object_name FROM camera_vision")
        available_objects = [row[0] for row in self.cursor.fetchall()]
        logger.info(f"Available objects: {available_objects}")
        return available_objects

    def get_unprocessed_unified_command(self) -> Dict:
        self.cursor.execute(
            """
            SELECT id, unified_command FROM unified_instructions
            WHERE processed = FALSE ORDER BY id DESC LIMIT 1
        """
        )
        result = self.cursor.fetchone()
        return (
            {"id": result["id"], "unified_command": result["unified_command"]}
            if result
            else None
        )

    def get_available_objects_with_colors(self) -> List[str]:
        self.cursor.execute("SELECT object_name, object_color FROM camera_vision")
        return [f"{name} ({color})" for name, color in self.cursor.fetchall()]

    def get_sort_order(self) -> List[str]:
        self.cursor.execute(
            "SELECT object_name, object_color FROM sort_order ORDER BY order_id ASC"
        )
        return [f"{name} ({color})" for name, color in self.cursor.fetchall()]

    def get_task_templates(self) -> Dict[str, List[str]]:
        self.cursor.execute("SELECT task_name, default_sequence FROM task_templates")
        return {row[0]: row[1] for row in self.cursor.fetchall()}

    def validate_operation(self, operation: Dict) -> bool:
        """Validate operation structure"""
        if operation["sequence_name"] not in self.available_sequences:
            logger.error(f"Invalid sequence name: {operation['sequence_name']}")
            return False

        # Allow empty object names
        if not operation.get("object_name", ""):
            return True

        if operation["object_name"] in self.available_objects:
            return True
        else:
            logger.error(f"Invalid object name: {operation['object_name']}")
            return False

    def extract_json_array(self, raw_response: str) -> List[Dict]:
        import re

        try:
            if not PromptBuilder.validate_llm_json(raw_response):
                raise ValueError(
                    "Invalid JSON format: response must start with [ and end with ]"
                )

            # Match the first complete JSON array only
            match = re.search(r"\[\s*{[\s\S]*?}\s*]", raw_response)
            if not match:
                raise ValueError("No valid JSON array found in LLM response.")

            json_str = match.group(0)
            return json.loads(json_str)

        except Exception as e:
            logger.error(
                "Failed to extract JSON array from LLM response: %s", e, exc_info=True
            )
            raise

    def infer_operation_name_from_llm(self, command_text: str) -> str:
        self.cursor.execute("SELECT operation_name FROM operation_library")
        available_operations = [row[0] for row in self.cursor.fetchall()]

        prompt = f"""
        Given the following user command:

        "{command_text}"

        Choose the most appropriate operation name from the list:
        {', '.join(available_operations)}

        Only respond with the exact operation_name string ‚Äî no extra words or explanation.
        """

        response = ollama.chat(
            model=self.llm_model,
            messages=[
                {
                    "role": "system",
                    "content": "You are a smart classifier for robotic task types.",
                },
                {"role": "user", "content": prompt},
            ],
        )
        result = response["message"]["content"].strip()

        if result not in available_operations:
            raise ValueError(f"LLM returned invalid operation_name: {result}")

        return result

    def get_task_order(self, operation_name: str) -> List[str]:
        self.cursor.execute(
            "SELECT task_order FROM operation_library WHERE operation_name = %s",
            (operation_name,),
        )
        row = self.cursor.fetchone()
        return [s.strip() for s in row[0].split(",")] if row else []

    def generate_operations_from_sort_order(
        self, task_order: List[str], command_id: int
    ) -> List[Dict]:
        self.cursor.execute("SELECT object_name FROM sort_order ORDER BY order_id")
        objects = [row[0] for row in self.cursor.fetchall()]

        self.cursor.execute(
            "SELECT COALESCE(MAX(operation_id), 0) + 1 FROM operation_sequence"
        )
        operation_id_start = self.cursor.fetchone()[0]

        ops = []
        idx = 0

        for obj in objects:
            for seq in task_order:
                self.cursor.execute(
                    "SELECT sequence_id FROM sequence_library WHERE sequence_name = %s",
                    (seq,),
                )
                seq_id_row = self.cursor.fetchone()
                if not seq_id_row:
                    continue

                ops.append(
                    {
                        "operation_id": operation_id_start + idx,
                        "sequence_id": seq_id_row[0],
                        "sequence_name": seq,
                        "object_name": obj,
                        "command_id": command_id,
                    }
                )
                idx += 1

        # Optionally add a final "go_home"
        self.cursor.execute(
            "SELECT sequence_id FROM sequence_library WHERE sequence_name = 'go_home'"
        )
        go_home_seq = self.cursor.fetchone()
        if go_home_seq:
            ops.append(
                {
                    "operation_id": operation_id_start + idx,
                    "sequence_id": go_home_seq[0],
                    "sequence_name": "go_home",
                    "object_name": "",
                    "command_id": command_id,
                }
            )

        return ops

    def process_command(self, unified_command: Dict) -> Tuple[bool, List[Dict]]:
        try:
            self.populate_sort_order_from_llm(unified_command["unified_command"])

            operation_name = self.infer_operation_name_from_llm(
                unified_command["unified_command"]
            )
            task_order = self.get_task_order(operation_name)

            operations = self.generate_operations_from_sort_order(
                task_order, unified_command["id"]
            )

            # Wipe old unprocessed
            self.cursor.execute(
                "UPDATE operation_sequence SET processed = TRUE WHERE processed = FALSE"
            )

            # Insert new
            insert_count = 0
            for op in operations:
                self.cursor.execute(
                    """
                    INSERT INTO operation_sequence
                    (operation_id, sequence_id, sequence_name, object_name, command_id)
                    VALUES (%s, %s, %s, %s, %s)
                    """,
                    (
                        op["operation_id"],
                        op["sequence_id"],
                        op["sequence_name"],
                        op["object_name"],
                        op["command_id"],
                    ),
                )
                insert_count += 1
            logger.info(f"‚úÖ Inserted {insert_count} rows into operation_sequence.")

            # Mark as processed
            self.cursor.execute(
                "UPDATE unified_instructions SET processed = TRUE WHERE id = %s",
                (unified_command["id"],),
            )

            # Auto-populate operation parameter tables
            self.populate_operation_parameters()

            self.conn.commit()
            return True, operations

        except Exception as e:
            logger.error("Failed to process command: %s", e, exc_info=True)
            self.conn.rollback()
            return False, []

    def populate_operation_parameters(self):
        logger.info("Populating operation-specific parameters...")

        # Step 1: Get all unique sequence types planned
        self.cursor.execute("SELECT DISTINCT sequence_name FROM operation_sequence")
        sequence_types = [row[0] for row in self.cursor.fetchall()]

        insert_count = 0

        if "pick" in sequence_types:
            self.cursor.execute("DELETE FROM pick_op_parameters")
            self.cursor.execute(
                "SELECT object_name FROM operation_sequence WHERE sequence_name = 'pick'"
            )
            pick_data = self.cursor.fetchall()
            for i, (obj,) in enumerate(pick_data):
                self.cursor.execute(
                    """
                    INSERT INTO pick_op_parameters (
                        operation_order, object_id, slide_state_status, slide_direction, distance_travel, operation_status
                    ) VALUES (%s, %s, %s, %s, %s, %s)
                    """,
                    (i + 1, obj, False, "y", 0.01, False),
                )
                insert_count += 1
            logger.info(f"‚úÖ Inserted {insert_count} rows into pick_op_parameters.")

        if "travel" in sequence_types:
            self.cursor.execute("DELETE FROM travel_op_parameters")
            self.cursor.execute(
                "SELECT object_name FROM operation_sequence WHERE sequence_name = 'travel'"
            )
            travel_data = self.cursor.fetchall()
            for i, (obj,) in enumerate(travel_data):
                self.cursor.execute(
                    """
                    INSERT INTO travel_op_parameters (
                        operation_order, object_id, travel_height, gripper_rotation, operation_status
                    ) VALUES (%s, %s, %s, %s, %s)
                    """,
                    (i + 1, obj, 0.085, "y-axis", False),
                )
                insert_count += 1
            logger.info(f"‚úÖ Inserted {insert_count} rows into travel_op_parameters.")

        if "drop" in sequence_types:
            self.cursor.execute("DELETE FROM drop_op_parameters")
            self.cursor.execute(
                "SELECT object_name FROM operation_sequence WHERE sequence_name = 'drop'"
            )
            drop_data = self.cursor.fetchall()
            for i, (obj,) in enumerate(drop_data):
                self.cursor.execute(
                    """
                    INSERT INTO drop_op_parameters (
                        operation_order, object_id, drop_height, operation_status
                    ) VALUES (%s, %s, %s, %s)
                    """,
                    (i + 1, obj, 0.0, False),
                )
                insert_count += 1
            logger.info(f"‚úÖ Inserted {insert_count} rows into drop_op_parameters.")

        if "screw" in sequence_types:
            self.cursor.execute("DELETE FROM screw_op_parameters")
            self.cursor.execute(
                "SELECT sequence_id, object_name FROM operation_sequence WHERE sequence_name = 'screw'"
            )
            screw_data = self.cursor.fetchall()
            for i, (seq_id, obj) in enumerate(screw_data):
                self.cursor.execute(
                    """
                    INSERT INTO screw_op_parameters (
                        operation_order, sequence_id, object_id,
                        rotation_dir, number_of_rotations,
                        current_rotation, operation_status
                    ) VALUES (%s, %s, %s, %s, %s, %s, %s)
                    """,
                    (i + 1, seq_id, obj, i % 2 == 0, 3, 0, False),
                )
                insert_count += 1
            logger.info(f"Inserted {insert_count} rows into screw_op_parameters.")

        if "screw" in sequence_types:
            self.cursor.execute("DELETE FROM rotate_state_parameters")
            self.cursor.execute(
                "SELECT sequence_id, operation_order, object_id FROM screw_op_parameters"
            )
            rotate_data = self.cursor.fetchall()
            for seq_id, op_order, obj in rotate_data:
                self.cursor.execute(
                    """
                    INSERT INTO rotate_state_parameters (
                        sequence_id, operation_order, object_id,
                        rotation_angle, operation_status
                    ) VALUES (%s, %s, %s, %s, %s)
                    """,
                    (seq_id, op_order, obj, 90, False),
                )
                insert_count += 1
            logger.info(f"Inserted {insert_count} rows into rotate_state_parameters.")

        self.conn.commit()
        logger.info("‚úÖ Operation-specific parameter tables updated.")

    def extract_sort_order_from_llm(self, command_text: str) -> List[Tuple[str, str]]:
        try:
            logger.info("üß† Extracting sort order using LLM...")
            prompt = PromptBuilder.sort_order_prompt(command_text)
            response = ollama.chat(
                model=self.llm_model,
                messages=[
                    PromptBuilder.sort_order_system_msg(),
                    {"role": "user", "content": prompt},
                ],
            )

            parsed = self.extract_json_array(response["message"]["content"])
            if not isinstance(parsed, list):
                logger.warning("‚ö†Ô∏è Unexpected format from LLM sort extraction.")
                return []

            results = []
            for item in parsed:
                object_name = (item.get("object_name") or "").strip()
                object_color = (item.get("object_color") or "").strip()
                results.append((object_name, object_color))

            logger.info(f"‚úÖ Extracted sort order: {results}")
            return results

        except Exception as e:
            logger.error(
                "‚ùå Failed to extract sort order using LLM: %s", str(e), exc_info=True
            )
            return []

    def populate_sort_order_from_llm(self, command_text: str) -> None:
        try:
            logger.info("üß† Asking LLM to extract sort order...")

            extracted = self.extract_sort_order_from_llm(command_text)

            if not extracted:
                logger.warning("‚ö†Ô∏è No sort order to insert.")
                return

            # Clear previous sort_order
            self.cursor.execute("DELETE FROM sort_order")

            # Fetch color-to-object_name mapping from camera_vision
            self.cursor.execute("SELECT object_name, object_color FROM camera_vision")
            camera_color_map = self.cursor.fetchall()
            color_to_names = {}
            for name, color in camera_color_map:
                color_to_names.setdefault(color.lower(), []).append(name)

            inserted = []
            for item in extracted:
                color = item[1].lower()
                name_list = color_to_names.get(color, [])
                if not name_list:
                    logger.warning(f"‚ö†Ô∏è No match in camera_vision for color: {color}")
                    continue
                obj_name = name_list.pop(0)  # Use and remove to avoid duplicates
                self.cursor.execute(
                    "INSERT INTO sort_order (object_name, object_color) VALUES (%s, %s)",
                    (obj_name, color),
                )
                inserted.append((obj_name, color))

            logger.info(f"‚úÖ sort_order table populated: {inserted}")

        except Exception as e:
            logger.error(
                "‚ùå Failed to populate sort_order table: %s", str(e), exc_info=True
            )

    def run_processing_cycle(self):
        """
        Process the latest unprocessed unified command
        """
        logger.info("Checking for new unified_commands...")
        unified_command = self.get_unprocessed_unified_command()

        if unified_command:
            logger.info(f"Processing command ID: {unified_command['id']}")
            if self.process_command(unified_command):
                logger.info(
                    f"Successfully processed unified_command {unified_command['id']}"
                )
            else:
                logger.error(
                    f"Failed to process unified_command {unified_command['id']}"
                )
        else:
            logger.info("No unprocessed unified_commands found")

    def close(self):
        """Close the persistent SQLite connection."""
        if self.conn:
            self.conn.close()
            self.conn = None
            logger.info("Database connection closed.")


if __name__ == "__main__":
    processor = CommandProcessor(llm_model="mistral:latest")
    # Register the close method so it gets called when the program exits
    atexit.register(processor.close)
    processor.run_processing_cycle()



# === gesture_processor.py ===

# modalities/gesture_processor.py


import logging
import sqlite3
import threading
import time
import uuid
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple

import cv2
import mediapipe as mp

from config.app_config import *

# Initialize logging with desired level (optional)
setup_logging(level=logging.INFO)
logger = logging.getLogger("GestureProcessor")


class GestureDetector:
    def __init__(
        self,
        db_path: str = DB_PATH,
        min_detection_confidence: float = MIN_DETECTION_CONFIDENCE,
        min_tracking_confidence: float = MIN_TRACKING_CONFIDENCE,
        max_num_hands: int = MAX_NUM_HANDS,
        frame_skip: int = FRAME_SKIP,
        session_id: Optional[str] = None,
    ):
        self.mp_hands = mp.solutions.hands
        self.hands = self.mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=max_num_hands,
            min_detection_confidence=min_detection_confidence,
            min_tracking_confidence=min_tracking_confidence,
        )
        self.mp_drawing = mp.solutions.drawing_utils
        self.mp_drawing_styles = mp.solutions.drawing_styles

        self.conn = sqlite3.connect(db_path, check_same_thread=False)
        self._init_db()
        self.cursor = self.conn.cursor()

        self.session_id = session_id or str(uuid.uuid4())
        self.frame_skip = frame_skip
        self.frame_counter = 0

        loaded_gestures = self.load_gesture_definitions()
        if loaded_gestures:
            self.gesture_map = loaded_gestures
        # else:
        #     self.gesture_map = {
        #         "thumbs_up": {"func": self._is_thumbs_up, "text": "Approval"},
        #         "open_hand": {"func": self._is_open_hand, "text": "Stop"},
        #         "pointing": {"func": self._is_pointing, "text": "Select Object"},
        #         "closed_fist": {"func": self._is_closed_fist, "text": "Grab"},
        #         "victory": {"func": self._is_victory, "text": "Confirm"},
        #         "ok_sign": {"func": self._is_ok_sign, "text": "OK"},
        #     }

        self.last_gesture: Optional[str] = None
        self.last_log_time: float = 0
        self.min_log_interval: float = 2.0
        self.last_detection: Optional[List[Dict[str, Any]]] = None

    def _init_db(self):
        self.conn.execute(
            """
            CREATE TABLE IF NOT EXISTS gesture_instructions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                session_id TEXT NOT NULL,
                timestamp DATETIME DEFAULT (datetime('now','localtime')),
                gesture_text TEXT NOT NULL,
                natural_description TEXT,
                confidence REAL,
                hand_label TEXT
            )
            """
        )
        self.conn.commit()
        logger.info("Gesture database initialized with gesture_instructions table.")

    def _log_gesture(
        self,
        gesture_type: str,
        gesture_text: str,
        natural_description: str,
        confidence: float,
        hand_label: str,
    ):
        timestamp = datetime.now().isoformat()
        try:
            with self.conn:
                self.conn.execute(
                    """
                    INSERT INTO gesture_instructions (session_id, timestamp, gesture_text, natural_description, confidence, hand_label)
                    VALUES (?, ?, ?, ?, ?, ?)
                    """,
                    (
                        self.session_id,
                        timestamp,
                        gesture_text,
                        natural_description,
                        confidence,
                        hand_label,
                    ),
                )
            logger.info(
                f"Gesture: [{gesture_text}], Hand: [{hand_label}], Confidence: [{confidence:.2f}], Description: [{natural_description}]"
            )
        except sqlite3.Error as e:
            logger.error(f"Database error: {e}")

    def _get_landmark_coords(
        self, landmarks, landmark_id: int
    ) -> Tuple[float, float, float]:
        landmark = landmarks.landmark[landmark_id]
        return (landmark.x, landmark.y, landmark.z)

    def _euclidean_distance(
        self, a: Tuple[float, float, float], b: Tuple[float, float, float]
    ) -> float:
        return ((a[0] - b[0]) ** 2 + (a[1] - b[1]) ** 2 + (a[2] - b[2]) ** 2) ** 0.5

    def _is_thumbs_up(self, landmarks) -> bool:
        thumb_tip = self._get_landmark_coords(
            landmarks, self.mp_hands.HandLandmark.THUMB_TIP
        )
        index_pip = self._get_landmark_coords(
            landmarks, self.mp_hands.HandLandmark.INDEX_FINGER_PIP
        )
        return thumb_tip[1] < index_pip[1]

    def _is_open_hand(self, landmarks) -> bool:
        fingertips = [
            self._get_landmark_coords(landmarks, tip)[1]
            for tip in [
                self.mp_hands.HandLandmark.THUMB_TIP,
                self.mp_hands.HandLandmark.INDEX_FINGER_TIP,
                self.mp_hands.HandLandmark.MIDDLE_FINGER_TIP,
                self.mp_hands.HandLandmark.RING_FINGER_TIP,
                self.mp_hands.HandLandmark.PINKY_TIP,
            ]
        ]
        wrist_y = landmarks.landmark[self.mp_hands.HandLandmark.WRIST].y
        return all(y < wrist_y for y in fingertips)

    def _is_pointing(self, landmarks) -> bool:
        index_tip = self._get_landmark_coords(
            landmarks, self.mp_hands.HandLandmark.INDEX_FINGER_TIP
        )
        middle_tip = self._get_landmark_coords(
            landmarks, self.mp_hands.HandLandmark.MIDDLE_FINGER_TIP
        )
        return index_tip[1] < middle_tip[1]

    def _is_closed_fist(self, landmarks) -> bool:
        fingertips = [
            self._get_landmark_coords(landmarks, tip)[1]
            for tip in [
                self.mp_hands.HandLandmark.THUMB_TIP,
                self.mp_hands.HandLandmark.INDEX_FINGER_TIP,
                self.mp_hands.HandLandmark.MIDDLE_FINGER_TIP,
            ]
        ]
        mcp_joints = [
            self._get_landmark_coords(landmarks, joint)[1]
            for joint in [
                self.mp_hands.HandLandmark.THUMB_MCP,
                self.mp_hands.HandLandmark.INDEX_FINGER_MCP,
                self.mp_hands.HandLandmark.MIDDLE_FINGER_MCP,
            ]
        ]
        return all(tip > mcp for tip, mcp in zip(fingertips, mcp_joints))

    def _is_victory(self, landmarks) -> bool:
        index_tip = self._get_landmark_coords(
            landmarks, self.mp_hands.HandLandmark.INDEX_FINGER_TIP
        )
        middle_tip = self._get_landmark_coords(
            landmarks, self.mp_hands.HandLandmark.MIDDLE_FINGER_TIP
        )
        ring_tip = self._get_landmark_coords(
            landmarks, self.mp_hands.HandLandmark.RING_FINGER_TIP
        )
        wrist_y = self._get_landmark_coords(
            landmarks, self.mp_hands.HandLandmark.WRIST
        )[1]
        return (
            index_tip[1] < ring_tip[1]
            and middle_tip[1] < ring_tip[1]
            and ring_tip[1] > wrist_y
        )

    def _is_ok_sign(self, landmarks) -> bool:
        thumb_tip = self._get_landmark_coords(
            landmarks, self.mp_hands.HandLandmark.THUMB_TIP
        )
        index_tip = self._get_landmark_coords(
            landmarks, self.mp_hands.HandLandmark.INDEX_FINGER_TIP
        )
        # Define a threshold for the OK sign; adjust if needed.
        threshold = 0.05
        distance = self._euclidean_distance(thumb_tip, index_tip)
        return distance < threshold

    def _analyze_thumb(self, landmarks) -> str:
        thumb_tip = self._get_landmark_coords(
            landmarks, self.mp_hands.HandLandmark.THUMB_TIP
        )
        thumb_ip = self._get_landmark_coords(
            landmarks, self.mp_hands.HandLandmark.THUMB_IP
        )
        return "up" if thumb_tip[1] < thumb_ip[1] else "down"

    def _count_open_fingers(self, landmarks) -> int:
        count = 0
        if (
            landmarks.landmark[self.mp_hands.HandLandmark.INDEX_FINGER_TIP].y
            < landmarks.landmark[self.mp_hands.HandLandmark.INDEX_FINGER_MCP].y
        ):
            count += 1
        if (
            landmarks.landmark[self.mp_hands.HandLandmark.MIDDLE_FINGER_TIP].y
            < landmarks.landmark[self.mp_hands.HandLandmark.MIDDLE_FINGER_MCP].y
        ):
            count += 1
        if (
            landmarks.landmark[self.mp_hands.HandLandmark.RING_FINGER_TIP].y
            < landmarks.landmark[self.mp_hands.HandLandmark.RING_FINGER_MCP].y
        ):
            count += 1
        if (
            landmarks.landmark[self.mp_hands.HandLandmark.PINKY_TIP].y
            < landmarks.landmark[self.mp_hands.HandLandmark.PINKY_MCP].y
        ):
            count += 1
        return count

    def convert_features_to_description(self, gesture_type: str, hand_landmarks) -> str:
        thumb_state = self._analyze_thumb(hand_landmarks)
        open_fingers = self._count_open_fingers(hand_landmarks)
        if gesture_type == "thumbs_up":
            base_desc = "The thumb is raised above the index finger, indicating a thumbs-up or approval gesture."
        elif gesture_type == "open_hand":
            base_desc = "All fingers are extended, showing an open hand posture which may signal a stop command."
        elif gesture_type == "pointing":
            base_desc = "The index finger is extended while the other fingers remain curled, suggesting the user is pointing."
        elif gesture_type == "closed_fist":
            base_desc = "The hand is clenched into a fist, a posture often associated with grabbing or assertiveness."
        elif gesture_type == "victory":
            base_desc = "The hand forms a V-shape with the index and middle fingers extended, commonly used to signal victory or confirmation."
        elif gesture_type == "ok_sign":
            base_desc = "The thumb and index finger are touching to form a circle, commonly known as the OK sign."
        else:
            base_desc = "A neutral gesture with no distinct features."
        return f"{base_desc} Additionally, the thumb is {thumb_state} and {open_fingers} fingers are open."

    def detect_gesture(self, frame) -> Optional[List[Dict[str, Any]]]:
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.hands.process(rgb_frame)
        if not results.multi_hand_landmarks or not results.multi_handedness:
            return None

        detections = []
        for hand_landmarks, handedness_info in zip(
            results.multi_hand_landmarks, results.multi_handedness
        ):
            hand_label = handedness_info.classification[0].label
            hand_confidence = handedness_info.classification[0].score
            for gesture, config in self.gesture_map.items():
                if config["func"](hand_landmarks):
                    description = self.convert_features_to_description(
                        gesture, hand_landmarks
                    )
                    detections.append(
                        {
                            "modality": "gesture",
                            "gesture": gesture,
                            "gesture_text": config["text"],
                            "confidence": hand_confidence,
                            "hand_label": hand_label,
                            "description": description,
                            "landmarks": hand_landmarks,
                        }
                    )
                    break
        return detections

    def _process_frame(self, frame):
        """
        Process a single frame: flip, resize, update detection, overlay gesture info, and draw landmarks.
        """
        # Flip and resize for a consistent display.
        frame = cv2.flip(frame, 1)
        frame = cv2.resize(frame, (640, 480))
        self.frame_counter += 1

        # Update detection on every frame_skip-th frame.
        if self.frame_counter % self.frame_skip == 0:
            detection = self.detect_gesture(frame)
            self.last_detection = detection if detection is not None else None
            current_time = time.time()
            if self.last_detection:
                for idx, d in enumerate(self.last_detection):
                    if d["gesture"] == self.last_gesture and (
                        current_time - self.last_log_time < self.min_log_interval
                    ):
                        continue
                    self._log_gesture(
                        d["gesture"],
                        d["gesture_text"],
                        d["description"],
                        d["confidence"],
                        d["hand_label"],
                    )
                    self.last_gesture = d["gesture"]
                    self.last_log_time = current_time

        # Overlay detection info if available.
        if self.last_detection:
            for idx, d in enumerate(self.last_detection):
                cv2.putText(
                    frame,
                    f"{d['gesture_text']} [{d['hand_label']}]",
                    (10, 30 + idx * 60),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    1,
                    (0, 255, 0),
                    2,
                )
                cv2.putText(
                    frame,
                    d["description"],
                    (10, 70 + idx * 60),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.6,
                    (255, 255, 0),
                    2,
                )
                if d["gesture"] == "thumbs_up":
                    cv2.putText(
                        frame,
                        "Thumb Highlight",
                        (10, 110 + idx * 60),
                        cv2.FONT_HERSHEY_SIMPLEX,
                        0.6,
                        (0, 0, 255),
                        2,
                    )

        # Draw landmarks on the frame.
        if self.last_detection:
            for d in self.last_detection:
                self.mp_drawing.draw_landmarks(
                    frame,
                    d["landmarks"],
                    self.mp_hands.HAND_CONNECTIONS,
                    self.mp_drawing_styles.get_default_hand_landmarks_style(),
                    self.mp_drawing_styles.get_default_hand_connections_style(),
                )
        else:
            rgb_for_drawing = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = self.hands.process(rgb_for_drawing)
            if results.multi_hand_landmarks:
                for hand_landmarks in results.multi_hand_landmarks:
                    self.mp_drawing.draw_landmarks(
                        frame,
                        hand_landmarks,
                        self.mp_hands.HAND_CONNECTIONS,
                        self.mp_drawing_styles.get_default_hand_landmarks_style(),
                        self.mp_drawing_styles.get_default_hand_connections_style(),
                    )
        return frame

    def load_gesture_definitions(self) -> Dict[str, Dict[str, Any]]:
        try:
            query = "SELECT gesture_type, gesture_text, natural_description, config FROM gesture_library"
            self.cursor.execute(query)
            definitions = {}
            for row in self.cursor.fetchall():
                gesture_type, gesture_text, natural_description, config = row
                definitions[gesture_type] = {
                    "text": gesture_text,
                    "description": natural_description,
                    # You can parse the JSON config if needed:
                    "config": config,
                    # Map to your detection function via gesture_map_functions:
                    "func": self.gesture_map_functions().get(gesture_type),
                }
            return definitions
        except sqlite3.Error as e:
            logger.error(f"Error loading gesture definitions: {e}")
            return {}

    def gesture_map_functions(self) -> Dict[str, Any]:
        """Returns a mapping of gesture types to detection functions."""
        return {
            "thumbs_up": self._is_thumbs_up,
            "open_hand": self._is_open_hand,
            "pointing": self._is_pointing,
            "closed_fist": self._is_closed_fist,
            "victory": self._is_victory,
            "ok_sign": self._is_ok_sign,
        }

    def process_video_stream(self, termination_event: Optional[threading.Event] = None):
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            logger.error("Error: Could not open video stream.")
            return

        def video_loop():
            while cap.isOpened() and not (
                termination_event and termination_event.is_set()
            ):
                ret, frame = cap.read()
                if not ret:
                    logger.error("Failed to capture frame from camera.")
                    break
                processed_frame = self._process_frame(frame)
                cv2.imshow("Gesture Detection", processed_frame)
                if cv2.waitKey(1) & 0xFF == ord("q"):
                    break
            cap.release()
            cv2.destroyAllWindows()
            self.conn.close()
            logger.info("Video stream ended and database connection closed.")

        video_thread = threading.Thread(target=video_loop)
        video_thread.start()
        video_thread.join()


if __name__ == "__main__":
    gd = GestureDetector()
    gd.process_video_stream()



# === llm_main_langchain.py ===

import json
import logging
import os
import random
import re
import string
import struct
import threading
import time
import uuid
import warnings
from datetime import datetime
from pathlib import Path
from typing import Literal

import ollama
import pvporcupine
import pyttsx3
import requests
import sounddevice as sd
from gtts import gTTS
from langchain.chains import LLMChain
from langchain.memory import ConversationBufferMemory
from langchain.schema import messages_from_dict, messages_to_dict
from langchain_core._api.deprecation import LangChainDeprecationWarning
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda, RunnableSequence
from langchain_ollama import ChatOllama

from config.app_config import BASE_DIR, setup_logging
from mini_project.database.connection import get_connection
from mini_project.modalities.FOR_SHAPES.command_processor import CommandProcessor
from mini_project.modalities.FOR_SHAPES.voice_processor import (
    SpeechSynthesizer,
    VoiceProcessor,
)
from mini_project.modalities.prompt_utils import PromptBuilder

# ========== Wake Word Setup ==========
ACCESS_KEY = "E0O2AD01eT6cJ83n1yYf5bekfdIOEGUky9q6APkwdx9enDaMLZQtLw=="
WAKEWORD = (
    r"C:\Users\oscik559\Projects\mini_project_repo\assets\robot_wakewords\hey_yummy.ppn"
)
WAKE_RESPONSES = [
    "yes?",
    "I'm listening",
    "what's up?",
    "go ahead.",
    "at your service.",
    "hello?",
    "I'm here!",
    "you called?",
    "what do you want?",
    "I'm listening.",
    "hi?",
    "what is it?",
]
# Use built-in or custom model path
porcupine = pvporcupine.create(
    access_key=ACCESS_KEY,
    keywords=[
        "computer",  # Built-in wake word
    ],
    keyword_paths=[WAKEWORD],
)
wake_word_triggered = threading.Event()

import random

# =====================================


# CHAT_MEMORY_PATH = Path("chat_memory.json")
CHAT_MEMORY_PATH = BASE_DIR / "assets" / "chat_memory" / "chat_memory.json"

warnings.filterwarnings("ignore", category=LangChainDeprecationWarning)


# === Logging Config ===
logging.getLogger("comtypes").setLevel(logging.WARNING)
logging.getLogger("faster_whisper").setLevel(logging.WARNING)
logger = logging.getLogger("VoiceAssistant")

# === Configuration ===
OLLAMA_MODEL = "llama3.2:latest"
voice_speed = 180  # 165

TASK_VERBS = {
    "sort",
    "move",
    "place",
    "assemble",
    "pick",
    "drop",
    "grab",
    "stack",
    "push",
    "pull",
}
QUESTION_WORDS = {
    "what",
    "where",
    "which",
    "who",
    "how many",
    "is there",
    "are there",
}
CONFIRM_WORDS = {
    "yes",
    "sure",
    "okay",
    "go ahead",
    "absolutely",
    "yep",
    "definitely",
    "please do",
}
CANCEL_WORDS = {"no", "cancel", "not now", "stop", "never mind", "don't"}
# ==========================ADD MORE

SCENE_PROMPT_TEMPLATE = """
You are an intelligent robotic assistant, with the camera as your eye. Based on the objects in the scene, listed in a camera_vision database table, respond concisely and clearly to the user question. One line answers are acceptable.
if there are any, the objects here are sitting on a table. Do not assume objects unless they are listed.
---
Each object has the following fields:
# - object_name: the name of the object in the scene.
# - object_color: the color of the object in the scene
# - pos_x, pos_y, pos_z: the 3D position of the object in the scene relative to table (0,0). You can use the object position to imagine the relative distances of the objects from each other
# - rot_x, rot_y, rot_z: the orientation of the object in the scene

---
if the object is a slide, it will have a usd_name of slide.usd, and the holder object will have a usd_name of holder.usd
any objects with object_name that does not start with \"slide...\" are not slides
---

Avoid technical terms like rot_x or pos_y. Instead, describe in natural language (e.g., "position x", "rotation y").
Assume the pos_x, pos_y, pos_z are coordinates of the objects on the table with respect to a 0,0,0 3D cordinate which is thereference ( the far right edge of the table top rectangle). the values are tenth of an mm unit.
---
Previous conversation:
{chat_history}

User question: {question}
Objects in scene:
{data}
---
Answer:
"""


# === LangChain Setup ===
llm = ChatOllama(model=OLLAMA_MODEL)
memory = ConversationBufferMemory(memory_key="chat_history", input_key="question")
prompt = PromptTemplate.from_template(SCENE_PROMPT_TEMPLATE)


# Helper to load chat history into the prompt input
def load_memory(inputs: dict) -> dict:
    chat_history = memory.load_memory_variables({})["chat_history"]
    return {**inputs, "chat_history": chat_history}


# Save original input for memory context
def build_chain_with_input(input_data):
    return (
        RunnableLambda(load_memory)
        | prompt
        | llm
        | RunnableLambda(lambda output: save_memory_with_input(input_data, output))
    )


# Save memory context
def save_memory_with_input(original_input, output):
    memory.save_context(
        {"question": original_input["question"]},
        {"answer": output.content},
    )

    return output


chain = (
    {
        # "input": RunnableLambda(load_memory),
        "original_input": lambda x: x,  # pass-through original input
    }
    | RunnableLambda(lambda d: d["input"])  # continue with prompt | llm
    | prompt
    | llm
    | RunnableLambda(
        lambda output, inputs: {"input": inputs["original_input"], "output": output}
    )
    | RunnableLambda(save_memory_with_input)
)


# === Command Classification ===
def classify_command(command_text: str, llm) -> Literal["scene", "task"]:
    lowered = command_text.lower().strip()

    # # === Try LLM-based classification ===
    try:
        classification_prompt = """
        Classify the following user command as either:
        - 'scene' if it is a question about the current camera scene (e.g., object positions, colors, etc.)
        - 'task' if it is a command to take action (e.g., sort, move, place, etc.)
        Just return one word: either 'scene' or 'task'.

        Command: {command}
        Answer:
        """
        classification_chain = LLMChain(
            llm=llm, prompt=PromptTemplate.from_template(classification_prompt)
        )
        result = classification_chain.invoke({"command": command_text})
        classification = result.get("text", "").strip().lower()
        if classification in {"scene", "task"}:
            return classification
    except Exception as e:
        print(f"[‚ö†Ô∏è] LLM failed, using rule-based fallback: {e}")

    # === Rule-based fallback ===
    if any(lowered.startswith(q) for q in QUESTION_WORDS):
        return "scene"

    if re.search(r"\b(is|are|how many|what|which|where|who)\b", lowered):
        return "scene"

    if any(verb in lowered for verb in TASK_VERBS):
        return "task"

    return "task"


# === Persistent Chat Memory ===
def save_chat_history():
    try:
        serialized = messages_to_dict(memory.chat_memory.messages)
        with open(CHAT_MEMORY_PATH, "w", encoding="utf-8") as f:
            json.dump(serialized, f, indent=2)
        logger.info("üíæ Chat history saved.")
    except Exception as e:
        logger.error(f"Failed to save chat memory: {e}")


def load_chat_history():
    if CHAT_MEMORY_PATH.exists():
        try:
            with open(CHAT_MEMORY_PATH, "r", encoding="utf-8") as f:
                raw_messages = json.load(f)
            memory.chat_memory.messages = messages_from_dict(raw_messages)
            logger.info("üß† Loaded past chat history.")
        except Exception as e:
            logger.warning(f"Could not load chat memory: {e}")


# === Scene Description ===
def fetch_camera_objects() -> list:
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute(
        """
        SELECT object_name, object_color, pos_x, pos_y, pos_z, rot_x, rot_y, rot_z, last_detected, usd_name
        FROM camera_vision
        """
    )
    rows = cursor.fetchall()
    cursor.close()
    conn.close()
    return rows


def format_camera_data(objects: list) -> str:
    return "\n".join(
        f"- {name} ({color}) at ({x:.1f}, {y:.1f}, {z:.1f}) oriented at ({r:.1f}, {p:.1f}, {w:.1f}) last seen at {timestamp}, usd_name: {usd}"
        for name, color, x, y, z, r, p, w, timestamp, usd in objects
    )


# === Scene Query ===
def query_scene(question: str) -> str:
    try:
        objects = fetch_camera_objects()
        if not objects:
            return "I can only see the camera. No other objects are currently visible."
        formatted_data = format_camera_data(objects)
        input_data = {"question": question, "data": formatted_data}
        response = build_chain_with_input(input_data).invoke(input_data)

        return response.content
    except Exception as e:
        logger.error("Scene query failed", exc_info=True)
        return "[Scene query failed.]"


# === Task Processing ===
def process_task(command_text: str) -> str:
    try:
        conn = get_connection()
        cursor = conn.cursor()
        cursor.execute(
            """
            INSERT INTO unified_instructions (
                session_id, timestamp, liu_id,
                voice_command, gesture_command, unified_command,
                confidence, processed
            )
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
            RETURNING id;
            """,
            (
                "session_voice_001",
                datetime.now(),
                "oscik559",
                command_text,
                "",
                command_text,
                0.95,
                False,
            ),
        )
        command_id = cursor.fetchone()[0]
        conn.commit()
        cursor.execute("DELETE FROM operation_sequence")
        conn.commit()
        cursor.close()
        conn.close()

        processor = CommandProcessor()
        success, _ = processor.process_command(
            {"id": command_id, "unified_command": command_text}
        )
        processor.close()

        return (
            "The task has been planned and added successfully."
            if success
            else "Sorry, I couldn't understand the task."
        )
    except Exception as e:
        logging.error(f"Task processing failed: {e}")
        return "[Task execution failed.]"


# === Simple LLM Greeting ===
def get_weather_description(latitude=58.41, longitude=15.62) -> str:
    try:
        url = f"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current_weather=true"
        response = requests.get(url, timeout=5)
        response.raise_for_status()
        data = response.json()
        weather = data["current_weather"]
        temperature = round(weather["temperature"])
        description = f"{temperature}¬∞C, wind {weather['windspeed']} km/h"
        return description
    except Exception as e:
        print(f"Weather fetch failed: {e}")
        return "mysterious skies"


# ========== Wake Word Listener ==========
def listen_for_wake_word(vp, tts):
    def callback(indata, frames, time, status):
        try:
            pcm = struct.unpack_from("h" * porcupine.frame_length, bytes(indata))
            keyword_index = porcupine.process(pcm)
            if keyword_index >= 0:
                logger.info("üü¢ Wake word detected!")
                wake_word_triggered.set()  # ‚úÖ Set flag
        except Exception as e:
            logger.warning(f"Wake word callback error: {e}")

    with sd.RawInputStream(
        samplerate=porcupine.sample_rate,
        blocksize=porcupine.frame_length,
        dtype="int16",
        channels=1,
        callback=callback,
    ):
        logger.info("üéôÔ∏è  Passive listening for wake word...")
        while not wake_word_triggered.is_set():
            sd.sleep(100)  # non-blocking wait


# === Greeting ===
def generate_llm_greeting():
    now = datetime.now()
    weekday = now.strftime("%A")
    month = now.strftime("%B")
    hour = now.hour
    time_of_day = "morning" if hour < 12 else "afternoon" if hour < 18 else "evening"

    base_greetings = [
        f"Good {time_of_day}! Happy {weekday}.",
        f"Hope you're having a great {weekday}!",
        f"Hello and welcome this fine {time_of_day}.",
        f"It's {month} already! Let's get started.",
        f"Hi! What‚Äôs the first thing you'd like me to do this {time_of_day}?",
    ]
    seed = random.choice(base_greetings)

    try:
        prompt = f"""
        You're Yumi, a clever and friendly assistant robot in a research lab at the Product Realization division of Link√∂ping University.
        It's {time_of_day} on a {weekday} in {month}.
        Say one short and creative sentence (under 20 words) suitable for voice use ‚Äî
        a fun robotics fact, quirky comment, or a science-themed greeting.
        Inspiration: '{seed}' ‚Äî but do not repeat it.
        """
        response = llm.invoke(
            [
                PromptBuilder.greeting_system_msg(),
                {"role": "user", "content": prompt},
            ]
        )
        logger.info(f"üì¢ {response.content}")
        return response.content.strip().strip('"‚Äú‚Äù') or seed
    except Exception as e:
        logger.error(f"Greeting failed: {e}")
        return fallback_llm_greeting(seed)


def fallback_llm_greeting(seed_greeting: str) -> str:
    try:
        response = ollama.chat(
            model=OLLAMA_MODEL,
            messages=[
                {
                    "role": "system",
                    "content": "You are a friendly assistant that creates warm spoken greetings.",
                },
                {
                    "role": "user",
                    "content": f"Improve this fallback greeting for voice use: '{seed_greeting}'",
                },
            ],
        )
        return response["message"]["content"].strip().strip('"‚Äú‚Äù')
    except Exception:
        return seed_greeting


# === Voice Interaction ===
def voice_to_scene_response(
    vp: VoiceProcessor, tts: SpeechSynthesizer, conversational: bool = True
):
    # logger.info(f"üü† Speak your request (scene question or task)...")

    result = vp.capture_voice(conversational=conversational)
    if result is None:
        logger.info(f"üü° No speech detected. Try again.")
        tts.speak("I didn't catch that. Could you please repeat?")
        return

    request, lang = result
    logger.info(f"üéØ You said: {request}")

    cmd_type = classify_command(request, llm)
    logger.info(f"üß† Command classified as: {cmd_type}")

    if request.lower() in {"exit", "quit", "goodbye", "stop"}:
        tts.speak("Okay, goodbye!")
        exit(0)
    if request.lower() in {"reset memory", "clear memory"}:
        memory.clear()
        CHAT_MEMORY_PATH.unlink(missing_ok=True)

        tts.speak("Memory has been reset.")
        return

    logger.info(f"ü§ñ Thinking...")

    if cmd_type == "scene":
        answer = query_scene(request)
        logger.info(f"ü§ñ (Scene Response): {answer}")
    else:
        # Confirm before executing
        tts.speak("Should I plan this task?")
        confirm_result = vp.capture_voice()
        if confirm_result is None:
            tts.speak("No confirmation heard. Skipping the task.")
            return

        confirmation, _ = confirm_result
        # Clean and normalize response
        cleaned = confirmation.lower().translate(
            str.maketrans("", "", string.punctuation)
        )
        # Match negative intent first
        if any(word in cleaned for word in CANCEL_WORDS):
            tts.speak("Okay, discarding the task.")
            return

        # Match positive intent
        if any(word in cleaned for word in CONFIRM_WORDS):
            tts.speak("Okay, planning task...")

            # Only now store it in the database
            vp.storage.store_instruction(vp.session_id, lang, request)
            answer = process_task(request)
            print("ü§ñ (Task Response):", answer)
        else:
            tts.speak(
                "I wasn't sure what you meant. Could you please repeat your confirmation?"
            )
            retry_result = vp.capture_voice()
            if retry_result is None:
                tts.speak("Still couldn't hear you. Skipping the task.")
                return

            confirmation_retry, _ = retry_result
            cleaned_retry = confirmation_retry.lower().translate(
                str.maketrans("", "", string.punctuation)
            )

            if any(word in cleaned_retry for word in CONFIRM_WORDS):
                tts.speak("Okay, planning task...")
                vp.storage.store_instruction(vp.session_id, lang, request)
                answer = process_task(request)
                print("ü§ñ (Task Response):", answer)
            else:
                tts.speak("No confirmation. Skipping the task.")
    tts.speak(answer)


# === CLI Entry Point ===
if __name__ == "__main__":

    setup_logging()
    vp = VoiceProcessor()
    tts = SpeechSynthesizer()
    load_chat_history()

    if not hasattr(voice_to_scene_response, "greeted"):
        greeting = generate_llm_greeting()
        tts.speak(greeting)
        voice_to_scene_response.greeted = True

    first_turn = True

    try:
        # while True:
        #     voice_to_scene_response(vp, tts, conversational=not first_turn)
        #     save_chat_history()
        #     first_turn = False
        #     logger.info(f"üü° Listening again in a few seconds... (Ctrl+C to stop)")

        while True:
            wake_word_triggered.clear()  # reset flag
            listen_for_wake_word(vp, tts)  # blocks until flag is set
            tts.speak(random.choice(WAKE_RESPONSES))
            voice_to_scene_response(vp, tts, conversational=True)
            save_chat_history()
            first_turn = False
            logger.info(f"üü° Listening again in a few seconds...")

    except KeyboardInterrupt:
        logger.info("üëã Exiting session by user (Ctrl+C).")

    finally:
        if CHAT_MEMORY_PATH.exists():
            CHAT_MEMORY_PATH.unlink()
            logger.info("üßπ Deleted chat memory on exit.")



# === orchestrator.py ===

# modalities/orchestrator.py


import logging
import threading
import uuid

from config.app_config import *
from mini_project.modalities.gesture_processor import GestureDetector
from mini_project.modalities.synchronizer import synchronize_and_unify
from mini_project.modalities.voice_processor import VoiceProcessor

# Initialize logging
setup_logging(level=logging.INFO)
logger = logging.getLogger("Orchestrator")

# Global event to signal end of session capture
SESSION_END_EVENT = threading.Event()


def run_voice_capture(session_id: str):
    vp = VoiceProcessor(session_id=session_id)
    vp.capture_voice()
    logger.info("Voice capture completed.")
    # Signal that voice capture is complete; gesture capture should stop.
    SESSION_END_EVENT.set()


def run_gesture_capture(session_id: str):
    gd = GestureDetector(session_id=session_id)
    # Pass termination event so gesture capture can close gracefully.
    gd.process_video_stream(termination_event=SESSION_END_EVENT)
    logger.info("Gesture capture completed.")


if __name__ == "__main__":
    session_id = str(uuid.uuid4())
    logger.info(f"Starting session with session_id: {session_id}")

    voice_thread = threading.Thread(target=run_voice_capture, args=(session_id,))
    gesture_thread = threading.Thread(target=run_gesture_capture, args=(session_id,))

    voice_thread.start()
    gesture_thread.start()

    voice_thread.join()
    gesture_thread.join()

    logger.info("Session capture ended. Now running synchronizer/unifier...")

    try:
        synchronize_and_unify(liu_id=None)
        logger.info("Unification complete. Check the unified_instructions table.")
    except Exception as e:
        logger.error(f"Synchronization failed: {e}")
        logger.debug("Exception details:", exc_info=True)



# === prompt_utils.py ===

from datetime import datetime
from typing import Dict, List

from langchain_core.prompts import PromptTemplate


class PromptBuilder:
    @staticmethod
    def scene_prompt_template() -> PromptTemplate:
        return PromptTemplate.from_template(
            """
You are an intelligent robotic assistant, with the camera as your eye. Based on the objects in the scene, listed in a camera_vision database table, respond concisely and clearly to the user question. One line answers are acceptable.
if there are any, the objects here are sitting on a table. Do not assume objects unless they are listed.
---
Each object has the following fields:
# - object_name: the name of the object in the scene.
# - object_color: the color of the object in the scene
# - pos_x, pos_y, pos_z: the 3D position of the object in the scene relative to table (0,0). You can use the object position to imagine the relative distances of the objects from each other
# - rot_x, rot_y, rot_z: the orientation of the object in the scene

---
if the object is a slide, it will have a usd_name of slide.usd, and the holder object will have a usd_name of holder.usd
any objects with object_name that does not start with "slide..." are not slides
---

Avoid technical terms like rot_x or pos_y. Instead, describe in natural language (e.g., "position x", "rotation y").
Assume the pos_x, pos_y, pos_z are coordinates of the objects on the table with respect to a 0,0,0 3D coordinate which is the reference (the far right edge of the table top rectangle). the values are tenth of a mm unit.
---
Previous conversation:
{chat_history}

User question: {question}
Objects in scene:
{data}
---
Answer:
        """
        )

    @staticmethod
    def operation_sequence_prompt(
        available_sequences: str,
        task_templates: str,
        object_context: str,
        sort_order: str,
    ) -> str:
        return """
            You are a robotic task planner. Your job is to break down natural language commands into valid low-level robot operations.

            ### CONTEXT:

            #### 1. AVAILABLE SEQUENCES:
            The robot can only use the following valid sequence names from the sequence_library table:
            {available_sequences}

            ‚ö†Ô∏è Do NOT invent or assume sequences. Only use the names provided above. Invalid examples: checkColor, rotate, scan, verify, etc.


            #### 2. TASK TEMPLATES:
            These are default sequences for high-level tasks like sorting, assembling, etc.

            Examples:
            {task_templates}

            #### 3. OBJECT CONTEXT:
            Here are the known objects the robot can see, with color:
            {object_context}

            #### 4. SORT ORDER:
            {sort_order}



            ### INSTRUCTIONS:
            1. Determine the intended task (e.g., "sort").
            2. Use the default task template unless user modifies the plan.
            3. Match object names by color (e.g., "green slide").
            4. If the user specifies steps (e.g., ‚Äúrotate before drop‚Äù), update the sequence.
            5. Apply the sequence to each object in order.
            6. Must always Add `"go_home"` at the end unless told otherwise.
            7. The object names in must be Slide_1, Slide_2 etc without the colurs in them
            ### RESPONSE FORMAT:
            Example JSON array of operations:
            [
            {"sequence_name": "pick", "object_name": "Slide_1"},
            {"sequence_name": "travel", "object_name": "Slide_1"},
            {"sequence_name": "drop", "object_name": "Slide_1"},
            {"sequence_name": "go_home", "object_name": ""}
            ]

            ‚ö†Ô∏è Use only the object names listed under OBJECT CONTEXT. Do not invent or modify object names like ‚ÄúGreen_Slide‚Äù, ‚ÄúSlide #1‚Äù, etc.
            Return only one JSON array ‚Äî NEVER return multiple arrays or repeat the plan.

            ### Emphasis
            Match objects by their color, but use the actual object_name from context.

            üö´ DO NOT include explanations like "Here's the plan:" or "In reverse order:" ‚Äî only return ONE JSON array.

            Do NOT include extra text, markdown, or explanations.
            Note: All generated plans will be stored step-by-step in a planning table called "operation_sequence", indexed by a group ID called "operation_id".
            Each row in the output corresponds to one line in this table.
        """

    @staticmethod
    def sort_order_prompt(command_text: str) -> str:
        return f"""
            Given the following user instruction:
            \"{command_text}\"

            Extract the desired sort order as a JSON array of objects.
            Each item should include:
            - object_name (if mentioned)
            - object_color (if used for sorting)

            Respond only with a clean JSON array.
        """

    @staticmethod
    def sort_order_system_msg() -> Dict:
        return {
            "role": "system",
            "content": "You are a planner that helps extract object sorting order from commands.",
        }

    @staticmethod
    def validate_llm_json(raw: str) -> bool:
        """Check if LLM response looks like a valid JSON array."""
        return raw.strip().startswith("[") and raw.strip().endswith("]")

    @staticmethod
    def greeting_prompt() -> str:
        hour = datetime.now().hour
        if 5 <= hour < 12:
            time_context = "morning"
        elif 12 <= hour < 17:
            time_context = "afternoon"
        elif 17 <= hour < 22:
            time_context = "evening"
        else:
            time_context = "night"

        return f"""
        You're a friendly assistant robot, Yumi.

        It's {time_context} now.

        Say a very short, warm, and creative greeting (under 2 sentences), suitable for voice.
        Just one sentences, Please
        Mention you're ready to help. Avoid long phrases or explanations."""

    @staticmethod
    def greeting_system_msg() -> Dict:
        return {
            "role": "system",
            "content": "You generate short spoken greetings for a robotic assistant.",
        }



# === session_manager.py ===

# workflow/session_manager.py

import logging
import uuid

from config.app_config import DB_PATH, TEMP_AUDIO_PATH, VOICE_DATA_PATH
from mini_project.authentication._face_auth import FaceAuthSystem
from mini_project.authentication._voice_auth import VoiceAuth

logger = logging.getLogger("SessionManager")


class SessionManager:
    def __init__(self, face_auth: FaceAuthSystem = None, voice_auth: VoiceAuth = None):
        # Instantiate authentication modules if not provided.
        self.face_auth = face_auth if face_auth else FaceAuthSystem()
        self.voice_auth = (
            voice_auth
            if voice_auth
            else VoiceAuth()
        )
        self.session_id = None
        self.authenticated_user = (
            None  # Expected dict, keys: 'liu_id', 'first_name', 'last_name', etc.
        )
        self.running = False

    def authenticate_user(self):
        """
        Attempt face authentication. If the face is not recognized, trigger manual registration.
        Following successful face registration, trigger voice registration.
        """
        logger.info("Attempting face authentication...")
        user = self.face_auth.identify_user()
        if user:
            self.authenticated_user = user
            logger.info(
                f"Authenticated Face. Welcome {user['first_name']} {user['last_name']} (liu_id: {user['liu_id']})"
            )
        else:
            logger.warning(
                "Face not recognized. Initiating manual face registration..."
            )
            self.face_auth.register_user()

            # Retry identification after registration
            user = self.face_auth.identify_user()
            if user:
                self.authenticated_user = user
                logger.info(
                    f"Authenticated after registration. Welcome {user['first_name']} {user['last_name']} (liu_id: {user['liu_id']})"
                )
                # Trigger voice registration (this method should exist in VoiceAuth)
                try:
                    logger.info(f"Initiating voice registration...")
                    self.voice_auth.register_voice_for_user(
                        first_name=user.get("first_name"),
                        last_name=user.get("last_name"),
                        liu_id=user.get("liu_id"),
                    )
                    logger.info("Voice registration completed successfully.")
                except Exception as e:
                    logger.error(f"Voice registration failed: {str(e)}")
            else:
                logger.error("User authentication failed after registration.")
                return None
        return self.authenticated_user

    def create_session(self):
        """
        Create a new session by generating a unique session ID and setting the running flag.
        """
        self.session_id = str(uuid.uuid4())
        self.running = True
        logger.info(f"New session created with ID: {self.session_id}")
        return self.session_id

    def cancel_session(self):
        """
        Cancel the current session.
        """
        if self.running:
            logger.info(f"Cancelling session: {self.session_id}")
            self.running = False
        else:
            logger.info("No active session to cancel.")

    def retry_session(self):
        """
        Cancel the current session and create a new session.
        """
        self.cancel_session()
        return self.create_session()



# === synchronizer.py ===

# modalities/synchronizer.py


import json
import logging

# import sqlite3
import subprocess
import uuid
from datetime import datetime
from functools import lru_cache
from typing import Dict, List, Optional

from config.app_config import (
    BATCH_SIZE,
    # DB_PATH,
    LLM_MAX_RETRIES,
    LLM_MODEL,
    UNIFY_PROMPT_TEMPLATE,
    setup_logging,
)
from config.constants import GESTURE_TABLE, PROCESSED_COL, UNIFIED_TABLE, VOICE_TABLE
from mini_project.database.connection import get_connection

# Initialize logging with desired level
setup_logging(level=logging.INFO)
logger = logging.getLogger("Synchronizer")
DELIMITER = "\n"


def get_instructions_by_session(
    cursor, limit: int, offset: int
) -> Dict[str, List[Dict]]:
    """
    Fetches unprocessed instructions from voice and gesture tables in batches.

    Args:
        conn: SQLite database connection object.
        limit: Maximum number of records to fetch.
        offset: Starting offset for batch processing.
    Returns:
        A dictionary mapping session IDs to lists of instruction records.
    """
    query = f"""
        SELECT id, session_id, 'voice' AS modality, transcribed_text AS instruction_text, timestamp
        FROM {VOICE_TABLE} WHERE {PROCESSED_COL} = FALSE
        UNION ALL
        SELECT id, session_id, 'gesture' AS modality, gesture_text AS instruction_text, timestamp
        FROM {GESTURE_TABLE} WHERE {PROCESSED_COL} = FALSE
        ORDER BY timestamp ASC
        LIMIT %s OFFSET %s
    """
    cursor.execute(query, (limit, offset))
    rows = cursor.fetchall()
    sessions = {}

    for row in rows:
        ts = row[4] if isinstance(row[4], datetime) else datetime.fromisoformat(row[4])
        record = {
            "id": row[0],
            "session_id": row[1],
            "modality": row[2],
            "instruction_text": row[3],
            "timestamp": ts,
        }
        sessions.setdefault(record["session_id"], []).append(record)
    return sessions


def store_unified_instruction(
    cursor,
    session_id: str,
    timestamp: datetime,
    voice_command: str,
    gesture_command: str,
    unified_command: str,
    liu_id: Optional[str] = None,
) -> None:
    """
    Stores a unified instruction into the unified_instructions table.

    Args:
        session_id: The session identifier.
        timestamp: The timestamp of the instruction.
        voice_command: The voice instruction text.
        gesture_command: The gesture instruction text.
        unified_command: The unified command text.
        liu_id: Optional user ID.
        db_path: Path to the SQLite database.
    """
    cursor.execute(
        f"""
        INSERT INTO {UNIFIED_TABLE} (session_id, timestamp, liu_id, voice_command, gesture_command, unified_command)
        VALUES (%s, %s, %s, %s, %s, %s)
        """,
        (
            session_id,
            timestamp,
            liu_id,
            voice_command,
            gesture_command,
            unified_command,
        ),
    )
    logger.info(
        f"Stored unified instruction for session {session_id}: {unified_command}"
    )


def mark_instructions_as_processed(cursor, session_id: str) -> None:
    """
    Marks all voice and gesture instructions for the given session as processed.

    Args:
        conn: SQLite database connection object.
        session_id: The session identifier.
    """
    cursor.execute(
        f"UPDATE {VOICE_TABLE} SET {PROCESSED_COL} = TRUE WHERE session_id = %s AND {PROCESSED_COL} = FALSE",
        (session_id,),
    )
    cursor.execute(
        f"UPDATE {GESTURE_TABLE} SET {PROCESSED_COL} = TRUE WHERE session_id = %s AND {PROCESSED_COL} = FALSE",
        (session_id,),
    )
    logger.info(f"Marked instructions as processed for session {session_id}.")


@lru_cache(maxsize=128)
def llm_unify(voice_text: str, gesture_text: str, max_retries=LLM_MAX_RETRIES) -> str:
    """
    Combines a voice command with a gesture cue into a unified instruction using an LLM.

    Args:
        voice_text: The primary voice instruction (e.g., "Turn right").
        gesture_text: The supplementary gesture instruction (e.g., "Pointing up").
        max_retries: Number of retry attempts for LLM calls.
    Returns:
        A unified command string, or a fallback if unification fails.
    Example:
        >>> llm_unify("Stop", "Hand raised")
        'Stop with hand raised'
    """
    formatted_prompt = UNIFY_PROMPT_TEMPLATE.format(
        voice_text=voice_text, gesture_text=gesture_text
    )
    for attempt in range(max_retries):
        try:
            result = subprocess.run(
                ["ollama", "run", LLM_MODEL, formatted_prompt],
                capture_output=True,
                text=True,
                check=True,
                encoding="utf-8",
            )
            output = result.stdout.strip()
            if output and len(output) > 3:
                return output
            logger.warning(f"Attempt {attempt + 1}: Invalid output '{output}'")
        except subprocess.CalledProcessError as e:
            logger.warning(f"Attempt {attempt + 1} failed: {e}")
    logger.error("All LLM attempts failed. Using fallback.")
    return f"Voice: {voice_text}, Gesture: {gesture_text}"


def merge_session_commands(
    session_commands: List[Dict], delimiter: str = DELIMITER
) -> Dict[str, str]:
    """
    Merges instructions from a session into a single string per modality.

    Args:
        session_commands: List of instruction records for a session.
        delimiter: String used to join multiple instructions.
    Returns:
        A dictionary with merged voice and gesture instructions.
    """
    voice_records = [cmd for cmd in session_commands if cmd["modality"] == "voice"]
    gesture_records = [cmd for cmd in session_commands if cmd["modality"] == "gesture"]

    voice_records.sort(key=lambda x: x["timestamp"])
    gesture_records.sort(key=lambda x: x["timestamp"])

    merged_voice = delimiter.join(
        record["instruction_text"] for record in voice_records
    ).strip()
    merged_gesture = delimiter.join(
        record["instruction_text"] for record in gesture_records
    ).strip()

    return {"voice": merged_voice, "gesture": merged_gesture}


def synchronize_and_unify(
    liu_id: Optional[str] = None, batch_size: int = BATCH_SIZE
) -> None:
    """
    Synchronizes and unifies voice and gesture instructions in batches.

    Args:
        db_path: Path to the SQLite database.
        liu_id: Optional user ID.
        batch_size: Number of records to process per batch.
    """
    offset = 0
    try:
        conn = get_connection()
    except Exception as e:
        logger.error(f"‚ùå Failed to connect to PostgreSQL: {e}")
        raise
    with conn:
        cursor = conn.cursor()
        while True:
            sessions = get_instructions_by_session(
                cursor, limit=batch_size, offset=offset
            )
            if not sessions:
                logger.info("No more unprocessed instructions found.")
                break

            for session_id, records in sessions.items():
                merged = merge_session_commands(records)
                voice_text = merged.get("voice", "")
                gesture_text = merged.get("gesture", "")
                session_timestamp = max(r["timestamp"] for r in records)
                unified_command = llm_unify(voice_text, gesture_text)
                store_unified_instruction(
                    cursor,
                    session_id,
                    session_timestamp,
                    voice_text,
                    gesture_text,
                    unified_command,
                    liu_id,
                )
                mark_instructions_as_processed(cursor, session_id)
            offset += batch_size
            logger.info(f"Processed batch of {batch_size} records. Offset: {offset}")
    conn.commit()
    logger.info("Synchronization and unification complete.")


if __name__ == "__main__":
    synchronize_and_unify()



# === task_manager.py ===

# workflow/task_manager.py

import logging
import threading
import time
import tkinter as tk
from tkinter import messagebox, scrolledtext

from config.app_config import DB_PATH, TEMP_AUDIO_PATH, VOICE_DATA_PATH
from mini_project.authentication._face_auth import FaceAuthSystem
from mini_project.authentication._voice_auth import VoiceAuth
from mini_project.modalities.command_processor import CommandProcessor
from mini_project.modalities.orchestrator import run_gesture_capture, run_voice_capture
from mini_project.modalities.synchronizer import synchronize_and_unify
from mini_project.workflow.session_manager import SessionManager

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(message)s")
logger = logging.getLogger("TaskManagerGUI_Approach1")


class TaskManagerGUIApproach1:
    def __init__(self):
        # Create the main GUI window
        self.root = tk.Tk()
        self.root.title("HRI Task Manager - Approach 1")
        self.root.geometry("700x550")

        # Instantiate session manager and command processor.
        self.session_manager = SessionManager()
        self.cmd_processor = CommandProcessor()

        # Build GUI elements
        tk.Label(
            self.root,
            text="Human-Robot Interaction System (Approach 1)",
            font=("Arial", 18),
        ).pack(pady=10)
        self.status_label = tk.Label(self.root, text="Status: Idle", font=("Arial", 12))
        self.status_label.pack(pady=5)

        btn_frame = tk.Frame(self.root)
        btn_frame.pack(pady=10)
        self.start_btn = tk.Button(
            btn_frame, text="Start Execution", width=15, command=self.start_execution
        )
        self.start_btn.grid(row=0, column=0, padx=5, pady=5)
        self.stop_btn = tk.Button(
            btn_frame,
            text="Stop Execution",
            width=15,
            command=self.stop_execution,
            state=tk.DISABLED,
        )
        self.stop_btn.grid(row=0, column=1, padx=5, pady=5)
        self.new_cmd_btn = tk.Button(
            btn_frame,
            text="New Command",
            width=15,
            command=self.new_command,
            state=tk.DISABLED,
        )
        self.new_cmd_btn.grid(row=1, column=0, padx=5, pady=5)
        tk.Button(
            btn_frame, text="Clear Tables", width=15, command=self.clear_tables
        ).grid(row=1, column=1, padx=5, pady=5)
        tk.Button(btn_frame, text="Exit", width=15, command=self.exit_application).grid(
            row=2, column=0, columnspan=2, pady=10
        )

        self.log_text = scrolledtext.ScrolledText(
            self.root, width=80, height=15, wrap=tk.WORD
        )
        self.log_text.pack(pady=5)
        self.log_event("Application started. Please authenticate.")

    def log_event(self, message, level=logging.INFO):
        logger.log(level, message)
        self.log_text.insert(tk.END, f"{message}\n")
        self.log_text.see(tk.END)
        self.status_label.config(text=f"Status: {message}")

    def set_controls_state(
        self, start_enabled=True, stop_enabled=False, new_cmd_enabled=False
    ):
        self.start_btn.config(state=tk.NORMAL if start_enabled else tk.DISABLED)
        self.stop_btn.config(state=tk.NORMAL if stop_enabled else tk.DISABLED)
        self.new_cmd_btn.config(state=tk.NORMAL if new_cmd_enabled else tk.DISABLED)

    def start_execution(self):
        if not self.session_manager.running:
            # Ensure that the user is authenticated.
            if not self.session_manager.authenticated_user:
                user = self.session_manager.authenticate_user()
                if not user:
                    self.log_event(
                        "Authentication failed. Aborting execution.",
                        level=logging.ERROR,
                    )
                    return
                else:
                    # Display a welcome message once authenticated.
                    welcome_msg = f"Welcome, {user['first_name']} {user['last_name']} (ID: {user['liu_id']})"
                    self.log_event(welcome_msg)
            # Create a new session if one does not exist.
            if not self.session_manager.session_id:
                self.session_manager.create_session()
            self.set_controls_state(
                start_enabled=False, stop_enabled=True, new_cmd_enabled=False
            )
            threading.Thread(target=self.execution_pipeline, daemon=True).start()
            self.log_event("Execution started.")

    def stop_execution(self):
        if self.session_manager.running:
            self.session_manager.cancel_session()
            self.set_controls_state(
                start_enabled=True, stop_enabled=False, new_cmd_enabled=True
            )
            self.log_event("Execution stopped.")

    def new_command(self):
        # Retry session and start a new capture cycle.
        self.session_manager.retry_session()
        self.log_event("New command session started.")
        self.start_execution()

    def clear_tables(self):
        try:
            cursor = self.cmd_processor.conn.cursor()
            cursor.execute("DELETE FROM unified_instructions")
            cursor.execute("DELETE FROM voice_instructions")
            cursor.execute("DELETE FROM gesture_instructions")
            cursor.execute("DELETE FROM instruction_operation_sequence")
            self.cmd_processor.conn.commit()
            self.log_event("Database instructions cleared.")
        except Exception as e:
            self.log_event(f"Error clearing tables: {str(e)}", level=logging.ERROR)

    def exit_application(self):
        self.stop_execution()
        self.cmd_processor.close()
        self.root.destroy()

    def execution_pipeline(self):
        """
        Execution Pipeline:
          1. Start voice and gesture capture concurrently.
          2. Display prompt for voice input.
          3. Wait for the capture threads to complete.
          4. Merge inputs via the synchronizer.
          5. Retrieve and confirm the unified command with the user.
          6. If confirmed and processed, end the session; otherwise, repeat capture.
        """
        while self.session_manager.running:
            try:
                # Start voice and gesture capture concurrently.
                voice_thread = threading.Thread(
                    target=run_voice_capture,
                    args=(self.session_manager.session_id,),
                    daemon=True,
                )
                gesture_thread = threading.Thread(
                    target=run_gesture_capture,
                    args=(self.session_manager.session_id,),
                    daemon=True,
                )
                voice_thread.start()
                gesture_thread.start()
                self.log_event(
                    "Voice and gesture capture started. Please speak your request."
                )
                voice_thread.join()
                gesture_thread.join()
                self.log_event("Voice and gesture capture completed.")

                # Merge inputs using the synchronizer.
                liu_id = (
                    self.session_manager.authenticated_user.get("liu_id")
                    if self.session_manager.authenticated_user
                    else None
                )
                self.log_event("Merging captured inputs...")
                synchronize_and_unify(db_path=DB_PATH, liu_id=liu_id)
                unified = self.cmd_processor.get_unprocessed_unified_command()
                if unified:
                    unified_text = unified.get("unified_command", "")
                    self.log_event(f"Unified Command: {unified_text}")
                    if messagebox.askyesno(
                        "Confirm Command",
                        f"Is this your intended command?\n\n{unified_text}",
                    ):
                        self.log_event("User confirmed command. Processing...")
                        if self.cmd_processor.process_command(unified):
                            self.log_event("Command processed successfully.")
                            self.session_manager.cancel_session()
                            break
                        else:
                            self.log_event(
                                "Command processing failed. Re-capturing input...",
                                level=logging.ERROR,
                            )
                            continue
                    else:
                        self.log_event(
                            "Command rejected by user. Re-capturing input..."
                        )
                        continue  # Repeat the capture loop.
                else:
                    self.log_event(
                        "No unified command generated. Retrying capture...",
                        level=logging.WARNING,
                    )
                time.sleep(2)
            except Exception as e:
                self.log_event(
                    f"Error during execution pipeline: {str(e)}", level=logging.ERROR
                )
                break

        self.set_controls_state(
            start_enabled=True, stop_enabled=False, new_cmd_enabled=True
        )
        self.log_event("Session ended. Use 'Start Execution' to begin a new session.")

    def run(self):
        self.root.mainloop()


if __name__ == "__main__":
    app = TaskManagerGUIApproach1()
    app.run()



# === voice_processor.py ===

# modalities/voice_processor.py

import logging
import os

# import sqlite3
import tempfile
import time
import uuid
from pathlib import Path
from typing import Any, Dict, Optional, Tuple

import numpy as np
import psycopg2
import pyttsx3
import sounddevice as sd
import webrtcvad
from faster_whisper import WhisperModel
from gtts import gTTS
from playsound import playsound
from pluggy import Result
from scipy.io.wavfile import write

from config.app_config import (
    MAX_TRANSCRIPTION_RETRIES,
    MIN_DURATION_SEC,
    VOICE_PROCESSING_CONFIG,
    VOICE_TTS_SETTINGS,
)
from config.constants import WHISPER_LANGUAGE_NAMES
from mini_project.database.connection import get_connection

logging.getLogger("comtypes").setLevel(logging.WARNING)
logger = logging.getLogger("VoiceProcessor")


class SpeechSynthesizer:
    _instance = None  # Singleton instance

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(SpeechSynthesizer, cls).__new__(cls)
            cls._instance._init_engine()
        return cls._instance

    def _init_engine(self):
        self.use_gtts = VOICE_TTS_SETTINGS["use_gtts"]
        self.voice_speed = VOICE_TTS_SETTINGS["speed"]
        self.ping_path = Path(VOICE_TTS_SETTINGS["ping_sound_path"]).resolve()
        self.ding_path = Path(VOICE_TTS_SETTINGS["ding_sound_path"]).resolve()
        self.voice_index = VOICE_TTS_SETTINGS.get("voice_index", 1)

        if not self.use_gtts:
            try:
                self.engine = pyttsx3.init()
                voices = self.engine.getProperty("voices")
                self.engine.setProperty("rate", self.voice_speed)
                self.engine.setProperty("voice", voices[self.voice_index].id)
            except Exception as e:
                logger.error(f"[TTS] Error initializing pyttsx3: {e}")

    def play_ping(self):
        try:
            if not self.ping_path.exists():
                raise FileNotFoundError(f"Ping sound file not found: {self.ping_path}")
            # playsound(str(self.ping_path))
        except Exception as e:
            logger.warning(f"[Ping Sound] Failed to play: {e}")

    def play_ding(self):
        try:
            if not self.ding_path.exists():
                raise FileNotFoundError(f"Ding sound file not found: {self.ding_path}")
            playsound(str(self.ding_path))
        except Exception as e:
            logger.warning(f"[Ding Sound] Failed to play: {e}")

    def speak(self, text: str):
        if self.use_gtts:
            try:
                temp_path = (
                    Path(tempfile.gettempdir()) / f"speech_{uuid.uuid4().hex}.mp3"
                )
                gTTS(text=text).save(temp_path)
                playsound(str(temp_path))
                os.remove(temp_path)
            except Exception as e:
                logger.error(f"[TTS:gTTS] Error: {e}")
                print(f"[TTS Fallback] {text}")
        else:
            try:
                self.engine.say(text)
                self.engine.runAndWait()
            except Exception as e:
                logger.error(f"[TTS:pyttsx3] Error during speech: {e}")
                print(f"[TTS Fallback] {text}")


class AudioRecorder:

    def __init__(self, synthesizer: Optional[SpeechSynthesizer] = None) -> None:
        self.synthesizer = synthesizer or SpeechSynthesizer()
        self.config: Dict[str, Any] = VOICE_PROCESSING_CONFIG["recording"]
        self.temp_audio_path: str = self.config["temp_audio_path"]
        self.sampling_rate: int = self.config["sampling_rate"]
        self.vad = webrtcvad.Vad()
        self.vad.set_mode(3)
        self.speech_detected = False
        self.noise_floor: Optional[float] = None
        self.calibrated = False

        if not isinstance(self.sampling_rate, int) or self.sampling_rate <= 0:
            raise ValueError("Sampling rate must be a positive integer.")
        if not isinstance(self.temp_audio_path, str):
            raise ValueError("Temp audio path must be a string.")

    def calibrate_noise(self) -> float:
        if not self.calibrated:
            logger.info("‚úÖ Calibrating ambient noise...")
        noise_rms_values: list[float] = []
        stream = sd.InputStream(
            samplerate=self.sampling_rate, channels=1, dtype="int16"
        )
        end_time = time.time() + self.config["calibration_duration"]
        with stream:
            while time.time() < end_time:
                frame, _ = stream.read(
                    int(self.sampling_rate * self.config["frame_duration"])
                )
                rms = np.sqrt(np.mean(frame.astype(np.float32) ** 2))
                noise_rms_values.append(rms)
        noise_floor = np.mean(noise_rms_values)
        if not self.calibrated:
            logger.info(
                f"‚úÖ Ambient noise calibration complete. Noise floor: {noise_floor:.2f}"
            )
        return noise_floor

    def record_audio(self, speak_prompt: bool = False, play_ding: bool = True) -> None:
        # üî∏ Use cached noise floor if available
        if self.noise_floor is None:
            self.noise_floor = self.calibrate_noise()
            if not self.calibrated:
                logger.info(
                    f"‚úÖ Amplitude threshold set to: {self.noise_floor + self.config['amplitude_margin']:.2f} (Noise floor: {self.noise_floor:.2f} + Margin: {self.config['amplitude_margin']})"
                )
                self.calibrated = True

        amplitude_threshold = self.noise_floor + self.config["amplitude_margin"]
        logger.info(
            f"‚úÖ Amplitude threshold set to: {amplitude_threshold:.2f} (Noise floor: {self.noise_floor:.2f} + Margin: {self.config['amplitude_margin']})"
        )

        # üó£Ô∏è Speak the instruction aloud
        if speak_prompt:
            try:
                self.synthesizer.speak("Tell me, how can I help you?")
            except Exception as e:
                logger.warning(f"[Recorder] Failed to speak instruction: {e}")

        logger.info("üì¢ Voice recording: Speak now...üü¢üü¢üü¢")

        # üîî Play ding sound immediately after prompt
        if play_ding:
            try:
                self.synthesizer.play_ding()
            except Exception as e:
                logger.warning(f"[Recorder] Failed to play ding: {e}")

        # logger.info("üü¢ Listening...")

        audio = []
        start_time = time.time()
        silence_start: Optional[float] = None
        self.speech_detected = False

        stream = sd.InputStream(
            samplerate=self.sampling_rate, channels=1, dtype="int16"
        )
        with stream:
            while True:
                frame, _ = stream.read(
                    int(self.sampling_rate * self.config["frame_duration"])
                )
                audio.append(frame)
                is_speech_vad = self.vad.is_speech(frame.tobytes(), self.sampling_rate)
                rms = np.sqrt(np.mean(frame.astype(np.float32) ** 2))
                is_speech_amplitude = rms > amplitude_threshold
                logger.debug(
                    f"VAD: {is_speech_vad}, RMS: {rms:.2f}, Amplitude: {is_speech_amplitude}"
                )
                if is_speech_vad and is_speech_amplitude:
                    self.speech_detected = True
                    silence_start = None
                elif silence_start is None:
                    silence_start = time.time()
                else:
                    threshold = (
                        self.config["post_speech_silence_duration"]
                        if self.speech_detected
                        else self.config["initial_silence_duration"]
                    )
                    if time.time() - silence_start > threshold:
                        break
                if time.time() - start_time > self.config["max_duration"]:
                    break
        audio = np.concatenate(audio, axis=0)
        duration = time.time() - start_time
        logger.info(f"üü¢ Recording completed. Duration: {duration:.2f} seconds")
        if duration < MIN_DURATION_SEC:
            logger.warning(
                f"Recording too short ({duration:.2f}s). Treating as no speech."
            )
            self.speech_detected = False  # suppress further processing
            return

        try:
            write(self.temp_audio_path, self.sampling_rate, audio)
            logger.info(f"üü¢ Audio saved to {self.temp_audio_path}")
        except Exception as e:
            logger.error(f"Error saving audio: {e}")
            raise


class Transcriber:

    def __init__(self) -> None:
        self.config: Dict[str, Any] = VOICE_PROCESSING_CONFIG["whisper"]
        self.model = WhisperModel(
            self.config["model"],
            device=self.config["device"],
            compute_type=self.config["compute_type"],
        )

    # =================== only store transcribed_text and language ‚Äî
    # =================== which means you're storing Swedish text if the speaker used Swedish.

    # def transcribe_audio(self, audio_path: str) -> Tuple[str, str]:
    #     try:
    #         segments, info = self.model.transcribe(audio_path)
    #         original_text = " ".join([segment.text for segment in segments])
    #         detected_language = info.language
    #         return original_text, detected_language
    #     except FileNotFoundError as e:
    #         logger.error(f"Audio file not found: {audio_path}")
    #         raise
    #     except RuntimeError as e:
    #         logger.error(f"Error loading Whisper model: {e}")
    #         raise
    #     except Exception as e:
    #         logger.error(f"Unexpected error during transcription: {e}")
    #         raise

    # =================== Keeps info.language accurate (auto-detected language)
    # =================== Transcribes as English, no matter the input language

    def transcribe_audio(self, audio_path: str) -> Tuple[str, str]:
        try:
            # Transcribe with forced translation to English
            segments, info = self.model.transcribe(audio_path, beam_size=5)
            detected_language = info.language
            language_prob = info.language_probability

            if language_prob < 0.3:
                logger.warning(
                    f"üî¥ Low language detection confidence: {language_prob:.2f} for '{detected_language}'"
                )
                raise ValueError("Unclear speech or unsupported language.")

            if detected_language != "en":
                # Re-run with translation
                segments, _ = self.model.transcribe(
                    audio_path, task="translate", beam_size=5
                )

            original_text = " ".join([segment.text for segment in segments])
            if not original_text.strip():
                raise ValueError("No intelligible speech detected.")

            language = WHISPER_LANGUAGE_NAMES.get(detected_language, detected_language)
            return original_text, language

        except FileNotFoundError as e:
            logger.error(f"Audio file not found: {audio_path}")
            raise
        except RuntimeError as e:
            logger.error(f"Error loading Whisper model: {e}")
            raise
        except Exception as e:
            logger.warning(f"üî¥ Unexpected error during transcription: {e}")
            raise


class Storage:
    def __init__(self) -> None:
        self.config: Dict[str, Any] = VOICE_PROCESSING_CONFIG["database"]
        self.db_path: str = self.config["db_path"]
        self.check_database()

    def check_database(self) -> None:
        pass  # PostgreSQL always exists; no need to create .db file

    def store_instruction(
        self,
        session_id: str,
        detected_language: str,
        transcribed_text: str,
        retries: int = 3,
        delay: float = 1.0,
    ) -> None:
        for attempt in range(retries):
            try:
                conn = get_connection()
                with conn:
                    with conn.cursor() as cursor:
                        cursor.execute(
                            """
                            INSERT INTO voice_instructions (session_id, transcribed_text, language)
                            VALUES (%s, %s, %s)
                            """,
                            (session_id, transcribed_text, detected_language),
                        )
                logger.info(
                    "‚úÖ Voice instruction stored successfully in voice_instructions table."
                )
                return
            except psycopg2.OperationalError as e:
                if "could not connect" in str(e).lower() and attempt < retries - 1:
                    logger.warning(
                        f"Database connection error. Retrying in {delay} seconds..."
                    )
                    time.sleep(delay)
                else:
                    logger.error(f"[PostgreSQL] Operational error: {e}")
                    raise
            except Exception as e:
                logger.error(
                    f"[PostgreSQL] Unexpected error storing voice instruction: {e}"
                )
                raise


class VoiceProcessor:
    def __init__(self, session_id: Optional[str] = None) -> None:
        self.recorder = AudioRecorder()
        self.transcriber = Transcriber()
        self.storage = Storage()
        self.session_id = session_id or str(uuid.uuid4())
        self.synthesizer = SpeechSynthesizer()
        self.recorder = AudioRecorder(self.synthesizer)

    # def capture_voice(self, conversational: bool = True) -> -> Optional[Tuple[str, str]]:
    #     try:
    #         logger.info("üü† Starting voice capture process...")
    #          # conversational = True ‚ûù just play ding
    #         self.recorder.record_audio(speak_prompt=not conversational, play_ding=True)
    #         # self.recorder.record_audio()

    #         if not self.recorder.speech_detected:
    #             logger.info("No speech detected. Skipping transcription and storage.")
    #             try:
    #                 os.remove(self.recorder.temp_audio_path)
    #                 logger.info(
    #                     f"Deleted temporary audio file: {self.recorder.temp_audio_path}"
    #                 )
    #             except Exception as e:
    #                 logger.error(f"Error deleting temporary audio file: {e}")
    #             return

    #         logger.info("üì• Audio recording completed. Starting transcription...")

    #         for attempt in range(MAX_TRANSCRIPTION_RETRIES):
    #             try:
    #                 text, language = self.transcriber.transcribe_audio(
    #                     self.recorder.temp_audio_path
    #                 )
    #                 break  # success
    #             except ValueError as e:
    #                 logger.warning(f"Attempt {attempt+1}: {e}")
    #                 if attempt == MAX_TRANSCRIPTION_RETRIES - 1:
    #                     logger.info(
    #                         "‚ùå Failed to transcribe clearly after retries. Skipping."
    #                     )
    #                     return
    #                 else:
    #                     time.sleep(1)

    #         logger.info(f"‚úÖ Transcription completed. Detected language: {language}")
    #         logger.info("‚úÖ Storing voice instruction in the database...")
    #         self.storage.store_instruction(self.session_id, language, text)
    #         logger.info("‚úÖ Voice instruction captured and stored successfully!")

    #     except KeyboardInterrupt:
    #         logger.info("Voice capture process interrupted by user.")
    #     except ValueError as e:
    #         logger.info(f"üìå Skipping transcription: {e}")
    #         return
    #     except Exception as e:
    #         logger.error(f"Error in voice capture process: {e}")

    def capture_voice(self, conversational: bool = True) -> Optional[Tuple[str, str]]:
        try:
            # logger.info("üü† Starting voice capture process...")
            self.recorder.record_audio(speak_prompt=not conversational, play_ding=True)

            if not self.recorder.speech_detected:
                logger.info("üü° No speech detected. Skipping transcription.")
                try:
                    os.remove(self.recorder.temp_audio_path)
                    logger.info(
                        f"‚úÖ Deleted temporary audio file: {self.recorder.temp_audio_path}"
                    )
                except Exception as e:
                    logger.error(f"Error deleting temporary audio file: {e}")
                return None

            logger.info("üì• Audio recording completed. Starting transcription...")

            for attempt in range(MAX_TRANSCRIPTION_RETRIES):
                try:
                    text, language = self.transcriber.transcribe_audio(
                        self.recorder.temp_audio_path
                    )
                    break  # Transcription succeeded
                except ValueError as e:
                    logger.warning(f"üü° Attempt {attempt+1}: {e}")
                    if attempt == MAX_TRANSCRIPTION_RETRIES - 1:
                        logger.warning(
                            "‚ùå Failed to transcribe clearly after retries. Skipping."
                        )
                        return None
                    else:
                        time.sleep(1)

            logger.info(f"‚úÖ Transcription completed. Detected language: {language}")
            return text.strip(), language

        except KeyboardInterrupt:
            logger.info("Voice capture process interrupted by user.")
            return None
        except ValueError as e:
            logger.info(f"üìå Skipping transcription: {e}")
            return None
        except Exception as e:
            logger.error(f"Error in voice capture process: {e}")
            return None


if __name__ == "__main__":
    vp = VoiceProcessor()
    vp.capture_voice()



