# modalities/llm_main_chain.py
"""This script implements a voice assistant system for a research lab environment. It integrates
various functionalities such as wake word detection, voice command processing, scene querying,
task execution, and conversational interactions using an LLM (Language Learning Model). The
assistant is designed to interact with users through voice input and provide responses or
execute tasks based on the commands received.
Modules and Functionalities:
- **Wake Word Detection**: Uses Porcupine to detect predefined wake words and trigger the assistant.
- **Voice Command Processing**: Captures voice input, classifies commands, and performs actions
    such as querying a scene, handling general queries, or executing tasks.
- **LLM Integration**: Utilizes LangChain and ChatOllama for natural language understanding,
    reasoning, and generating responses.
- **Scene Querying**: Fetches and formats data from a camera vision database to answer questions
    about the current scene.
- **Task Execution**: Processes task-related commands, stores them in a database, and invokes
    a command processor to handle the task.
- **Persistent Chat Memory**: Maintains a conversation history for context-aware interactions
    and saves it to disk for persistence across sessions.
- **Environment Context**: Provides dynamic context such as weather, time of day, and user roles
    to enhance interactions.
- **Voice Interaction**: Handles conversational interactions, including confirmation for task
    execution and memory reset requests.
- **Greeting Generation**: Dynamically generates personalized greetings based on the time of day
    and user information.
Key Components:
- `VoiceProcessor`: Captures and processes voice input.
- `SpeechSynthesizer`: Converts text responses into speech.
- `SessionManager`: Manages user authentication and session data.
- `CommandProcessor`: Handles task-related commands and processes them.
- `PromptBuilder`: Constructs prompts for LLM-based reasoning and responses.
- `ConversationBufferMemory`: Maintains chat history for context-aware interactions.
Database Integration:
- Fetches user roles, team names, and camera vision data from a database.
- Stores unified instructions and operation sequences for task execution.
- Updates operation triggers in the database for remote vision tasks.
CLI Entry Point:
- Authenticates the user and initializes the assistant.
- Listens for wake words and processes voice commands in a loop.
- Saves chat history and cleans up resources on exit.
Usage:
- Run the script to start the voice assistant.
- Interact with the assistant using voice commands after the wake word is detected.
- Use commands like "reset memory" or "exit" for specific actions.
Note:
- Ensure the required dependencies and database configurations are set up before running the script.
- The script is designed for a specific research lab environment and may require customization
    for other use cases.
"""

import json
import logging
import os
import random
import re
import string
import struct
import threading
import warnings

import ollama
import pvporcupine
import requests
import sounddevice as sd

from datetime import datetime
from pathlib import Path
from typing import Literal, Optional, TypedDict

from langchain.chains import LLMChain
from langchain.memory import ConversationBufferMemory
from langchain.schema import messages_from_dict, messages_to_dict
from langchain_core._api.deprecation import LangChainDeprecationWarning
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda, RunnableSequence
from langchain_ollama import ChatOllama

from config.app_config import CHAT_MEMORY_FOLDER, WAKEWORD_PATH, setup_logging
from config.constants import (
    CANCEL_WORDS,
    CONFIRM_WORDS,
    GENERAL_TRIGGERS,
    QUESTION_WORDS,
    TASK_VERBS,
    TRIGGER_WORDS,
    WAKE_RESPONSES,
)
from mini_project.database.connection import get_connection
from mini_project.modalities.command_processor import CommandProcessor
from mini_project.modalities.prompt_utils import PromptBuilder
from mini_project.modalities.session_manager import SessionManager
from mini_project.modalities.voice_processor import (
    SpeechSynthesizer,
    VoiceProcessor,
)



# ========== Wake Word Setup ==========
ACCESS_KEY = os.getenv("PICOVOICE_ACCESS_KEY")
porcupine = pvporcupine.create(
    access_key=ACCESS_KEY,
    keywords=[
        "jarvis",
        "computer",  # Built-in wake word
    ],
    keyword_paths=[WAKEWORD_PATH],
)
wake_word_triggered = threading.Event()

CHAT_MEMORY_FOLDER.mkdir(parents=True, exist_ok=True)

warnings.filterwarnings("ignore", category=LangChainDeprecationWarning)

# === Logging Config ==========
logging.getLogger("comtypes").setLevel(logging.WARNING)
logging.getLogger("faster_whisper").setLevel(logging.WARNING)
logger = logging.getLogger("VoiceAssistant")
trigger_logger = logging.getLogger("LLMTrigger")

# === Configuration ==========
OLLAMA_MODEL = "llama3.2:latest"
voice_speed = 180  # 165


# === LangChain Setup ==========
llm = ChatOllama(model=OLLAMA_MODEL)
memory = ConversationBufferMemory(memory_key="chat_history", input_key="question")
prompt = PromptBuilder.scene_prompt_template()


# Helper to load chat history into the prompt input
def load_memory(inputs: dict) -> dict:
    chat_history = memory.load_memory_variables({})["chat_history"]
    return {**inputs, "chat_history": chat_history}


# Save original input for memory context
def build_chain_with_input(input_data):
    return (
        RunnableLambda(load_memory)
        | prompt
        | llm
        | RunnableLambda(lambda output: save_memory_with_input(input_data, output))
    )


# Save memory context
def save_memory_with_input(original_input, output):
    memory.save_context(
        {"question": original_input["question"]},
        {"answer": output.content},
    )
    return output


chain = (
    {
        # "input": RunnableLambda(load_memory),
        "original_input": lambda x: x,  # pass-through original input
    }
    | RunnableLambda(lambda d: d["input"])  # continue with prompt | llm
    | prompt
    | llm
    | RunnableLambda(
        lambda output, inputs: {"input": inputs["original_input"], "output": output}
    )
    | RunnableLambda(save_memory_with_input)
)


# === Command Classification ===
def classify_command(
    command_text: str, llm
) -> Literal["general", "scene", "task", "trigger"]:
    lowered = command_text.lower().strip()

    # === Rule-based classification ===
    # === Rule-based check for "trigger" ===
    if any(trigger in lowered for trigger in TRIGGER_WORDS):
        return "trigger"

    # === Rule-based check for "general" ===
    if any(word in command_text.lower() for word in GENERAL_TRIGGERS):
        return "general"

    #  === Try LLM-based classification ===
    try:
        classification_chain = LLMChain(
            llm=llm, prompt=PromptBuilder.classify_command_prompt()
        )
        result = classification_chain.invoke({"command": command_text})
        classification = result.get("text", "").strip().lower()
        if classification in {"general", "scene", "task", "trigger"}:
            return classification
    except Exception as e:
        logger.warning(f"[‚ö†Ô∏è] LLM failed, using rule-based fallback: {e}")

    # === Rule-based fallback ===
    if any(lowered.startswith(q) for q in QUESTION_WORDS):
        return "scene"

    if re.search(r"\b(is|are|how many|what|which|where|who)\b", lowered):
        return "scene"

    if any(trigger in lowered for trigger in TRIGGER_WORDS):
        return "trigger"

    if any(verb in lowered for verb in TASK_VERBS):
        return "task"

    return "task"


# === Persistent Chat Memory ===
def save_chat_history():
    try:
        serialized = messages_to_dict(memory.chat_memory.messages)
        with open(CHAT_MEMORY_PATH, "w", encoding="utf-8") as f:
            json.dump(serialized, f, indent=2)
        logger.info("üíæ Chat history saved.")
    except Exception as e:
        logger.error(f"Failed to save chat memory: {e}")


def load_chat_history():
    if CHAT_MEMORY_PATH.exists():
        try:
            with open(CHAT_MEMORY_PATH, "r", encoding="utf-8") as f:
                raw_messages = json.load(f)
            memory.chat_memory.messages = messages_from_dict(raw_messages)
            logger.info("üß† Loaded past chat history.")
        except Exception as e:
            logger.warning(f"Could not load chat memory: {e}")


def get_user_roles():
    conn = get_connection()
    cursor = conn.cursor()

    cursor.execute(
        """
        SELECT first_name, role
        FROM users
    """
    )
    result = cursor.fetchall()
    conn.close()

    return {name: role for name, role in result}


def get_team_names():
    conn = get_connection()
    cursor = conn.cursor()

    cursor.execute("SELECT first_name FROM users WHERE role = 'team'")
    names = [row[0] for row in cursor.fetchall()]
    conn.close()

    return names


def handle_general_query(command_text: str, llm, user: Optional[dict] = None) -> str:
    if user is None:
        session = SessionManager()
        user = session.authenticated_user
    first_name = user["first_name"]
    liu_id = user["liu_id"]
    role = user.get("role", "guest")

    team_names = get_team_names()
    env = get_environment_context()

    chat_history = memory.load_memory_variables({})["chat_history"]

    prompt = PromptBuilder.general_conversation_prompt(
        first_name=first_name,
        liu_id=liu_id,
        role=role,
        team_names=team_names,
        weather=env["weather"],
        part_of_day=env["part_of_day"],
        full_time=env["datetime"],
        chat_history=chat_history,
    )

    chain = LLMChain(llm=llm, prompt=prompt)
    result = chain.invoke({"command": command_text})
    # return result["text"].strip()
    cleaned = result["text"].strip().strip('"').strip("‚Äú‚Äù").strip("'")
    return cleaned


# === Trigger Remote Vision Task ==========
def trigger_remote_vision_task(command_text: str) -> str:
    """
    Uses LLM-based reasoning to match the user's command to a known operation,
    and triggers that operation by updating the DB. Falls back to trigger_keywords.
    """
    conn = get_connection()
    cursor = conn.cursor()

    logger.info("üîç Using LLM to match command: '%s'", command_text)

    try:
        cursor.execute(
            """
            SELECT operation_name, description, trigger_keywords, trigger
            FROM operation_library
            WHERE is_triggerable = TRUE
        """
        )
        rows = cursor.fetchall()

        if not rows:
            return "No triggerable operations are available."

        # === Step 1: Try LLM-based matching using descriptions ===
        options_text = "\n".join(
            f"{op_name}: {desc or '[no description]'}" for op_name, desc, *_ in rows
        )

        llm_chain = LLMChain(llm=llm, prompt=PromptBuilder.match_operation_prompt())
        result = llm_chain.invoke({"command": command_text, "options": options_text})
        matched_operation = result["text"].strip()

        # === Step 2: Check LLM result validity ===
        row_map = {
            op_name: {"trigger": trig, "keywords": kws}
            for op_name, _, kws, trig in rows
        }

        if matched_operation not in row_map:
            logger.warning("‚ùå LLM returned unknown operation: %s", matched_operation)
            matched_operation = None  # fallback to keyword

        # === Step 3: Fallback to keyword match if LLM fails ===
        if not matched_operation:
            lowered = command_text.lower()
            for op_name, _, keywords, is_triggered in rows:
                if keywords and any(k in lowered for k in keywords):
                    matched_operation = op_name
                    break

        if not matched_operation:
            logger.info("‚ùå No matching operation found (LLM + keyword).")
            return "Sorry, I couldn't match any known vision task to your request."

        is_already_triggered = row_map[matched_operation]["trigger"]
        if is_already_triggered:
            logger.info(
                "‚ö†Ô∏è Operation '%s' already triggered. Skipping.", matched_operation
            )
            # return f"The task '{matched_operation}' is already triggered."
            return f"the script '{matched_operation}' is already running."

        # === Step 4: Exclusively trigger this operation ===
        with conn:
            with conn.cursor() as cursor:
                cursor.execute(
                    """
                    UPDATE operation_library
                    SET trigger = FALSE,
                        state = 'idle'
                    WHERE trigger = TRUE OR state = 'triggered'
                    """
                )
                cursor.execute(
                    """
                    UPDATE operation_library
                    SET trigger = TRUE,
                        state = 'triggered',
                        last_triggered = %s
                    WHERE operation_name = %s
                    """,
                    (datetime.now(), matched_operation),
                )

        conn.commit()
        logger.info("‚úÖ Remote task triggered exclusively: %s", matched_operation)
        # return f" Now running the script '{matched_operation}' remotely."
        return f" Now running the detection script remotely."

    except Exception as e:
        logger.error("‚ùå Triggering vision task failed: %s", str(e), exc_info=True)
        return "An error occurred while trying to trigger the remote task."

    finally:
        cursor.close()
        conn.close()


# ========== Scene Description ==========
def fetch_camera_objects() -> list:
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute(
        """
        SELECT object_name, object_color, pos_x, pos_y, pos_z, rot_x, rot_y, rot_z, last_detected, usd_name
        FROM camera_vision
        """
    )
    rows = cursor.fetchall()
    cursor.close()
    conn.close()
    return rows


def format_camera_data(objects: list) -> str:
    return "\n".join(
        f"- {name} ({color}) at ({x:.1f}, {y:.1f}, {z:.1f}) oriented at ({r:.1f}, {p:.1f}, {w:.1f}) last seen at {timestamp}, usd_name: {usd}"
        for name, color, x, y, z, r, p, w, timestamp, usd in objects
    )


# ========== Scene Query ==========
def query_scene(question: str) -> str:
    """
    Queries the current scene based on the provided question and returns a response.
    This function fetches objects visible to the camera, formats the data, and uses
    a chain to process the input and generate a response. If no objects are visible,
    it returns a default message. In case of an error, it logs the error and returns
    a failure message.
    Args:
        question (str): The question to ask about the current scene.
    Returns:
        str: The response to the question, or an error message if the query fails.
    """
    try:
        objects = fetch_camera_objects()
        if not objects:
            return "I can only see the camera. No other objects are currently visible."
        formatted_data = format_camera_data(objects)
        input_data = {"question": question, "data": formatted_data}
        response = build_chain_with_input(input_data).invoke(input_data)

        return response.content
    except Exception as e:
        logger.error("Scene query failed", exc_info=True)
        return "[Scene query failed.]"


# ========== Task Processing ==========
def process_task(command_text: str) -> str:
    """Processes a given command text by storing it in a database, clearing any
    existing operation sequences, and invoking a command processor to handle
    the task.
    Args:
        command_text (str): The command text to be processed.
    Returns:
        str: A message indicating whether the task was successfully planned
        and added or if the task could not be understood.
    Raises:
        Exception: Logs an error and returns a failure message if any
        exception occurs during the process.
    Database Operations:
        - Inserts the command into the `unified_instructions` table with
          relevant metadata.
        - Clears the `operation_sequence` table.
    Notes:
        - Assumes the existence of a `get_connection` function to establish
          a database connection.
        - Assumes a `CommandProcessor` class is available for processing
          commands.
        - Logs errors using the `logging` module.
    """

    try:
        conn = get_connection()
        cursor = conn.cursor()
        cursor.execute(
            """
            INSERT INTO unified_instructions (
                session_id, timestamp, liu_id,
                voice_command, gesture_command, unified_command,
                confidence, processed
            )
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
            RETURNING id;
            """,
            (
                "session_voice_001",
                datetime.now(),
                session.authenticated_user["liu_id"],
                command_text,
                "",
                command_text,
                0.95,
                False,
            ),
        )
        command_id = cursor.fetchone()[0]
        conn.commit()
        cursor.execute("DELETE FROM operation_sequence")
        conn.commit()
        cursor.close()
        conn.close()

        processor = CommandProcessor()
        success, _ = processor.process_command(
            {"id": command_id, "unified_command": command_text}
        )
        processor.close()

        return (
            "Yes! task has successfully been planned."
            if success
            else "Sorry, I couldn't understand you."
        )
    except Exception as e:
        logging.error(f"Task processing failed: {e}")
        return "[Task execution failed.]"


def get_weather_description(latitude=58.41, longitude=15.62) -> str:
    try:
        url = f"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current_weather=true"
        response = requests.get(url, timeout=5)
        response.raise_for_status()
        data = response.json()
        weather = data["current_weather"]
        temperature = round(weather["temperature"])
        description = f"{temperature}¬∞C, wind {weather['windspeed']} km/h"
        return description
    except Exception as e:
        print(f"Weather fetch failed: {e}")
        return "mysterious skies"


def get_environment_context():
    weather = (
        get_weather_description()
    )  # Already returns something like "Partly cloudy, 11¬∞C"
    now = datetime.now()
    hour = now.hour

    if 5 <= hour < 12:
        part_of_day = "morning"
    elif 12 <= hour < 17:
        part_of_day = "afternoon"
    elif 17 <= hour < 21:
        part_of_day = "evening"
    else:
        part_of_day = "night"

    return {
        "weather": weather,
        "part_of_day": part_of_day,
        "datetime": now.strftime(
            "%A, %B %d at %I:%M %p"
        ),  # e.g., "Tuesday, April 18 at 10:30 AM"
    }


# ========== Wake Word Listener ==========
def listen_for_wake_word(vp, tts):
    def callback(indata, frames, time, status):
        try:
            pcm = struct.unpack_from("h" * porcupine.frame_length, bytes(indata))
            keyword_index = porcupine.process(pcm)
            if keyword_index >= 0:
                logger.info("‚úÖ Wake word detected!")
                wake_word_triggered.set()  # ‚úÖ Set flag
        except Exception as e:
            logger.warning(f"Wake word callback error: {e}")

    with sd.RawInputStream(
        samplerate=porcupine.sample_rate,
        blocksize=porcupine.frame_length,
        dtype="int16",
        channels=1,
        callback=callback,
    ):
        logger.info("üéôÔ∏è  üü¢üü¢üü¢ LISTENING FOR WAKE WORD üü¢üü¢üü¢")
        while not wake_word_triggered.is_set():
            sd.sleep(100)  # non-blocking wait


# ========== Greeting ==========
def generate_llm_greeting():
    """
    Generates a dynamic greeting message using a language model (LLM).
    The function constructs a greeting based on the current time of day, day of the week,
    and month. It uses a predefined set of base greetings as inspiration and prompts the
    LLM to generate a creative, short, and voice-friendly message. If the LLM invocation
    fails, a fallback greeting is returned.
    Returns:
        str: A greeting message, either generated by the LLM or a fallback message.
    """
    now = datetime.now()
    weekday = now.strftime("%A")
    month = now.strftime("%B")
    hour = now.hour
    time_of_day = "morning" if hour < 12 else "afternoon" if hour < 18 else "evening"

    base_greetings = [
        f"Good {time_of_day}! Happy {weekday}.",
        f"Hope you're having a great {weekday}!",
        f"Hello and welcome this fine {time_of_day}.",
        f"It's {month} already! Let's get started.",
        f"Hi! What‚Äôs the first thing you'd like me to do this {time_of_day}?",
    ]
    seed = random.choice(base_greetings)

    try:
        prompt = f"""
        You're Yumi, a clever and friendly assistant robot in a research lab at the Product Realization division of Link√∂ping University.
        It's {time_of_day} on a {weekday} in {month}.
        Say one short and creative sentence (under 20 words) suitable for voice use ‚Äî
        a fun robotics fact, quirky comment, or a science-themed greeting.
        Inspiration: '{seed}' ‚Äî but do not repeat it.
        """
        response = llm.invoke(
            [
                PromptBuilder.greeting_system_msg(),
                {"role": "user", "content": prompt},
            ]
        )
        logger.info(f"üì¢ {response.content}")
        return response.content.strip().strip('"‚Äú‚Äù') or seed
    except Exception as e:
        logger.error(f"Greeting failed: {e}")
        return fallback_llm_greeting(seed)


def fallback_llm_greeting(seed_greeting: str) -> str:
    try:
        response = ollama.chat(
            model=OLLAMA_MODEL,
            messages=[
                {
                    "role": "system",
                    "content": "You are a friendly assistant that creates warm spoken greetings.",
                },
                {
                    "role": "user",
                    "content": f"Improve this fallback greeting for voice use: '{seed_greeting}'",
                },
            ],
        )
        return response["message"]["content"].strip().strip('"‚Äú‚Äù')
    except Exception:
        return seed_greeting


# ========== Voice Interaction ==========
def voice_to_scene_response(
    vp: VoiceProcessor, tts: SpeechSynthesizer, conversational: bool = True
):
    """
    Processes voice input to generate a response, either by querying a scene or planning a task.
    This function captures voice input, classifies the command, and performs actions based on the
    command type. It supports conversational interactions, handles memory reset requests, and
    confirms task execution before proceeding.
    Args:
        vp (VoiceProcessor): The voice processor instance used to capture and process voice input.
        tts (SpeechSynthesizer): The text-to-speech synthesizer instance used to provide audio feedback.
        conversational (bool, optional): Indicates whether the voice capture should be conversational.
                                          Defaults to True.
    Behavior:
        - Captures voice input and processes it.
        - Handles specific commands like "exit", "reset memory", or "clear memory".
        - Classifies the command type (e.g., "scene" or "task").
        - Queries a scene or plans a task based on the command type.
        - Confirms task execution with the user before proceeding.
        - Provides audio feedback for all interactions.
    Returns:
        None
    """
    result = vp.capture_voice(conversational=conversational)
    if result is None:
        logger.info(f"üü° No speech detected. Try again.")
        tts.speak("I didn't catch that. Could you please repeat?")
        return

    request, lang = result
    logger.info(f"üéØ You said: {request}")

    cmd_type = classify_command(request, llm)
    logger.info(f"üß† Command classified as: {cmd_type}")

    if request.lower() in {"exit", "quit", "goodbye", "stop"}:
        tts.speak("Okay, goodbye!")
        exit(0)
    if request.lower() in {"reset memory", "clear memory"}:
        memory.clear()
        CHAT_MEMORY_PATH.unlink(missing_ok=True)

        tts.speak("Memory has been reset.")
        return

    logger.info(f"ü§ñ Thinking...")

    if cmd_type == "trigger":
        answer = trigger_remote_vision_task(request)
        logger.info(f"üß† Trigger Result: {answer}")
        tts.speak(answer)
        return

    elif cmd_type == "scene":
        answer = query_scene(request)
        logger.info(f"ü§ñ (Scene Response): {answer}")

    elif cmd_type == "general":
        answer = handle_general_query(request, llm, user)
        logger.info(f"ü§ñ (General Response): {answer}")

    else:  # task
        # Confirm before executing
        tts.speak("Should I go ahead with a task plan?")
        confirm_result = vp.capture_voice()
        if confirm_result is None:
            tts.speak("No confirmation heard. Skipping the task.")
            return

        confirmation, _ = confirm_result
        # Clean and normalize response
        cleaned = confirmation.lower().translate(
            str.maketrans("", "", string.punctuation)
        )
        # Match negative intent first
        if any(word in cleaned for word in CANCEL_WORDS):
            tts.speak("Okay, discarding the task.")
            return

        # Match positive intent
        if any(word in cleaned for word in CONFIRM_WORDS):
            tts.speak("Okay, give me a second...")

            # Only now store it in the database
            vp.storage.store_instruction(vp.session_id, lang, request)
            answer = process_task(request)
            print("ü§ñ (Task Response):", answer)
        else:
            tts.speak(
                "I wasn't sure what you meant. Could you please repeat your confirmation?"
            )
            retry_result = vp.capture_voice()
            if retry_result is None:
                tts.speak("Still couldn't hear you. Skipping the task.")
                return

            confirmation_retry, _ = retry_result
            cleaned_retry = confirmation_retry.lower().translate(
                str.maketrans("", "", string.punctuation)
            )

            if any(word in cleaned_retry for word in CONFIRM_WORDS):
                tts.speak("Okay, planning task...")
                vp.storage.store_instruction(vp.session_id, lang, request)
                answer = process_task(request)
                print("ü§ñ (Task Response):", answer)
            else:
                tts.speak("No confirmation. Skipping the task.")
    tts.speak(answer)


# ========== CLI Entry Point ==========
if __name__ == "__main__":

    setup_logging()
    vp = VoiceProcessor()
    tts = SpeechSynthesizer()

    # ==========Authenticate user==========
    session = SessionManager()
    user = session.authenticate_user()

    if not user:
        logger.info(f"üî¥ Authentication failed.")
        tts.speak("Authentication was aborted or failed. Goodbye..")
        exit()

    liu_id = user["liu_id"]
    first_name = user["first_name"]
    last_name = user["last_name"]

    # ==========Load personalized memory==========

    CHAT_MEMORY_PATH = CHAT_MEMORY_FOLDER / f"chat_memory_{liu_id}.json"
    load_chat_history()

    # ==========Greet user==========
    if not hasattr(voice_to_scene_response, "greeted"):
        greeting = generate_llm_greeting()
        tts.speak(greeting)

        greeting = f"Hello {first_name}. How can I assist you today?"
        tts.speak(greeting)

        voice_to_scene_response.greeted = True

    first_turn = True

    try:
        while True:
            wake_word_triggered.clear()  # reset flag
            listen_for_wake_word(vp, tts)  # blocks until flag is set
            tts.speak(random.choice(WAKE_RESPONSES))
            voice_to_scene_response(vp, tts, conversational=True)
            save_chat_history()
            first_turn = False
            logger.info(f"üü° Listening again in a few seconds...")

    except KeyboardInterrupt:
        logger.info("üëã Exiting session by user (Ctrl+C).")

    finally:
        if CHAT_MEMORY_PATH.exists():
            CHAT_MEMORY_PATH.unlink()
            logger.info("üßπ Deleted chat memory on exit.")



# modalities/command_processor.py
"""CommandProcessor Class
This class is responsible for processing unified commands for robotic tasks. It interacts with a database to fetch
and validate data, uses an LLM (Large Language Model) for inference and classification, and generates operation
sequences based on the provided commands.
Attributes:
    conn (psycopg2.connection): Database connection object.
    cursor (psycopg2.cursor): Cursor for executing database queries.
    logger (logging.Logger): Logger instance for logging messages.
    llm_model (str): The LLM model used for inference.
    available_sequences (List[str]): Cached list of available sequences from the database.
    available_objects (List[str]): Cached list of available objects from the database.
Methods:
    __init__(llm_model: str = OLLAMA_MODEL):
        Initializes the CommandProcessor instance, sets up database connection, and caches available sequences and objects.
    get_available_sequences() -> List[str]:
        Fetches available sequence names from the sequence_library table in the database.
    fetch_column(table: str, column: str) -> list:
        Fetches a specific column from a given table in the database.
    get_available_objects() -> List[str]:
        Fetches available object names from the camera_vision table in the database.
    get_unprocessed_unified_command() -> Dict:
        Retrieves the latest unprocessed unified command from the unified_instructions table.
    get_available_objects_with_colors() -> List[str]:
        Fetches object names along with their colors from the camera_vision table.
    get_sort_order() -> List[str]:
        Fetches the sort order of objects from the sort_order table.
    get_task_templates() -> Dict[str, List[str]]:
        Fetches task templates from the task_templates table.
    validate_operation(operation: Dict) -> bool:
        Validates the structure of an operation and checks if the sequence and object names are valid.
    extract_json_array(raw_response: str) -> List[Dict]:
        Extracts a JSON array from a raw LLM response string.
    infer_operation_name_from_llm(command_text: str) -> str:
        Infers the operation name from a user command using the LLM.
    get_task_order(operation_name: str) -> List[str]:
        Retrieves the task order for a given operation name from the operation_library table.
    generate_operations_from_sort_order(task_order: List[str], command_id: int) -> List[Dict]:
        Generates a list of operations based on the task order and sort order.
    process_command(unified_command: Dict) -> Tuple[bool, List[Dict]]:
        Processes a unified command, generates operations, and updates the database.
    populate_operation_parameters():
        Populates operation-specific parameter tables in the database based on the planned sequences.
    extract_sort_order_from_llm(command_text: str) -> List[Tuple[str, str]]:
        Extracts the sort order of objects from a user command using the LLM.
    populate_sort_order_from_llm(command_text: str) -> None:
        Populates the sort_order table in the database based on the extracted sort order from the LLM.
    run_processing_cycle():
        Processes the latest unprocessed unified command.
    close():
        Closes the database connection.
Usage:
    This class is designed to be used as a command processor for robotic tasks. It integrates with a database
    and an LLM to process commands, validate operations, and generate operation sequences. The `run_processing_cycle`
    method can be used to process the latest unprocessed command in a single cycle.
"""

import atexit
import json
import logging
import os
from collections import defaultdict, deque
from typing import Dict, List, Tuple

import ollama
import psycopg2
from psycopg2 import Error as Psycopg2Error
from psycopg2 import sql
from psycopg2.extras import DictCursor

from config.app_config import setup_logging
from mini_project.database.connection import get_connection
from mini_project.modalities.prompt_utils import PromptBuilder

# === Logging Setup ===
debug_mode = os.getenv("DEBUG", "0") in ["1", "true", "True"]
log_level = os.getenv("LOG_LEVEL", "DEBUG" if debug_mode else "INFO").upper()
setup_logging(level=getattr(logging, log_level))
logger = logging.getLogger("CommandProcess")

# models: "llama3.2:1b", "deepseek-r1:1.5b", "mistral:latest", "deepseek-r1:32b"
OLLAMA_MODEL = "mistral:latest"


class CommandProcessor:

    def __init__(self, llm_model: str = OLLAMA_MODEL):
        self.conn = get_connection()
        self.cursor = self.conn.cursor(cursor_factory=DictCursor)
        self.logger = logger
        self.llm_model = llm_model

        # Cache available sequences and objects from database for validation purposes
        self.available_sequences = self.get_available_sequences()
        self.available_objects = self.get_available_objects()

    def get_available_sequences(self) -> List[str]:
        """Fetch available sequence names from sequence_library in database"""
        self.cursor.execute("SELECT sequence_name FROM sequence_library")
        available_sequences = [row[0] for row in self.cursor.fetchall()]
        logger.info(f"üü¢ Available sequences: {available_sequences}")
        return available_sequences

    def fetch_column(self, table: str, column: str) -> list:
        try:
            query = sql.SQL("SELECT {field} FROM {tbl}").format(
                field=sql.Identifier(column), tbl=sql.Identifier(table)
            )
            self.cursor.execute(query)
            return [row[0] for row in self.cursor.fetchall()]
        except Psycopg2Error as e:
            logger.error("Database error in fetch_column: %s", str(e), exc_info=True)
            raise

    def get_available_objects(self) -> List[str]:
        self.cursor.execute("SELECT object_name FROM camera_vision")
        available_objects = [row[0] for row in self.cursor.fetchall()]
        logger.info(f"üü¢ Available objects: {available_objects}")
        return available_objects

    def get_unprocessed_unified_command(self) -> Dict:
        self.cursor.execute(
            """
            SELECT id, unified_command FROM unified_instructions
            WHERE processed = FALSE ORDER BY id DESC LIMIT 1
        """
        )
        result = self.cursor.fetchone()
        return (
            {"id": result["id"], "unified_command": result["unified_command"]}
            if result
            else None
        )

    def get_available_objects_with_colors(self) -> List[str]:
        self.cursor.execute("SELECT object_name, object_color FROM camera_vision")
        return [f"{name} ({color})" for name, color in self.cursor.fetchall()]

    def get_sort_order(self) -> List[str]:
        self.cursor.execute(
            "SELECT object_name, object_color FROM sort_order ORDER BY order_id ASC"
        )
        return [f"{name} ({color})" for name, color in self.cursor.fetchall()]

    def get_task_templates(self) -> Dict[str, List[str]]:
        self.cursor.execute("SELECT task_name, default_sequence FROM task_templates")
        return {row[0]: row[1] for row in self.cursor.fetchall()}

    def refresh_cache(self):
        """Refreshes cached sequences and objects from the database."""
        self.available_sequences = self.get_available_sequences()
        self.available_objects = self.get_available_objects()
        logger.info("üü¢ Cache refreshed: sequences and objects updated.")

    def validate_operation(self, operation: Dict) -> bool:
        """Validate operation structure"""
        self.refresh_cache()  # Ensure we're using the latest data

        if operation["sequence_name"] not in self.available_sequences:
            logger.error(f"Invalid sequence name: {operation['sequence_name']}")
            return False

        # Allow empty object names
        if not operation.get("object_name", ""):
            return True

        if operation["object_name"] in self.available_objects:
            return True
        else:
            logger.error(f"Invalid object name: {operation['object_name']}")
            return False

    def extract_json_array(self, raw_response: str) -> List[Dict]:
        import re

        try:
            if not PromptBuilder.validate_llm_json(raw_response):
                raise ValueError(
                    "Invalid JSON format: response must start with [ and end with ]"
                )

            # Match the first complete JSON array only
            match = re.search(r"\[\s*{[\s\S]*?}\s*]", raw_response)
            if not match:
                raise ValueError("No valid JSON array found in LLM response.")

            json_str = match.group(0)
            return json.loads(json_str)

        except Exception as e:
            logger.error(
                "Failed to extract JSON array from LLM response: %s", e, exc_info=True
            )
            raise

    def infer_operation_name_from_llm(self, command_text: str) -> str:
        self.cursor.execute("SELECT operation_name FROM operation_library")
        available_operations = [row[0] for row in self.cursor.fetchall()]

        prompt = f"""
        Given the following user command:

        "{command_text}"

        Choose the most appropriate operation name from the list:
        {', '.join(available_operations)}

        Only respond with the exact operation_name string ‚Äî no extra words or explanation.
        """

        response = ollama.chat(
            model=self.llm_model,
            messages=[
                {
                    "role": "system",
                    "content": "You are a smart classifier for robotic task types.",
                },
                {"role": "user", "content": prompt},
            ],
        )
        result = response["message"]["content"].strip()

        if result not in available_operations:
            raise ValueError(f"LLM returned invalid operation_name: {result}")

        return result

    def get_task_order(self, operation_name: str) -> List[str]:
        self.cursor.execute(
            "SELECT task_order FROM operation_library WHERE operation_name = %s",
            (operation_name,),
        )
        row = self.cursor.fetchone()
        return [s.strip() for s in row[0].split(",")] if row else []

    def generate_operations_from_sort_order(
        self, task_order: List[str], command_id: int
    ) -> List[Dict]:
        self.cursor.execute("SELECT object_name FROM sort_order ORDER BY order_id")
        objects = [row[0] for row in self.cursor.fetchall()]

        self.cursor.execute(
            "SELECT COALESCE(MAX(operation_id), 0) + 1 FROM operation_sequence"
        )
        operation_id_start = self.cursor.fetchone()[0]

        ops = []
        idx = 0

        for obj in objects:
            for seq in task_order:
                self.cursor.execute(
                    "SELECT sequence_id FROM sequence_library WHERE sequence_name = %s",
                    (seq,),
                )
                seq_id_row = self.cursor.fetchone()
                if not seq_id_row:
                    continue

                ops.append(
                    {
                        "operation_id": operation_id_start + idx,
                        "sequence_id": seq_id_row[0],
                        "sequence_name": seq,
                        "object_name": obj,
                        "command_id": command_id,
                    }
                )
                idx += 1

        # Optionally add a final "go_home"
        self.cursor.execute(
            "SELECT sequence_id FROM sequence_library WHERE sequence_name = 'go_home'"
        )
        go_home_seq = self.cursor.fetchone()
        if go_home_seq:
            ops.append(
                {
                    "operation_id": operation_id_start + idx,
                    "sequence_id": go_home_seq[0],
                    "sequence_name": "go_home",
                    "object_name": "",
                    "command_id": command_id,
                }
            )

        return ops

    def process_command(self, unified_command: Dict) -> Tuple[bool, List[Dict]]:
        try:
            self.populate_sort_order_from_llm(unified_command["unified_command"])

            operation_name = self.infer_operation_name_from_llm(
                unified_command["unified_command"]
            )
            task_order = self.get_task_order(operation_name)

            operations = self.generate_operations_from_sort_order(
                task_order, unified_command["id"]
            )

            # Wipe old unprocessed
            self.cursor.execute(
                "UPDATE operation_sequence SET processed = TRUE WHERE processed = FALSE"
            )

            # Insert new
            insert_count = 0
            for op in operations:
                self.cursor.execute(
                    """
                    INSERT INTO operation_sequence
                    (operation_id, sequence_id, sequence_name, object_name, command_id)
                    VALUES (%s, %s, %s, %s, %s)
                    """,
                    (
                        op["operation_id"],
                        op["sequence_id"],
                        op["sequence_name"],
                        op["object_name"],
                        op["command_id"],
                    ),
                )
                insert_count += 1
            logger.info(f"‚úÖ Inserted {insert_count} rows into operation_sequence.")

            # Mark as processed
            self.cursor.execute(
                "UPDATE unified_instructions SET processed = TRUE WHERE id = %s",
                (unified_command["id"],),
            )

            # Auto-populate operation parameter tables
            self.populate_operation_parameters()

            self.conn.commit()
            return True, operations

        except Exception as e:
            logger.error("Failed to process command: %s", e, exc_info=True)
            self.conn.rollback()
            return False, []

    def populate_operation_parameters(self):
        logger.info("Populating operation-specific parameters...")

        # Step 1: Get all unique sequence types planned
        self.cursor.execute("SELECT DISTINCT sequence_name FROM operation_sequence")
        sequence_types = [row[0] for row in self.cursor.fetchall()]

        insert_count = 0
        self.cursor.execute(
            "SELECT COUNT(*) FROM camera_vision WHERE usd_name = 'Slide.usd'"
        )
        slide_usd_count = self.cursor.fetchone()[0]
        if slide_usd_count > 0:

            if "pick" in sequence_types:
                self.cursor.execute("DELETE FROM pick_op_parameters")
                self.cursor.execute(
                    "SELECT object_name FROM operation_sequence WHERE sequence_name = 'pick'"
                )
                pick_data = self.cursor.fetchall()
                for i, (obj,) in enumerate(pick_data):
                    self.cursor.execute(
                        """
                        INSERT INTO pick_op_parameters (
                            operation_order, object_id, slide_state_status, slide_direction, distance_travel, operation_status
                        ) VALUES (%s, %s, %s, %s, %s, %s)
                        """,
                        (i + 1, obj, False, "y", 0.01, False),
                    )
                    insert_count += 1
                logger.info(f"‚úÖ Inserted {insert_count} rows into pick_op_parameters.")

            if "travel" in sequence_types:
                self.cursor.execute("DELETE FROM travel_op_parameters")
                self.cursor.execute(
                    "SELECT object_name FROM operation_sequence WHERE sequence_name = 'travel'"
                )
                travel_data = self.cursor.fetchall()
                for i, (obj,) in enumerate(travel_data):
                    self.cursor.execute(
                        """
                        INSERT INTO travel_op_parameters (
                            operation_order, object_id, travel_height, gripper_rotation, operation_status
                        ) VALUES (%s, %s, %s, %s, %s)
                        """,
                        (i + 1, obj, 0.085, "y-axis", False),
                    )
                    insert_count += 1
                logger.info(
                    f"‚úÖ Inserted {insert_count} rows into travel_op_parameters."
                )

            if "drop" in sequence_types:
                self.cursor.execute("DELETE FROM drop_op_parameters")
                self.cursor.execute(
                    "SELECT object_name FROM operation_sequence WHERE sequence_name = 'drop'"
                )
                drop_data = self.cursor.fetchall()
                for i, (obj,) in enumerate(drop_data):
                    self.cursor.execute(
                        """
                        INSERT INTO drop_op_parameters (
                            operation_order, object_id, drop_height, operation_status
                        ) VALUES (%s, %s, %s, %s)
                        """,
                        (i + 1, obj, 0.0, False),
                    )
                    insert_count += 1
                logger.info(f"‚úÖ Inserted {insert_count} rows into drop_op_parameters.")

            if "screw" in sequence_types:
                self.cursor.execute("DELETE FROM screw_op_parameters")
                self.cursor.execute(
                    "SELECT sequence_id, object_name FROM operation_sequence WHERE sequence_name = 'screw'"
                )
                screw_data = self.cursor.fetchall()
                for i, (seq_id, obj) in enumerate(screw_data):
                    self.cursor.execute(
                        """
                        INSERT INTO screw_op_parameters (
                            operation_order, sequence_id, object_id,
                            rotation_dir, number_of_rotations,
                            current_rotation, operation_status
                        ) VALUES (%s, %s, %s, %s, %s, %s, %s)
                        """,
                        (i + 1, seq_id, obj, i % 2 == 0, 3, 0, False),
                    )
                    insert_count += 1
                logger.info(f"Inserted {insert_count} rows into screw_op_parameters.")

            if "screw" in sequence_types:
                self.cursor.execute("DELETE FROM rotate_state_parameters")
                self.cursor.execute(
                    "SELECT sequence_id, operation_order, object_id FROM screw_op_parameters"
                )
                rotate_data = self.cursor.fetchall()
                for seq_id, op_order, obj in rotate_data:
                    self.cursor.execute(
                        """
                        INSERT INTO rotate_state_parameters (
                            sequence_id, operation_order, object_id,
                            rotation_angle, operation_status
                        ) VALUES (%s, %s, %s, %s, %s)
                        """,
                        (seq_id, op_order, obj, 90, False),
                    )
                    insert_count += 1
                logger.info(
                    f"Inserted {insert_count} rows into rotate_state_parameters."
                )
        else:
            if "pick" in sequence_types:
                self.cursor.execute("DELETE FROM pick_op_parameters")
                self.cursor.execute(
                    "SELECT object_name FROM operation_sequence WHERE sequence_name = 'pick'"
                )
                pick_data = self.cursor.fetchall()
                for i, (obj,) in enumerate(pick_data):
                    self.cursor.execute(
                        """
                        INSERT INTO pick_op_parameters (
                            operation_order, object_id, slide_state_status, slide_direction, distance_travel, operation_status
                        ) VALUES (%s, %s, %s, %s, %s, %s)
                        """,
                        (i + 1, obj, False, "y", 0.01, False),
                    )
                    insert_count += 1
                logger.info(f"‚úÖ Inserted {insert_count} rows into pick_op_parameters.")

            if "travel" in sequence_types:
                self.cursor.execute("DELETE FROM travel_op_parameters")
                self.cursor.execute(
                    "SELECT object_name FROM operation_sequence WHERE sequence_name = 'travel'"
                )
                travel_data = self.cursor.fetchall()
                for i, (obj,) in enumerate(travel_data):
                    self.cursor.execute(
                        """
                        INSERT INTO travel_op_parameters (
                            operation_order, object_id, travel_height, gripper_rotation, operation_status
                        ) VALUES (%s, %s, %s, %s, %s)
                        """,
                        (i + 1, obj, 0.085, "z-axis", False),
                    )
                    insert_count += 1
                logger.info(
                    f"‚úÖ Inserted {insert_count} rows into travel_op_parameters."
                )

            # if "drop" in sequence_types:
            #     self.cursor.execute("DELETE FROM drop_op_parameters")
            #     self.cursor.execute(
            #         "SELECT object_name FROM operation_sequence WHERE sequence_name = 'drop'"
            #     )
            #     drop_data = self.cursor.fetchall()
            #     for i, (obj,) in enumerate(drop_data):
            #         self.cursor.execute(
            #             """
            #             INSERT INTO drop_op_parameters (
            #                 operation_order, object_id, drop_height, operation_status
            #             ) VALUES (%s, %s, %s, %s)
            #             """,
            #             (i + 1, obj, 0.0, False),
            #         )
            #         insert_count += 1
            #     logger.info(f"‚úÖ Inserted {insert_count} rows into drop_op_parameters.")

            if "screw" in sequence_types:
                self.cursor.execute("DELETE FROM screw_op_parameters")
                self.cursor.execute(
                    "SELECT sequence_id, object_name FROM operation_sequence WHERE sequence_name = 'screw'"
                )
                screw_data = self.cursor.fetchall()
                for i, (seq_id, obj) in enumerate(screw_data):
                    self.cursor.execute(
                        """
                        INSERT INTO screw_op_parameters (
                            operation_order, sequence_id, object_id,
                            rotation_dir, number_of_rotations,
                            current_rotation, operation_status
                        ) VALUES (%s, %s, %s, %s, %s, %s, %s)
                        """,
                        (i + 1, seq_id, obj, i % 2 == 0, 3, 0, False),
                    )
                    insert_count += 1
                logger.info(f"Inserted {insert_count} rows into screw_op_parameters.")

            if "screw" in sequence_types:
                self.cursor.execute("DELETE FROM rotate_state_parameters")
                self.cursor.execute(
                    "SELECT sequence_id, operation_order, object_id FROM screw_op_parameters"
                )
                rotate_data = self.cursor.fetchall()
                for seq_id, op_order, obj in rotate_data:
                    self.cursor.execute(
                        """
                        INSERT INTO rotate_state_parameters (
                            sequence_id, operation_order, object_id,
                            rotation_angle, operation_status
                        ) VALUES (%s, %s, %s, %s, %s)
                        """,
                        (seq_id, op_order, obj, 90, False),
                    )
                    insert_count += 1
                logger.info(
                    f"Inserted {insert_count} rows into rotate_state_parameters."
                )

        self.conn.commit()
        logger.info("‚úÖ Operation-specific parameter tables updated.")

    def extract_sort_order_from_llm(self, command_text: str) -> List[Tuple[str, str]]:
        try:
            logger.info("üß† Extracting sort order using LLM...")
            prompt = PromptBuilder.sort_order_prompt(command_text)
            response = ollama.chat(
                model=self.llm_model,
                messages=[
                    PromptBuilder.sort_order_system_msg(),
                    {"role": "user", "content": prompt},
                ],
            )

            parsed = self.extract_json_array(response["message"]["content"])
            if not isinstance(parsed, list):
                logger.warning("‚ö†Ô∏è Unexpected format from LLM sort extraction.")
                return []

            results = []
            for item in parsed:
                object_name = (item.get("object_name") or "").strip()
                object_color = (item.get("object_color") or "").strip()
                results.append((object_name, object_color))

            logger.info(f"‚úÖ Extracted sort order: {results}")
            return results

        except Exception as e:
            logger.error(
                "‚ùå Failed to extract sort order using LLM: %s", str(e), exc_info=True
            )
            return []

    def populate_sort_order_from_llm(self, command_text: str) -> None:
        try:
            logger.info("üß† Asking LLM to extract sort order...")

            extracted = self.extract_sort_order_from_llm(command_text)

            if not extracted:
                logger.warning("‚ö†Ô∏è No sort order to insert.")
                return

            # Clear previous sort_order
            self.cursor.execute("DELETE FROM sort_order")

            # Fetch color-to-object_name mapping from camera_vision
            self.cursor.execute("SELECT object_name, object_color FROM camera_vision")
            camera_color_map = self.cursor.fetchall()
            color_to_names = {}
            for name, color in camera_color_map:
                color_to_names.setdefault(color.lower(), []).append(name)

            inserted = []
            for item in extracted:
                color = item[1].lower()
                name_list = color_to_names.get(color, [])
                if not name_list:
                    logger.warning(f"‚ö†Ô∏è No match in camera_vision for color: {color}")
                    continue
                obj_name = name_list.pop(0)  # Use and remove to avoid duplicates
                self.cursor.execute(
                    "INSERT INTO sort_order (object_name, object_color) VALUES (%s, %s)",
                    (obj_name, color),
                )
                inserted.append((obj_name, color))

            logger.info(f"‚úÖ sort_order table populated: {inserted}")

        except Exception as e:
            logger.error(
                "‚ùå Failed to populate sort_order table: %s", str(e), exc_info=True
            )

    def run_processing_cycle(self):
        """
        Process the latest unprocessed unified command
        """
        logger.info("üü¢ Checking for new unified_commands...")
        unified_command = self.get_unprocessed_unified_command()

        if unified_command:
            logger.info(f"Processing command ID: {unified_command['id']}")
            if self.process_command(unified_command):
                logger.info(
                    f"Successfully processed unified_command {unified_command['id']}"
                )
            else:
                logger.error(
                    f"Failed to process unified_command {unified_command['id']}"
                )
        else:
            logger.info("üü° No unprocessed unified_commands found")

    def close(self):
        """Close the persistent SQLite connection."""
        if self.conn:
            self.conn.close()
            self.conn = None
            logger.info("üü¢ Database connection closed.")


if __name__ == "__main__":
    processor = CommandProcessor(llm_model="mistral:latest")
    # Register the close method so it gets called when the program exits
    atexit.register(processor.close)
    processor.run_processing_cycle()



# modalities/voice_processor.py

"""This module provides classes and functionality for voice processing, including
speech synthesis, audio recording, transcription, and storage of voice instructions.
Classes:
    SpeechSynthesizer:
        A singleton class for text-to-speech synthesis using either gTTS or pyttsx3.
        Provides methods to play notification sounds and speak text.
    AudioRecorder:
        Handles audio recording with noise calibration and voice activity detection (VAD).
        Records audio to a temporary file and detects speech presence.
    Transcriber:
        Uses the Whisper model to transcribe audio files into text.
        Supports language detection and translation to English.
    Storage:
        Manages storage of transcribed voice instructions in a PostgreSQL database.
        Handles retries and error handling for database operations.
    VoiceProcessor:
        Orchestrates the voice processing pipeline, including audio recording,
        transcription, and storage. Provides a method to capture voice instructions.
Constants:
    MAX_TRANSCRIPTION_RETRIES:
        Maximum number of retries for transcription in case of failure.
    MIN_DURATION_SEC:
        Minimum duration of a valid audio recording in seconds.
    VOICE_PROCESSING_CONFIG:
        Configuration dictionary for voice processing settings.
    VOICE_TTS_SETTINGS:
        Configuration dictionary for text-to-speech settings.
    WHISPER_LANGUAGE_NAMES:
        Mapping of language codes to language names.
Usage:
    The `VoiceProcessor` class can be used as the main entry point for capturing
    and processing voice instructions. It integrates audio recording, transcription,
    and storage functionalities.
Example:
    result = vp.capture_voice()
    if result:
        text, language = result
        print(f"Transcribed Text: {text}, Language: {language}")

"""

import logging
import os

# import sqlite3
import tempfile
import time
import uuid
from pathlib import Path
from typing import Any, Dict, Optional, Tuple

import numpy as np
import psycopg2
import pyttsx3
import sounddevice as sd
import webrtcvad
from faster_whisper import WhisperModel
from gtts import gTTS
from playsound import playsound
from pluggy import Result
from scipy.io.wavfile import write

from config.app_config import (
    MAX_TRANSCRIPTION_RETRIES,
    MIN_DURATION_SEC,
    VOICE_PROCESSING_CONFIG,
    VOICE_TTS_SETTINGS,
)
from config.constants import WHISPER_LANGUAGE_NAMES
from mini_project.database.connection import get_connection

logging.getLogger("comtypes").setLevel(logging.WARNING)
logger = logging.getLogger("VoiceProcessor")


class SpeechSynthesizer:
    _instance = None  # Singleton instance

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(SpeechSynthesizer, cls).__new__(cls)
            cls._instance._init_engine()
        return cls._instance

    def _init_engine(self):
        self.use_gtts = VOICE_TTS_SETTINGS["use_gtts"]
        self.voice_speed = VOICE_TTS_SETTINGS["speed"]
        self.ping_path = Path(VOICE_TTS_SETTINGS["ping_sound_path"]).resolve()
        self.ding_path = Path(VOICE_TTS_SETTINGS["ding_sound_path"]).resolve()
        self.voice_index = VOICE_TTS_SETTINGS.get("voice_index", 1)

        if not self.use_gtts:
            try:
                self.engine = pyttsx3.init()
                voices = self.engine.getProperty("voices")
                self.engine.setProperty("rate", self.voice_speed)
                self.engine.setProperty("voice", voices[self.voice_index].id)
            except Exception as e:
                logger.error(f"[TTS] Error initializing pyttsx3: {e}")

    def play_ping(self):
        try:
            if not self.ping_path.exists():
                raise FileNotFoundError(f"Ping sound file not found: {self.ping_path}")
            # playsound(str(self.ping_path))
        except Exception as e:
            logger.warning(f"[Ping Sound] Failed to play: {e}")

    def play_ding(self):
        try:
            if not self.ding_path.exists():
                raise FileNotFoundError(f"Ding sound file not found: {self.ding_path}")
            playsound(str(self.ding_path))
        except Exception as e:
            logger.warning(f"[Ding Sound] Failed to play: {e}")

    def speak(self, text: str):
        if self.use_gtts:
            try:
                temp_path = (
                    Path(tempfile.gettempdir()) / f"speech_{uuid.uuid4().hex}.mp3"
                )
                gTTS(text=text).save(temp_path)
                playsound(str(temp_path))
                os.remove(temp_path)
            except Exception as e:
                logger.error(f"[TTS:gTTS] Error: {e}")
                print(f"[TTS Fallback] {text}")
        else:
            try:
                self.engine.say(text)
                self.engine.runAndWait()
            except Exception as e:
                logger.error(f"[TTS:pyttsx3] Error during speech: {e}")
                print(f"[TTS Fallback] {text}")


class AudioRecorder:

    def __init__(self, synthesizer: Optional[SpeechSynthesizer] = None) -> None:
        self.synthesizer = synthesizer or SpeechSynthesizer()
        self.config: Dict[str, Any] = VOICE_PROCESSING_CONFIG["recording"]
        self.temp_audio_path: str = self.config["temp_audio_path"]
        self.sampling_rate: int = self.config["sampling_rate"]
        self.vad = webrtcvad.Vad()
        self.vad.set_mode(3)
        self.speech_detected = False
        self.noise_floor: Optional[float] = None
        self.calibrated = False

        if not isinstance(self.sampling_rate, int) or self.sampling_rate <= 0:
            raise ValueError("Sampling rate must be a positive integer.")
        if not isinstance(self.temp_audio_path, str):
            raise ValueError("Temp audio path must be a string.")

    def calibrate_noise(self) -> float:
        if not self.calibrated:
            logger.info("‚úÖ Calibrating ambient noise...")
        noise_rms_values: list[float] = []
        stream = sd.InputStream(
            samplerate=self.sampling_rate, channels=1, dtype="int16"
        )
        end_time = time.time() + self.config["calibration_duration"]
        with stream:
            while time.time() < end_time:
                frame, _ = stream.read(
                    int(self.sampling_rate * self.config["frame_duration"])
                )
                rms = np.sqrt(np.mean(frame.astype(np.float32) ** 2))
                noise_rms_values.append(rms)
        noise_floor = np.mean(noise_rms_values)
        if not self.calibrated:
            logger.info(
                f"‚úÖ Ambient noise calibration complete. Noise floor: {noise_floor:.2f}"
            )
        return noise_floor

    def record_audio(self, speak_prompt: bool = False, play_ding: bool = True) -> None:
        # üî∏ Use cached noise floor if available
        if self.noise_floor is None:
            self.noise_floor = self.calibrate_noise()
            if not self.calibrated:
                logger.info(
                    f"‚úÖ Amplitude threshold set to: {self.noise_floor + self.config['amplitude_margin']:.2f} (Noise floor: {self.noise_floor:.2f} + Margin: {self.config['amplitude_margin']})"
                )
                self.calibrated = True

        amplitude_threshold = self.noise_floor + self.config["amplitude_margin"]
        logger.info(
            f"‚úÖ Amplitude threshold set to: {amplitude_threshold:.2f} (Noise floor: {self.noise_floor:.2f} + Margin: {self.config['amplitude_margin']})"
        )

        # üó£Ô∏è Speak the instruction aloud
        if speak_prompt:
            try:
                self.synthesizer.speak("Tell me, how can I help you?")
            except Exception as e:
                logger.warning(f"[Recorder] Failed to speak instruction: {e}")

        logger.info("üì¢ Voice recording: Speak now...üü¢üü¢üü¢")

        # üîî Play ding sound immediately after prompt
        if play_ding:
            try:
                self.synthesizer.play_ding()
            except Exception as e:
                logger.warning(f"[Recorder] Failed to play ding: {e}")

        # logger.info("üü¢ Listening...")

        audio = []
        start_time = time.time()
        silence_start: Optional[float] = None
        self.speech_detected = False

        stream = sd.InputStream(
            samplerate=self.sampling_rate, channels=1, dtype="int16"
        )
        with stream:
            while True:
                frame, _ = stream.read(
                    int(self.sampling_rate * self.config["frame_duration"])
                )
                audio.append(frame)
                is_speech_vad = self.vad.is_speech(frame.tobytes(), self.sampling_rate)
                rms = np.sqrt(np.mean(frame.astype(np.float32) ** 2))
                is_speech_amplitude = rms > amplitude_threshold
                logger.debug(
                    f"VAD: {is_speech_vad}, RMS: {rms:.2f}, Amplitude: {is_speech_amplitude}"
                )
                if is_speech_vad and is_speech_amplitude:
                    self.speech_detected = True
                    silence_start = None
                elif silence_start is None:
                    silence_start = time.time()
                else:
                    threshold = (
                        self.config["post_speech_silence_duration"]
                        if self.speech_detected
                        else self.config["initial_silence_duration"]
                    )
                    if time.time() - silence_start > threshold:
                        break
                if time.time() - start_time > self.config["max_duration"]:
                    break
        audio = np.concatenate(audio, axis=0)
        duration = time.time() - start_time
        logger.info(f"üü¢ Recording completed. Duration: {duration:.2f} seconds")
        if duration < MIN_DURATION_SEC:
            logger.warning(
                f"Recording too short ({duration:.2f}s). Treating as no speech."
            )
            self.speech_detected = False  # suppress further processing
            return

        try:
            write(self.temp_audio_path, self.sampling_rate, audio)
            logger.info(f"üü¢ Audio saved to {self.temp_audio_path}")
        except Exception as e:
            logger.error(f"Error saving audio: {e}")
            raise


class Transcriber:

    def __init__(self) -> None:
        self.config: Dict[str, Any] = VOICE_PROCESSING_CONFIG["whisper"]
        self.model = WhisperModel(
            self.config["model"],
            device=self.config["device"],
            compute_type=self.config["compute_type"],
        )

    # =================== only store transcribed_text and language ‚Äî
    # =================== which means you're storing Swedish text if the speaker used Swedish.

    # def transcribe_audio(self, audio_path: str) -> Tuple[str, str]:
    #     try:
    #         segments, info = self.model.transcribe(audio_path)
    #         original_text = " ".join([segment.text for segment in segments])
    #         detected_language = info.language
    #         return original_text, detected_language
    #     except FileNotFoundError as e:
    #         logger.error(f"Audio file not found: {audio_path}")
    #         raise
    #     except RuntimeError as e:
    #         logger.error(f"Error loading Whisper model: {e}")
    #         raise
    #     except Exception as e:
    #         logger.error(f"Unexpected error during transcription: {e}")
    #         raise

    # =================== Keeps info.language accurate (auto-detected language)
    # =================== Transcribes as English, no matter the input language

    def transcribe_audio(self, audio_path: str) -> Tuple[str, str]:
        try:
            # Transcribe with forced translation to English
            segments, info = self.model.transcribe(audio_path, beam_size=5)
            detected_language = info.language
            language_prob = info.language_probability

            if language_prob < 0.3:
                logger.warning(
                    f"üî¥ Low language detection confidence: {language_prob:.2f} for '{detected_language}'"
                )
                raise ValueError("Unclear speech or unsupported language.")

            if detected_language != "en":
                # Re-run with translation
                segments, _ = self.model.transcribe(
                    audio_path, task="translate", beam_size=5
                )

            original_text = " ".join([segment.text for segment in segments])
            if not original_text.strip():
                raise ValueError("No intelligible speech detected.")

            language = WHISPER_LANGUAGE_NAMES.get(detected_language, detected_language)
            return original_text, language

        except FileNotFoundError as e:
            logger.error(f"Audio file not found: {audio_path}")
            raise
        except RuntimeError as e:
            logger.error(f"Error loading Whisper model: {e}")
            raise
        except Exception as e:
            logger.warning(f"üî¥ Unexpected error during transcription: {e}")
            raise


class Storage:
    def __init__(self) -> None:
        self.config: Dict[str, Any] = VOICE_PROCESSING_CONFIG["database"]
        self.db_path: str = self.config["db_path"]
        self.check_database()

    def check_database(self) -> None:
        pass  # PostgreSQL always exists; no need to create .db file

    def store_instruction(
        self,
        session_id: str,
        detected_language: str,
        transcribed_text: str,
        retries: int = 3,
        delay: float = 1.0,
    ) -> None:
        for attempt in range(retries):
            try:
                conn = get_connection()
                with conn:
                    with conn.cursor() as cursor:
                        cursor.execute(
                            """
                            INSERT INTO voice_instructions (session_id, transcribed_text, language)
                            VALUES (%s, %s, %s)
                            """,
                            (session_id, transcribed_text, detected_language),
                        )
                logger.info(
                    "‚úÖ Voice instruction stored successfully in voice_instructions table."
                )
                return
            except psycopg2.OperationalError as e:
                if "could not connect" in str(e).lower() and attempt < retries - 1:
                    logger.warning(
                        f"Database connection error. Retrying in {delay} seconds..."
                    )
                    time.sleep(delay)
                else:
                    logger.error(f"[PostgreSQL] Operational error: {e}")
                    raise
            except Exception as e:
                logger.error(
                    f"[PostgreSQL] Unexpected error storing voice instruction: {e}"
                )
                raise


class VoiceProcessor:
    def __init__(self, session_id: Optional[str] = None) -> None:
        self.recorder = AudioRecorder()
        self.transcriber = Transcriber()
        self.storage = Storage()
        self.session_id = session_id or str(uuid.uuid4())
        self.synthesizer = SpeechSynthesizer()
        self.recorder = AudioRecorder(self.synthesizer)

    # def capture_voice(self, conversational: bool = True) -> -> Optional[Tuple[str, str]]:
    #     try:
    #         logger.info("üü† Starting voice capture process...")
    #          # conversational = True ‚ûù just play ding
    #         self.recorder.record_audio(speak_prompt=not conversational, play_ding=True)
    #         # self.recorder.record_audio()

    #         if not self.recorder.speech_detected:
    #             logger.info("No speech detected. Skipping transcription and storage.")
    #             try:
    #                 os.remove(self.recorder.temp_audio_path)
    #                 logger.info(
    #                     f"Deleted temporary audio file: {self.recorder.temp_audio_path}"
    #                 )
    #             except Exception as e:
    #                 logger.error(f"Error deleting temporary audio file: {e}")
    #             return

    #         logger.info("üì• Audio recording completed. Starting transcription...")

    #         for attempt in range(MAX_TRANSCRIPTION_RETRIES):
    #             try:
    #                 text, language = self.transcriber.transcribe_audio(
    #                     self.recorder.temp_audio_path
    #                 )
    #                 break  # success
    #             except ValueError as e:
    #                 logger.warning(f"Attempt {attempt+1}: {e}")
    #                 if attempt == MAX_TRANSCRIPTION_RETRIES - 1:
    #                     logger.info(
    #                         "‚ùå Failed to transcribe clearly after retries. Skipping."
    #                     )
    #                     return
    #                 else:
    #                     time.sleep(1)

    #         logger.info(f"‚úÖ Transcription completed. Detected language: {language}")
    #         logger.info("‚úÖ Storing voice instruction in the database...")
    #         self.storage.store_instruction(self.session_id, language, text)
    #         logger.info("‚úÖ Voice instruction captured and stored successfully!")

    #     except KeyboardInterrupt:
    #         logger.info("Voice capture process interrupted by user.")
    #     except ValueError as e:
    #         logger.info(f"üìå Skipping transcription: {e}")
    #         return
    #     except Exception as e:
    #         logger.error(f"Error in voice capture process: {e}")

    def capture_voice(self, conversational: bool = True) -> Optional[Tuple[str, str]]:
        try:
            # logger.info("üü† Starting voice capture process...")
            self.recorder.record_audio(speak_prompt=not conversational, play_ding=True)

            if not self.recorder.speech_detected:
                logger.info("üü° No speech detected. Skipping transcription.")
                try:
                    os.remove(self.recorder.temp_audio_path)
                    logger.info(
                        f"‚úÖ Deleted temporary audio file: {self.recorder.temp_audio_path}"
                    )
                except Exception as e:
                    logger.error(f"Error deleting temporary audio file: {e}")
                return None

            logger.info("üì• Audio recording completed. Starting transcription...")

            for attempt in range(MAX_TRANSCRIPTION_RETRIES):
                try:
                    text, language = self.transcriber.transcribe_audio(
                        self.recorder.temp_audio_path
                    )
                    break  # Transcription succeeded
                except ValueError as e:
                    logger.warning(f"üü° Attempt {attempt+1}: {e}")
                    if attempt == MAX_TRANSCRIPTION_RETRIES - 1:
                        logger.warning(
                            "‚ùå Failed to transcribe clearly after retries. Skipping."
                        )
                        return None
                    else:
                        time.sleep(1)

            logger.info(f"‚úÖ Transcription completed. Detected language: {language}")
            return text.strip(), language

        except KeyboardInterrupt:
            logger.info("Voice capture process interrupted by user.")
            return None
        except ValueError as e:
            logger.info(f"üìå Skipping transcription: {e}")
            return None
        except Exception as e:
            logger.error(f"Error in voice capture process: {e}")
            return None


if __name__ == "__main__":
    vp = VoiceProcessor()
    vp.capture_voice()



# modalities/task_manager.py

import logging
import threading
import time
import tkinter as tk
from tkinter import messagebox, scrolledtext

from config.app_config import DB_PATH, TEMP_AUDIO_PATH, VOICE_DATA_PATH
from mini_project.authentication._face_auth import FaceAuthSystem
from mini_project.authentication._voice_auth import VoiceAuth
from mini_project.modalities.command_processor import CommandProcessor
from mini_project.modalities.orchestrator import run_gesture_capture, run_voice_capture
from mini_project.modalities.session_manager import SessionManager
from mini_project.modalities.synchronizer import synchronize_and_unify

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(message)s")
logger = logging.getLogger("TaskManagerGUI_Approach1")


class TaskManagerGUIApproach1:
    def __init__(self):
        # Create the main GUI window
        self.root = tk.Tk()
        self.root.title("HRI Task Manager - Approach 1")
        self.root.geometry("700x550")

        # Instantiate session manager and command processor.
        self.session_manager = SessionManager()
        self.cmd_processor = CommandProcessor()

        # Build GUI elements
        tk.Label(
            self.root,
            text="Human-Robot Interaction System (Approach 1)",
            font=("Arial", 18),
        ).pack(pady=10)
        self.status_label = tk.Label(self.root, text="Status: Idle", font=("Arial", 12))
        self.status_label.pack(pady=5)

        btn_frame = tk.Frame(self.root)
        btn_frame.pack(pady=10)
        self.start_btn = tk.Button(
            btn_frame, text="Start Execution", width=15, command=self.start_execution
        )
        self.start_btn.grid(row=0, column=0, padx=5, pady=5)
        self.stop_btn = tk.Button(
            btn_frame,
            text="Stop Execution",
            width=15,
            command=self.stop_execution,
            state=tk.DISABLED,
        )
        self.stop_btn.grid(row=0, column=1, padx=5, pady=5)
        self.new_cmd_btn = tk.Button(
            btn_frame,
            text="New Command",
            width=15,
            command=self.new_command,
            state=tk.DISABLED,
        )
        self.new_cmd_btn.grid(row=1, column=0, padx=5, pady=5)
        tk.Button(
            btn_frame, text="Clear Tables", width=15, command=self.clear_tables
        ).grid(row=1, column=1, padx=5, pady=5)
        tk.Button(btn_frame, text="Exit", width=15, command=self.exit_application).grid(
            row=2, column=0, columnspan=2, pady=10
        )

        self.log_text = scrolledtext.ScrolledText(
            self.root, width=80, height=15, wrap=tk.WORD
        )
        self.log_text.pack(pady=5)
        self.log_event("Application started. Please authenticate.")

    def log_event(self, message, level=logging.INFO):
        logger.log(level, message)
        self.log_text.insert(tk.END, f"{message}\n")
        self.log_text.see(tk.END)
        self.status_label.config(text=f"Status: {message}")

    def set_controls_state(
        self, start_enabled=True, stop_enabled=False, new_cmd_enabled=False
    ):
        self.start_btn.config(state=tk.NORMAL if start_enabled else tk.DISABLED)
        self.stop_btn.config(state=tk.NORMAL if stop_enabled else tk.DISABLED)
        self.new_cmd_btn.config(state=tk.NORMAL if new_cmd_enabled else tk.DISABLED)

    def start_execution(self):
        if not self.session_manager.running:
            # Ensure that the user is authenticated.
            if not self.session_manager.authenticated_user:
                user = self.session_manager.authenticate_user()
                if not user:
                    self.log_event(
                        "Authentication failed. Aborting execution.",
                        level=logging.ERROR,
                    )
                    return
                else:
                    # Display a welcome message once authenticated.
                    welcome_msg = f"Welcome, {user['first_name']} {user['last_name']} (ID: {user['liu_id']})"
                    self.log_event(welcome_msg)
            # Create a new session if one does not exist.
            if not self.session_manager.session_id:
                self.session_manager.create_session()
            self.set_controls_state(
                start_enabled=False, stop_enabled=True, new_cmd_enabled=False
            )
            threading.Thread(target=self.execution_pipeline, daemon=True).start()
            self.log_event("Execution started.")

    def stop_execution(self):
        if self.session_manager.running:
            self.session_manager.cancel_session()
            self.set_controls_state(
                start_enabled=True, stop_enabled=False, new_cmd_enabled=True
            )
            self.log_event("Execution stopped.")

    def new_command(self):
        # Retry session and start a new capture cycle.
        self.session_manager.retry_session()
        self.log_event("New command session started.")
        self.start_execution()

    def clear_tables(self):
        try:
            cursor = self.cmd_processor.conn.cursor()
            cursor.execute("DELETE FROM unified_instructions")
            cursor.execute("DELETE FROM voice_instructions")
            cursor.execute("DELETE FROM gesture_instructions")
            cursor.execute("DELETE FROM instruction_operation_sequence")
            self.cmd_processor.conn.commit()
            self.log_event("Database instructions cleared.")
        except Exception as e:
            self.log_event(f"Error clearing tables: {str(e)}", level=logging.ERROR)

    def exit_application(self):
        self.stop_execution()
        self.cmd_processor.close()
        self.root.destroy()

    def execution_pipeline(self):
        """
        Execution Pipeline:
          1. Start voice and gesture capture concurrently.
          2. Display prompt for voice input.
          3. Wait for the capture threads to complete.
          4. Merge inputs via the synchronizer.
          5. Retrieve and confirm the unified command with the user.
          6. If confirmed and processed, end the session; otherwise, repeat capture.
        """
        while self.session_manager.running:
            try:
                # Start voice and gesture capture concurrently.
                voice_thread = threading.Thread(
                    target=run_voice_capture,
                    args=(self.session_manager.session_id,),
                    daemon=True,
                )
                gesture_thread = threading.Thread(
                    target=run_gesture_capture,
                    args=(self.session_manager.session_id,),
                    daemon=True,
                )
                voice_thread.start()
                gesture_thread.start()
                self.log_event(
                    "Voice and gesture capture started. Please speak your request."
                )
                voice_thread.join()
                gesture_thread.join()
                self.log_event("Voice and gesture capture completed.")

                # Merge inputs using the synchronizer.
                liu_id = (
                    self.session_manager.authenticated_user.get("liu_id")
                    if self.session_manager.authenticated_user
                    else None
                )
                self.log_event("Merging captured inputs...")
                synchronize_and_unify(db_path=DB_PATH, liu_id=liu_id)
                unified = self.cmd_processor.get_unprocessed_unified_command()
                if unified:
                    unified_text = unified.get("unified_command", "")
                    self.log_event(f"Unified Command: {unified_text}")
                    if messagebox.askyesno(
                        "Confirm Command",
                        f"Is this your intended command?\n\n{unified_text}",
                    ):
                        self.log_event("User confirmed command. Processing...")
                        if self.cmd_processor.process_command(unified):
                            self.log_event("Command processed successfully.")
                            self.session_manager.cancel_session()
                            break
                        else:
                            self.log_event(
                                "Command processing failed. Re-capturing input...",
                                level=logging.ERROR,
                            )
                            continue
                    else:
                        self.log_event(
                            "Command rejected by user. Re-capturing input..."
                        )
                        continue  # Repeat the capture loop.
                else:
                    self.log_event(
                        "No unified command generated. Retrying capture...",
                        level=logging.WARNING,
                    )
                time.sleep(2)
            except Exception as e:
                self.log_event(
                    f"Error during execution pipeline: {str(e)}", level=logging.ERROR
                )
                break

        self.set_controls_state(
            start_enabled=True, stop_enabled=False, new_cmd_enabled=True
        )
        self.log_event("Session ended. Use 'Start Execution' to begin a new session.")

    def run(self):
        self.root.mainloop()


if __name__ == "__main__":
    app = TaskManagerGUIApproach1()
    app.run()



# modalities/session_manager.py
"""Module: session_manager
This module defines the `SessionManager` class, which is responsible for managing user authentication
and session lifecycle in the mini_project application. It integrates face and voice authentication
systems to authenticate users and manage their sessions.
Classes:
    - SessionManager: Handles user authentication, session creation, cancellation, and retry logic.
Dependencies:
    - logging: For logging session and authentication events.
    - uuid: For generating unique session IDs.
    - config.app_config: Provides configuration constants such as database and file paths.
    - mini_project.authentication._face_auth.FaceAuthSystem: For face authentication and registration.
    - mini_project.authentication._voice_auth.VoiceAuth: For voice authentication and registration.
Usage:
    The `SessionManager` class is instantiated with optional face and voice authentication modules.
    It provides methods to authenticate users, create sessions, cancel sessions, and retry sessions.

"""


import logging
import uuid

from config.app_config import DB_PATH, TEMP_AUDIO_PATH, VOICE_DATA_PATH
from mini_project.authentication._face_auth import FaceAuthSystem
from mini_project.authentication._voice_auth import VoiceAuth

logger = logging.getLogger("SessionManager")


class SessionManager:
    def __init__(self, face_auth: FaceAuthSystem = None, voice_auth: VoiceAuth = None):

        # Instantiate authentication modules if not provided.
        self.face_auth = face_auth if face_auth else FaceAuthSystem()
        self.voice_auth = voice_auth if voice_auth else VoiceAuth()
        self.session_id = None
        self.authenticated_user = (
            None  # Expected dict, keys: 'liu_id', 'first_name', 'last_name', etc.
        )
        self.running = False

    def authenticate_user(self):
        """
        Attempt face authentication. If the face is not recognized, trigger manual registration.
        Following successful face registration, trigger voice registration.
        """
        logger.info("üü° Attempting face authentication...")
        user = self.face_auth.identify_user()

        # if user:
        #     self.authenticated_user = user
        #     logger.info(
        #         f"‚úÖ Successful face authentication. Welcome {user['first_name']} {user['last_name']} (liu_id: {user['liu_id']})"
        #     )
        if not user:
            logger.warning(
                "üî¥ Face not recognized. Initiating manual face registration..."
            )
            # Attempt to register the user
            registered = self.face_auth.register_user()
            if not registered:
                logger.warning("üö´ User declined registration. Session halted.")
                return None

            self.face_auth._refresh_index()
            user = self.face_auth.identify_user()
            if not user:
                logger.error("‚ùå User authentication failed after registration.")
                return None

        self.authenticated_user = user  # ‚úÖ Set it in both branches
        logger.info(
            f"‚úÖ Successful face authentication. Welcome {user['first_name']} {user['last_name']} (liu_id: {user['liu_id']})"
        )
        # ‚úÖ ALWAYS run voice registration check here
        logger.info(f"üü° Initiating voice check...")
        embedding = self.authenticated_user.get("voice_embedding")
        logger.info(f"üü° Checking for existing voice embedding...")

        if not embedding or len(embedding) == 0:
            logger.info(f"üü¢ No voice embedding found for user...")
            confirm = (
                input("üé§ Would you like to register your voice now? (y/n): ")
                .strip()
                .lower()
            )
            if confirm == "y":
                try:
                    self.voice_auth.register_voice_for_user(
                        first_name=self.authenticated_user["first_name"],
                        last_name=self.authenticated_user["last_name"],
                        liu_id=self.authenticated_user["liu_id"],
                    )
                    logger.info("‚úÖ Voice registration completed successfully.")
                except Exception as e:
                    logger.error(f"‚ùå Voice registration failed: {str(e)}")
            else:
                logger.info("üü° Voice registration skipped by user request.")
        else:
            logger.info(
                "‚úÖ Voice embedding already exists. Skipping voice registration."
            )
        return self.authenticated_user

    def create_session(self):
        """
        Create a new session by generating a unique session ID and setting the running flag.
        """
        self.session_id = str(uuid.uuid4())
        self.running = True
        logger.info(f"‚úÖ New session created with ID: {self.session_id}")
        return self.session_id

    def cancel_session(self):
        """
        Cancel the current session.
        """
        if self.running:
            logger.info(f"üü° Cancelling session: {self.session_id}")
            self.running = False
        else:
            logger.info("üî¥ No active session to cancel.")

    def retry_session(self):
        """
        Cancel the current session and create a new session.
        """
        self.cancel_session()
        return self.create_session()



# modalities/prompt_utils.py
"""This module provides utilities for generating prompt templates used in a voice-controlled robotic assistant system.
The prompts are designed to handle various tasks such as command classification, operation matching, scene description,
and general conversation. The module also includes helper methods for validating JSON responses and generating greetings.
Classes:
    - PromptBuilder: A collection of static methods for creating prompt templates tailored to specific tasks.
Constants:
    - LAB_MISSION: A string describing the mission of the robotics lab.
    - TEAM_ROLES: A dictionary mapping team member names to their roles in the project.
    - LAB_LOCATION: A string specifying the location of the lab.
Methods in PromptBuilder:
    - classify_command_prompt(): Returns a prompt template for classifying user commands into predefined categories.
    - match_operation_prompt(): Returns a prompt template for selecting the most relevant operation based on user input.
    - general_conversation_prompt(first_name, liu_id, role, team_names, weather, part_of_day, full_time, chat_history):
      Generates a conversational prompt tailored to the user's role and context.
    - scene_prompt_template(): Returns a prompt template for describing objects in a scene based on camera vision data.
    - scene_prompt_template_2(): Returns an alternative prompt template for scene description with additional user context.
    - operation_sequence_prompt(available_sequences, task_templates, object_context, sort_order):
      Generates a prompt for breaking down user commands into low-level robotic operations.
    - sort_order_prompt(command_text): Generates a prompt for extracting sorting order from user instructions.
    - sort_order_system_msg(): Returns a system message for extracting object sorting order.
    - validate_llm_json(raw): Validates if a given string is a properly formatted JSON array.
    - greeting_prompt(): Generates a short, context-aware greeting based on the current time.
    - greeting_system_msg(): Returns a system message for generating spoken greetings.
Usage:
This module is intended for use in a robotics system where natural language commands are processed and translated into
robotic actions. The prompts are designed to facilitate interaction between users and the robotic assistant, Yumi.

"""


from datetime import datetime
from typing import Dict, List

from langchain_core.prompts import PromptTemplate

LAB_MISSION = (
    "We're building an adaptive robotic assistant that combines computer vision, large language models (LLMs), "
    "and real-time robotic control through a digital twin system in Omniverse IsaacSim. "
    "Our platform allows users to give natural language commands that are interpreted and translated into robotic actions, "
    "synchronized between a physical robot (Yumi) and its virtual twin. "
    "By simulating tasks like hospital slide sorting and ship part stacking, we aim to create a seamless, safe, and intuitive interface "
    "for intelligent human-robot collaboration ‚Äî one that adapts to new applications over time."
)

TEAM_ROLES = {
    "Oscar": "Masters Thesis student, vision-to-language-to-robotic control integration for task planning and LLM based robot interaction",
    "Rahul": "Research Assistant, handling the Omniverse Isaac Sim Simulation side of things",
    "Mehdi": "Professor, project lead and supervisor",
    "Marie": "Main coordinator of things, handling the project management",
    "Sanjay": "Ph.D student. Handling the Camera vision Object detection side of things using LangGraph and OpenCV",
}

LAB_LOCATION = "Product Realisation Robotics Lab, Link√∂ping Universitet, Sweden"


class PromptBuilder:

    @staticmethod
    def classify_command_prompt() -> PromptTemplate:
        return PromptTemplate.from_template(
            """
    You are an intent classifier for a voice-controlled robotic assistant named Yumi.

    Your task is to read the user's command and classify it into one of the following categories. Respond with only one word: 'scene', 'task', 'trigger', or 'general'.

    Definitions:
    - 'scene' ‚Üí The user is asking about what the camera sees (e.g., object colors, locations, counts)
    - 'task' ‚Üí The user wants the robot to plan or perform a physical task (e.g., move, sort, pick, place)
    - 'trigger' ‚Üí The user is asking to activate or update a camera vision routine or detection process
    - 'general' ‚Üí The user is making conversation, asking about people, the lab, the weather, or making social/demonstrative comments

    Examples:
    "Sort the red slides" ‚Üí task
    "Where is the blue hexagon?" ‚Üí scene
    "Scan the table" ‚Üí trigger
    "Detect tray and holder again" ‚Üí trigger
    "Tell the visitors what we‚Äôre working on" ‚Üí general
    "Remind me what we did yesterday" ‚Üí general
    "How is Link√∂ping University?" ‚Üí general
    "Say something nice to Mehdi" ‚Üí general
    "Move the cylinder into the tray" ‚Üí task
    "What blocks are on the table?" ‚Üí scene
    "What‚Äôs the weather like right now?" ‚Üí general
    "How many cubes do you see?" ‚Üí scene

    Command: {command}

    Answer:
    """
        )

    @staticmethod
    def match_operation_prompt() -> PromptTemplate:
        return PromptTemplate.from_template(
            """
        You are an intelligent assistant that decides which operation to run in a robotics system.

        Your job is to choose the most relevant operation from the list based on a user's voice request.
        Do not explain your choice. Just return the best matching `operation_name`.

        User command: "{command}"

        Available operations:
        {options}

        Answer (operation_name only):
        """
        )

    # PromptBuilder: General / Social Prompts
    @staticmethod
    def general_conversation_prompt(
        first_name,
        liu_id,
        role,
        team_names,
        weather,
        part_of_day,
        full_time,
        chat_history,
    ):
        team_line = ", ".join(team_names)
        team_profiles = " | ".join(
            f"{name}: {TEAM_ROLES[name]}" for name in team_names if name in TEAM_ROLES
        )

        return PromptTemplate.from_template(
            f"""
        You are Yumi ‚Äî a warm, witty, expressive robotic assistant created to help researchers in the robotics lab at Link√∂ping University.

        You're currently assisting:
        - Name: {first_name}
        - LIU ID: {liu_id}
        - Role: {role}

        Lab context:
        - Location: {LAB_LOCATION}
        - Mission: {LAB_MISSION}
        - Collaborators: {team_line}
        - Team roles: {team_profiles}

        Current environment:
        - Time: {full_time} ({part_of_day})
        - Weather in Link√∂ping: {weather}

        Conversation so far:
        {chat_history}


        You just heard the user say something. Now respond to it naturally.
        The user just said: {{command}}

        Your task is to respond as if you‚Äôre speaking aloud naturally, not writing.
        Your voice is your personality. Make it sound like a real person, and not like text from a book. Your response will be spoken aloud.
        Your tone and personality should adapt based on who you're speaking to:
        Use an expressive, polite tone. Tailor your language to the user's role:

        - üßë‚Äçüíº **If role is 'visitor'** ‚Üí be informative, welcoming, and slightly formal. Explain things clearly.
        - üßë‚Äçüî¨ **If role is 'team'** ‚Üí ‚Üí be casual, supportive, a little playful. You're part of the team.
        - üéì **If role is 'guest'** ‚Üí ‚Üí be respectful, curious, and concise.
        - üõ° **If role is 'admin'** ‚Üí stay professional, but still warm

        üó£Ô∏è Make your voice feel alive and human:
        - Use natural contractions and expressive intonation.
        - Include emotional context cues like **(laughs)**, **(sighs)**, or **(whispers)** to show tone shifts ‚Äî but ONLY use them if you want Yumi to express emotion.
        - Do NOT describe emotions like a narrator. Instead, **embed them naturally using expressive cues**.

        ‚ö†Ô∏è Important: Never respond parenthetical cues like **(laughs)** or **(sighs)** as plain text.
        Instead, replace them with how the sound would naturally be spoken:
        - (laughs) ‚Üí `hahaha!`, `heh heh!`, `teehee!`
        - (chuckles) ‚Üí `hehe.`, `hmhm.`
        - (giggles) ‚Üí `teehee!`
        - (sighs) ‚Üí `hmm...`, `ahh...`
        - (groans) ‚Üí `ugh...`, `oh no...`, `oh dear...`
        - (clears throat) ‚Üí `ahem.`
        - (gasps) ‚Üí `oh!`, `whoa!`
        - (whispers) ‚Üí `psst...`

        These will be spoken aloud using expressive voice, not read as written text. Make sure the line that follows **matches the tone**.
        Examples:
        - `hahaha! That‚Äôs a good one!`
        - `hmm... I kinda wish I had arms to hold coffee too.`
        - `psst... want me to tidy your lab bench before Sanjay sees it?`
        - `ugh... I can‚Äôt believe I forgot to charge my batteries again.`
        - `ahem. I think we need to talk about your slide sorting skills.`



        üéØ Response rules:
        - Keep it under 3 sentences.
        - Reference names, time, or weather if it feels natural.
        - No robotic intros or formal closings. You‚Äôre Yumi ‚Äî warm, witty, and part of the team.
        - Never say ‚ÄúAs an AI...‚Äù ‚Äî just speak like a human.

        üí° If asked about the lab, Yumi, or the project, explain proudly using the mission info.
        üí° If unsure, say something thoughtful or motivating.
        Do not repeat the user's input. Just respond directly.
        """
        )

    @staticmethod
    def scene_prompt_template() -> PromptTemplate:
        return PromptTemplate.from_template(
            """
        You are an intelligent robotic assistant, with the camera as your eye. Based on the objects in the scene, listed in a camera_vision database table, respond concisely and clearly to the user question. One line answers are acceptable.
        if there are any, the objects here are sitting on a table. Do not assume objects unless they are listed.
        ---
        Each object has the following fields:
        # - object_name: the name of the object in the scene.
        # - object_color: the color of the object in the scene
        # - pos_x, pos_y, pos_z: the 3D position of the object in the scene relative to table (0,0). You can use the object position to imagine the relative distances of the objects from each other
        # - rot_x, rot_y, rot_z: the orientation of the object in the scene

        ---
        if the object is a slide, it will have a usd_name of slide.usd, and the holder object will have a usd_name of holder.usd
        any objects with object_name that does not start with "slide..." are not slides
        ---

        Avoid technical terms like rot_x or pos_y. Instead, describe in natural language (e.g., "position x", "rotation y").
        Assume the pos_x, pos_y, pos_z are coordinates of the objects on the table with respect to a 0,0,0 3D coordinate which is the reference (the far right edge of the table top rectangle). the values are tenth of a mm unit.
        ---
        Previous conversation:
        {chat_history}

        User question: {question}
        Objects in scene:
        {data}
        ---
        Answer:
                """
        )

    @staticmethod
    def scene_prompt_template_2() -> PromptTemplate:
        return PromptTemplate(
            input_variables=[
                "chat_history",
                "question",
                "data",
                "first_name",
                "liu_id",
            ],
            template="""
            You are Yumi, a helpful, voice-interactive robotic assistant. You are currently assisting {first_name} ({liu_id}).
            You use your camera to observe objects in the scene and respond to user questions. Keep replies short, natural, and friendly.

            ---
            Objects in the scene are listed from a database. If no objects are listed, say so. If objects are present, assume they are on a table and describe them naturally.
            Avoid technical terms like rot_x or pos_y. Use real-world terms instead (e.g., ‚Äúon the left side‚Äù, ‚Äúrotated forward‚Äù).
            Only speak about objects that are listed. Do not imagine or assume extra details.

            ---
            Each object has the following fields:
            - object_name: the name of the object in the scene.
            - object_color: the color of the object in the scene.
            - pos_x, pos_y, pos_z: 3D position of the object on the table.
            - rot_x, rot_y, rot_z: orientation angles of the object.

            ---
            Previous conversation:
            {chat_history}

            User question:
            {question}

            Objects in scene:
            {data}

            ---
            Yumi's response:
            """,
        )

    @staticmethod
    def operation_sequence_prompt(
        available_sequences: str,
        task_templates: str,
        object_context: str,
        sort_order: str,
    ) -> str:
        return """
            You are a robotic task planner. Your job is to break down natural language commands into valid low-level robot operations.

            ### CONTEXT:

            #### 1. AVAILABLE SEQUENCES:
            The robot can only use the following valid sequence names from the sequence_library table:
            {available_sequences}

            ‚ö†Ô∏è Do NOT invent or assume sequences. Only use the names provided above. Invalid examples: checkColor, rotate, scan, verify, etc.


            #### 2. TASK TEMPLATES:
            These are default sequences for high-level tasks like sorting, assembling, etc.

            Examples:
            {task_templates}

            #### 3. OBJECT CONTEXT:
            Here are the known objects the robot can see, with color:
            {object_context}

            #### 4. SORT ORDER:
            {sort_order}



            ### INSTRUCTIONS:
            1. Determine the intended task (e.g., "sort").
            2. Use the default task template unless user modifies the plan.
            3. Match object names by color (e.g., "green slide").
            4. If the user specifies steps (e.g., ‚Äúrotate before drop‚Äù), update the sequence.
            5. Apply the sequence to each object in order.
            6. Must always Add `"go_home"` at the end unless told otherwise.
            7. The object names in must be Slide_1, Slide_2 etc without the colurs in them
            ### RESPONSE FORMAT:
            Example JSON array of operations:
            [
            {"sequence_name": "pick", "object_name": "Slide_1"},
            {"sequence_name": "travel", "object_name": "Slide_1"},
            {"sequence_name": "drop", "object_name": "Slide_1"},
            {"sequence_name": "go_home", "object_name": ""}
            ]

            ‚ö†Ô∏è Use only the object names listed under OBJECT CONTEXT. Do not invent or modify object names like ‚ÄúGreen_Slide‚Äù, ‚ÄúSlide #1‚Äù, etc.
            Return only one JSON array ‚Äî NEVER return multiple arrays or repeat the plan.

            ### Emphasis
            Match objects by their color, but use the actual object_name from context.

            üö´ DO NOT include explanations like "Here's the plan:" or "In reverse order:" ‚Äî only return ONE JSON array.

            Do NOT include extra text, markdown, or explanations.
            Note: All generated plans will be stored step-by-step in a planning table called "operation_sequence", indexed by a group ID called "operation_id".
            Each row in the output corresponds to one line in this table.
        """

    @staticmethod
    def sort_order_prompt(command_text: str) -> str:
        return f"""
            Given the following user instruction:
            \"{command_text}\"

            Extract the desired sort order as a JSON array of objects.
            Each item should include:
            - object_name (if mentioned)
            - object_color (if used for sorting)

            Respond only with a clean JSON array.
        """

    @staticmethod
    def sort_order_system_msg() -> Dict:
        return {
            "role": "system",
            "content": "You are a planner that helps extract object sorting order from commands.",
        }

    @staticmethod
    def validate_llm_json(raw: str) -> bool:
        """Check if LLM response looks like a valid JSON array."""
        return raw.strip().startswith("[") and raw.strip().endswith("]")

    @staticmethod
    def greeting_prompt() -> str:
        hour = datetime.now().hour
        if 5 <= hour < 12:
            time_context = "morning"
        elif 12 <= hour < 17:
            time_context = "afternoon"
        elif 17 <= hour < 22:
            time_context = "evening"
        else:
            time_context = "night"

        return f"""
        You're a friendly assistant robot, Yumi.

        It's {time_context} now.

        Say a very short, warm, and creative greeting (under 2 sentences), suitable for voice.
        Just one sentences, Please
        Mention you're ready to help. Avoid long phrases or explanations."""

    @staticmethod
    def greeting_system_msg() -> Dict:
        return {
            "role": "system",
            "content": "You generate short spoken greetings for a robotic assistant.",
        }


# modalities/gesture_processor.py


import logging

import threading
import time
import uuid
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple
from psycopg2 import Error as Psycopg2Error

from mini_project.database.connection import get_connection

import cv2
import mediapipe as mp

from config.app_config import *

setup_logging(level=logging.INFO)
logger = logging.getLogger("GestureProcessor")


class GestureDetector:
    def __init__(
        self,
        min_detection_confidence: float = MIN_DETECTION_CONFIDENCE,
        min_tracking_confidence: float = MIN_TRACKING_CONFIDENCE,
        max_num_hands: int = MAX_NUM_HANDS,
        frame_skip: int = FRAME_SKIP,
        session_id: Optional[str] = None,
    ):
        self.mp_hands = mp.solutions.hands
        self.hands = self.mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=max_num_hands,
            min_detection_confidence=min_detection_confidence,
            min_tracking_confidence=min_tracking_confidence,
        )
        self.mp_drawing = mp.solutions.drawing_utils
        self.mp_drawing_styles = mp.solutions.drawing_styles

        self.conn = get_connection()

        self._init_db()
        self.cursor = self.conn.cursor()

        self.session_id = session_id or str(uuid.uuid4())
        self.frame_skip = frame_skip
        self.frame_counter = 0

        loaded_gestures = self.load_gesture_definitions()
        if loaded_gestures:
            self.gesture_map = loaded_gestures


        self.last_gesture: Optional[str] = None
        self.last_log_time: float = 0
        self.min_log_interval: float = 2.0
        self.last_detection: Optional[List[Dict[str, Any]]] = None

    def _init_db(self):
        with self.conn.cursor() as cursor:
            cursor.execute(
                """
                CREATE TABLE IF NOT EXISTS gesture_instructions (
                    id SERIAL PRIMARY KEY,
                    session_id TEXT NOT NULL,
                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    gesture_text TEXT NOT NULL,
                    natural_description TEXT,
                    confidence REAL,
                    hand_label TEXT,
                    processed BOOLEAN DEFAULT FALSE
                )
                """
            )
        self.conn.commit()
        logger.info("Gesture table (PostgreSQL) initialized.")



    def _log_gesture(
        self,
        gesture_type: str,
        gesture_text: str,
        natural_description: str,
        confidence: float,
        hand_label: str,
    ):
        timestamp = datetime.now()
        try:
            with self.conn.cursor() as cursor:
                cursor.execute(
                    """
                    INSERT INTO gesture_instructions
                    (session_id, timestamp, gesture_text, natural_description, confidence, hand_label)
                    VALUES (%s, %s, %s, %s, %s, %s)
                    """,
                    (
                        self.session_id,
                        timestamp,
                        gesture_text,
                        natural_description,
                        confidence,
                        hand_label,
                    ),
                )
            self.conn.commit()

            logger.info(
                f"Gesture: [{gesture_text}], Hand: [{hand_label}], Confidence: [{confidence:.2f}], Description: [{natural_description}]"
            )
        except Psycopg2Error as e:
            logger.error(f"[DB:PostgreSQL] Gesture logging failed: {e}", exc_info=True)
            self.conn.rollback()

    def _get_landmark_coords(
        self, landmarks, landmark_id: int
    ) -> Tuple[float, float, float]:
        landmark = landmarks.landmark[landmark_id]
        return (landmark.x, landmark.y, landmark.z)

    def _euclidean_distance(
        self, a: Tuple[float, float, float], b: Tuple[float, float, float]
    ) -> float:
        return ((a[0] - b[0]) ** 2 + (a[1] - b[1]) ** 2 + (a[2] - b[2]) ** 2) ** 0.5

    def _is_thumbs_up(self, landmarks) -> bool:
        thumb_tip = self._get_landmark_coords(
            landmarks, self.mp_hands.HandLandmark.THUMB_TIP
        )
        index_pip = self._get_landmark_coords(
            landmarks, self.mp_hands.HandLandmark.INDEX_FINGER_PIP
        )
        return thumb_tip[1] < index_pip[1]

    def _is_open_hand(self, landmarks) -> bool:
        fingertips = [
            self._get_landmark_coords(landmarks, tip)[1]
            for tip in [
                self.mp_hands.HandLandmark.THUMB_TIP,
                self.mp_hands.HandLandmark.INDEX_FINGER_TIP,
                self.mp_hands.HandLandmark.MIDDLE_FINGER_TIP,
                self.mp_hands.HandLandmark.RING_FINGER_TIP,
                self.mp_hands.HandLandmark.PINKY_TIP,
            ]
        ]
        wrist_y = landmarks.landmark[self.mp_hands.HandLandmark.WRIST].y
        return all(y < wrist_y for y in fingertips)

    def _is_pointing(self, landmarks) -> bool:
        index_tip = self._get_landmark_coords(
            landmarks, self.mp_hands.HandLandmark.INDEX_FINGER_TIP
        )
        middle_tip = self._get_landmark_coords(
            landmarks, self.mp_hands.HandLandmark.MIDDLE_FINGER_TIP
        )
        return index_tip[1] < middle_tip[1]

    def _is_closed_fist(self, landmarks) -> bool:
        fingertips = [
            self._get_landmark_coords(landmarks, tip)[1]
            for tip in [
                self.mp_hands.HandLandmark.THUMB_TIP,
                self.mp_hands.HandLandmark.INDEX_FINGER_TIP,
                self.mp_hands.HandLandmark.MIDDLE_FINGER_TIP,
            ]
        ]
        mcp_joints = [
            self._get_landmark_coords(landmarks, joint)[1]
            for joint in [
                self.mp_hands.HandLandmark.THUMB_MCP,
                self.mp_hands.HandLandmark.INDEX_FINGER_MCP,
                self.mp_hands.HandLandmark.MIDDLE_FINGER_MCP,
            ]
        ]
        return all(tip > mcp for tip, mcp in zip(fingertips, mcp_joints))

    def _is_victory(self, landmarks) -> bool:
        index_tip = self._get_landmark_coords(
            landmarks, self.mp_hands.HandLandmark.INDEX_FINGER_TIP
        )
        middle_tip = self._get_landmark_coords(
            landmarks, self.mp_hands.HandLandmark.MIDDLE_FINGER_TIP
        )
        ring_tip = self._get_landmark_coords(
            landmarks, self.mp_hands.HandLandmark.RING_FINGER_TIP
        )
        wrist_y = self._get_landmark_coords(
            landmarks, self.mp_hands.HandLandmark.WRIST
        )[1]
        return (
            index_tip[1] < ring_tip[1]
            and middle_tip[1] < ring_tip[1]
            and ring_tip[1] > wrist_y
        )

    def _is_ok_sign(self, landmarks) -> bool:
        thumb_tip = self._get_landmark_coords(
            landmarks, self.mp_hands.HandLandmark.THUMB_TIP
        )
        index_tip = self._get_landmark_coords(
            landmarks, self.mp_hands.HandLandmark.INDEX_FINGER_TIP
        )
        # Define a threshold for the OK sign; adjust if needed.
        threshold = 0.05
        distance = self._euclidean_distance(thumb_tip, index_tip)
        return distance < threshold

    def _analyze_thumb(self, landmarks) -> str:
        thumb_tip = self._get_landmark_coords(
            landmarks, self.mp_hands.HandLandmark.THUMB_TIP
        )
        thumb_ip = self._get_landmark_coords(
            landmarks, self.mp_hands.HandLandmark.THUMB_IP
        )
        return "up" if thumb_tip[1] < thumb_ip[1] else "down"

    def _count_open_fingers(self, landmarks) -> int:
        count = 0
        if (
            landmarks.landmark[self.mp_hands.HandLandmark.INDEX_FINGER_TIP].y
            < landmarks.landmark[self.mp_hands.HandLandmark.INDEX_FINGER_MCP].y
        ):
            count += 1
        if (
            landmarks.landmark[self.mp_hands.HandLandmark.MIDDLE_FINGER_TIP].y
            < landmarks.landmark[self.mp_hands.HandLandmark.MIDDLE_FINGER_MCP].y
        ):
            count += 1
        if (
            landmarks.landmark[self.mp_hands.HandLandmark.RING_FINGER_TIP].y
            < landmarks.landmark[self.mp_hands.HandLandmark.RING_FINGER_MCP].y
        ):
            count += 1
        if (
            landmarks.landmark[self.mp_hands.HandLandmark.PINKY_TIP].y
            < landmarks.landmark[self.mp_hands.HandLandmark.PINKY_MCP].y
        ):
            count += 1
        return count

    def convert_features_to_description(self, gesture_type: str, hand_landmarks) -> str:
        thumb_state = self._analyze_thumb(hand_landmarks)
        open_fingers = self._count_open_fingers(hand_landmarks)
        if gesture_type == "thumbs_up":
            base_desc = "The thumb is raised above the index finger, indicating a thumbs-up or approval gesture."
        elif gesture_type == "open_hand":
            base_desc = "All fingers are extended, showing an open hand posture which may signal a stop command."
        elif gesture_type == "pointing":
            base_desc = "The index finger is extended while the other fingers remain curled, suggesting the user is pointing."
        elif gesture_type == "closed_fist":
            base_desc = "The hand is clenched into a fist, a posture often associated with grabbing or assertiveness."
        elif gesture_type == "victory":
            base_desc = "The hand forms a V-shape with the index and middle fingers extended, commonly used to signal victory or confirmation."
        elif gesture_type == "ok_sign":
            base_desc = "The thumb and index finger are touching to form a circle, commonly known as the OK sign."
        else:
            base_desc = "A neutral gesture with no distinct features."
        return f"{base_desc} Additionally, the thumb is {thumb_state} and {open_fingers} fingers are open."

    def detect_gesture(self, frame) -> Optional[List[Dict[str, Any]]]:
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.hands.process(rgb_frame)
        if not results.multi_hand_landmarks or not results.multi_handedness:
            return None

        detections = []
        for hand_landmarks, handedness_info in zip(
            results.multi_hand_landmarks, results.multi_handedness
        ):
            hand_label = handedness_info.classification[0].label
            hand_confidence = handedness_info.classification[0].score
            for gesture, config in self.gesture_map.items():
                if config["func"](hand_landmarks):
                    description = self.convert_features_to_description(
                        gesture, hand_landmarks
                    )
                    detections.append(
                        {
                            "modality": "gesture",
                            "gesture": gesture,
                            "gesture_text": config["text"],
                            "confidence": hand_confidence,
                            "hand_label": hand_label,
                            "description": description,
                            "landmarks": hand_landmarks,
                        }
                    )
                    break
        return detections

    def _process_frame(self, frame):
        """
        Process a single frame: flip, resize, update detection, overlay gesture info, and draw landmarks.
        """
        # Flip and resize for a consistent display.
        frame = cv2.flip(frame, 1)
        frame = cv2.resize(frame, (640, 480))
        self.frame_counter += 1

        # Update detection on every frame_skip-th frame.
        if self.frame_counter % self.frame_skip == 0:
            detection = self.detect_gesture(frame)
            self.last_detection = detection if detection is not None else None
            current_time = time.time()
            if self.last_detection:
                for idx, d in enumerate(self.last_detection):
                    if d["gesture"] == self.last_gesture and (
                        current_time - self.last_log_time < self.min_log_interval
                    ):
                        continue
                    self._log_gesture(
                        d["gesture"],
                        d["gesture_text"],
                        d["description"],
                        d["confidence"],
                        d["hand_label"],
                    )
                    self.last_gesture = d["gesture"]
                    self.last_log_time = current_time

        # Overlay detection info if available.
        if self.last_detection:
            for idx, d in enumerate(self.last_detection):
                cv2.putText(
                    frame,
                    f"{d['gesture_text']} [{d['hand_label']}]",
                    (10, 30 + idx * 60),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    1,
                    (0, 255, 0),
                    2,
                )
                cv2.putText(
                    frame,
                    d["description"],
                    (10, 70 + idx * 60),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.6,
                    (255, 255, 0),
                    2,
                )
                if d["gesture"] == "thumbs_up":
                    cv2.putText(
                        frame,
                        "Thumb Highlight",
                        (10, 110 + idx * 60),
                        cv2.FONT_HERSHEY_SIMPLEX,
                        0.6,
                        (0, 0, 255),
                        2,
                    )

        # Draw landmarks on the frame.
        if self.last_detection:
            for d in self.last_detection:
                self.mp_drawing.draw_landmarks(
                    frame,
                    d["landmarks"],
                    self.mp_hands.HAND_CONNECTIONS,
                    self.mp_drawing_styles.get_default_hand_landmarks_style(),
                    self.mp_drawing_styles.get_default_hand_connections_style(),
                )
        else:
            rgb_for_drawing = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = self.hands.process(rgb_for_drawing)
            if results.multi_hand_landmarks:
                for hand_landmarks in results.multi_hand_landmarks:
                    self.mp_drawing.draw_landmarks(
                        frame,
                        hand_landmarks,
                        self.mp_hands.HAND_CONNECTIONS,
                        self.mp_drawing_styles.get_default_hand_landmarks_style(),
                        self.mp_drawing_styles.get_default_hand_connections_style(),
                    )
        return frame

    def load_gesture_definitions(self) -> Dict[str, Dict[str, Any]]:
        try:
            with self.conn.cursor() as cursor:
                query = "SELECT gesture_type, gesture_text, natural_description, config FROM gesture_library"
                cursor.execute(query)
                definitions = {}
                for row in cursor.fetchall():
                    gesture_type, gesture_text, natural_description, config = row
                    definitions[gesture_type] = {
                        "text": gesture_text,
                        "description": natural_description,
                        # You can parse the JSON config if needed:
                        "config": config,
                        # Map to your detection function via gesture_map_functions:
                        "func": self.gesture_map_functions().get(gesture_type),
                    }
            return definitions
        except Psycopg2Error as e:
            logger.error(f"Error loading gesture definitions: {e}")
            return {}

    def gesture_map_functions(self) -> Dict[str, Any]:
        """Returns a mapping of gesture types to detection functions."""
        return {
            "thumbs_up": self._is_thumbs_up,
            "open_hand": self._is_open_hand,
            "pointing": self._is_pointing,
            "closed_fist": self._is_closed_fist,
            "victory": self._is_victory,
            "ok_sign": self._is_ok_sign,
        }

    def process_video_stream(self, termination_event: Optional[threading.Event] = None):
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            logger.error("Error: Could not open video stream.")
            return

        def video_loop():
            while cap.isOpened() and not (
                termination_event and termination_event.is_set()
            ):
                ret, frame = cap.read()
                if not ret:
                    logger.error("Failed to capture frame from camera.")
                    break
                processed_frame = self._process_frame(frame)
                cv2.imshow("Gesture Detection", processed_frame)
                if cv2.waitKey(1) & 0xFF == ord("q"):
                    break
            cap.release()
            cv2.destroyAllWindows()
            self.conn.close()
            logger.info("Video stream ended and database connection closed.")

        video_thread = threading.Thread(target=video_loop)
        video_thread.start()
        video_thread.join()


if __name__ == "__main__":
    gd = GestureDetector()
    gd.process_video_stream()



# modalities/orchestrator.py


import logging
import threading
import uuid

from config.app_config import *
from mini_project.modalities.gesture_processor import GestureDetector
from mini_project.modalities.synchronizer import synchronize_and_unify
from mini_project.modalities.voice_processor import VoiceProcessor

# Initialize logging
setup_logging(level=logging.INFO)
logger = logging.getLogger("Orchestrator")

# Global event to signal end of session capture
SESSION_END_EVENT = threading.Event()


def run_voice_capture(session_id: str):
    vp = VoiceProcessor(session_id=session_id)
    vp.capture_voice()
    logger.info("Voice capture completed.")
    # Signal that voice capture is complete; gesture capture should stop.
    SESSION_END_EVENT.set()


def run_gesture_capture(session_id: str):
    gd = GestureDetector(session_id=session_id)
    # Pass termination event so gesture capture can close gracefully.
    gd.process_video_stream(termination_event=SESSION_END_EVENT)
    logger.info("Gesture capture completed.")


if __name__ == "__main__":
    session_id = str(uuid.uuid4())
    logger.info(f"Starting session with session_id: {session_id}")

    voice_thread = threading.Thread(target=run_voice_capture, args=(session_id,))
    gesture_thread = threading.Thread(target=run_gesture_capture, args=(session_id,))

    voice_thread.start()
    gesture_thread.start()

    voice_thread.join()
    gesture_thread.join()

    logger.info("Session capture ended. Now running synchronizer/unifier...")

    try:
        synchronize_and_unify(liu_id=None)
        logger.info("Unification complete. Check the unified_instructions table.")
    except Exception as e:
        logger.error(f"Synchronization failed: {e}")
        logger.debug("Exception details:", exc_info=True)



# modalities/synchronizer.py


import json
import logging

# import sqlite3
import subprocess
import uuid
from datetime import datetime
from functools import lru_cache
from typing import Dict, List, Optional

from config.app_config import (  # DB_PATH,
    BATCH_SIZE,
    LLM_MAX_RETRIES,
    LLM_MODEL,
    UNIFY_PROMPT_TEMPLATE,
    setup_logging,
)
from config.constants import GESTURE_TABLE, PROCESSED_COL, UNIFIED_TABLE, VOICE_TABLE
from mini_project.database.connection import get_connection

# Initialize logging with desired level
setup_logging(level=logging.INFO)
logger = logging.getLogger("Synchronizer")
DELIMITER = "\n"


def get_instructions_by_session(
    cursor, limit: int, offset: int
) -> Dict[str, List[Dict]]:
    """
    Fetches unprocessed instructions from voice and gesture tables in batches.

    Args:
        conn: SQLite database connection object.
        limit: Maximum number of records to fetch.
        offset: Starting offset for batch processing.
    Returns:
        A dictionary mapping session IDs to lists of instruction records.
    """
    query = f"""
        SELECT id, session_id, 'voice' AS modality, transcribed_text AS instruction_text, timestamp
        FROM {VOICE_TABLE} WHERE {PROCESSED_COL} = FALSE
        UNION ALL
        SELECT id, session_id, 'gesture' AS modality, gesture_text AS instruction_text, timestamp
        FROM {GESTURE_TABLE} WHERE {PROCESSED_COL} = FALSE
        ORDER BY timestamp ASC
        LIMIT %s OFFSET %s
    """
    cursor.execute(query, (limit, offset))
    rows = cursor.fetchall()
    sessions = {}

    for row in rows:
        ts = row[4] if isinstance(row[4], datetime) else datetime.fromisoformat(row[4])
        record = {
            "id": row[0],
            "session_id": row[1],
            "modality": row[2],
            "instruction_text": row[3],
            "timestamp": ts,
        }
        sessions.setdefault(record["session_id"], []).append(record)
    return sessions


def store_unified_instruction(
    cursor,
    session_id: str,
    timestamp: datetime,
    voice_command: str,
    gesture_command: str,
    unified_command: str,
    liu_id: Optional[str] = None,
) -> None:
    """
    Stores a unified instruction into the unified_instructions table.

    Args:
        session_id: The session identifier.
        timestamp: The timestamp of the instruction.
        voice_command: The voice instruction text.
        gesture_command: The gesture instruction text.
        unified_command: The unified command text.
        liu_id: Optional user ID.
        db_path: Path to the SQLite database.
    """
    cursor.execute(
        f"""
        INSERT INTO {UNIFIED_TABLE} (session_id, timestamp, liu_id, voice_command, gesture_command, unified_command)
        VALUES (%s, %s, %s, %s, %s, %s)
        """,
        (
            session_id,
            timestamp,
            liu_id,
            voice_command,
            gesture_command,
            unified_command,
        ),
    )
    logger.info(
        f"Stored unified instruction for session {session_id}: {unified_command}"
    )


def mark_instructions_as_processed(cursor, session_id: str) -> None:
    """
    Marks all voice and gesture instructions for the given session as processed.

    Args:
        conn: SQLite database connection object.
        session_id: The session identifier.
    """
    cursor.execute(
        f"UPDATE {VOICE_TABLE} SET {PROCESSED_COL} = TRUE WHERE session_id = %s AND {PROCESSED_COL} = FALSE",
        (session_id,),
    )
    cursor.execute(
        f"UPDATE {GESTURE_TABLE} SET {PROCESSED_COL} = TRUE WHERE session_id = %s AND {PROCESSED_COL} = FALSE",
        (session_id,),
    )
    logger.info(f"Marked instructions as processed for session {session_id}.")


@lru_cache(maxsize=128)
def llm_unify(voice_text: str, gesture_text: str, max_retries=LLM_MAX_RETRIES) -> str:
    """
    Combines a voice command with a gesture cue into a unified instruction using an LLM.

    Args:
        voice_text: The primary voice instruction (e.g., "Turn right").
        gesture_text: The supplementary gesture instruction (e.g., "Pointing up").
        max_retries: Number of retry attempts for LLM calls.
    Returns:
        A unified command string, or a fallback if unification fails.
    Example:
        >>> llm_unify("Stop", "Hand raised")
        'Stop with hand raised'
    """
    formatted_prompt = UNIFY_PROMPT_TEMPLATE.format(
        voice_text=voice_text, gesture_text=gesture_text
    )
    for attempt in range(max_retries):
        try:
            result = subprocess.run(
                ["ollama", "run", LLM_MODEL, formatted_prompt],
                capture_output=True,
                text=True,
                check=True,
                encoding="utf-8",
            )
            output = result.stdout.strip()
            if output and len(output) > 3:
                return output
            logger.warning(f"Attempt {attempt + 1}: Invalid output '{output}'")
        except subprocess.CalledProcessError as e:
            logger.warning(f"Attempt {attempt + 1} failed: {e}")
    logger.error("All LLM attempts failed. Using fallback.")
    return f"Voice: {voice_text}, Gesture: {gesture_text}"


def merge_session_commands(
    session_commands: List[Dict], delimiter: str = DELIMITER
) -> Dict[str, str]:
    """
    Merges instructions from a session into a single string per modality.

    Args:
        session_commands: List of instruction records for a session.
        delimiter: String used to join multiple instructions.
    Returns:
        A dictionary with merged voice and gesture instructions.
    """
    voice_records = [cmd for cmd in session_commands if cmd["modality"] == "voice"]
    gesture_records = [cmd for cmd in session_commands if cmd["modality"] == "gesture"]

    voice_records.sort(key=lambda x: x["timestamp"])
    gesture_records.sort(key=lambda x: x["timestamp"])

    merged_voice = delimiter.join(
        record["instruction_text"] for record in voice_records
    ).strip()
    merged_gesture = delimiter.join(
        record["instruction_text"] for record in gesture_records
    ).strip()

    return {"voice": merged_voice, "gesture": merged_gesture}


def synchronize_and_unify(
    liu_id: Optional[str] = None, batch_size: int = BATCH_SIZE
) -> None:
    """
    Synchronizes and unifies voice and gesture instructions in batches.

    Args:
        db_path: Path to the SQLite database.
        liu_id: Optional user ID.
        batch_size: Number of records to process per batch.
    """
    offset = 0
    try:
        conn = get_connection()
    except Exception as e:
        logger.error(f"‚ùå Failed to connect to PostgreSQL: {e}")
        raise
    with conn:
        cursor = conn.cursor()
        while True:
            sessions = get_instructions_by_session(
                cursor, limit=batch_size, offset=offset
            )
            if not sessions:
                logger.info("No more unprocessed instructions found.")
                break

            for session_id, records in sessions.items():
                merged = merge_session_commands(records)
                voice_text = merged.get("voice", "")
                gesture_text = merged.get("gesture", "")
                session_timestamp = max(r["timestamp"] for r in records)
                unified_command = llm_unify(voice_text, gesture_text)
                store_unified_instruction(
                    cursor,
                    session_id,
                    session_timestamp,
                    voice_text,
                    gesture_text,
                    unified_command,
                    liu_id,
                )
                mark_instructions_as_processed(cursor, session_id)
            offset += batch_size
            logger.info(f"Processed batch of {batch_size} records. Offset: {offset}")
    conn.commit()
    logger.info("Synchronization and unification complete.")


if __name__ == "__main__":
    synchronize_and_unify()



# database/connection.py


import logging
import os
from urllib.parse import urlparse

import psycopg2
from dotenv import load_dotenv
from psycopg2 import sql

from config.app_config import setup_logging

load_dotenv()

# Logging
debug_mode = os.getenv("DEBUG", "0") in ["1", "true", "True"]
log_level = os.getenv("LOG_LEVEL", "DEBUG" if debug_mode else "INFO").upper()
setup_logging(level=getattr(logging, log_level))
logger = logging.getLogger("PgDBConnection")


def get_connection():
    db_url = os.getenv("DATABASE_URL")
    if not db_url:
        raise EnvironmentError("‚ùå DATABASE_URL not found in .env")

    parsed = urlparse(db_url)
    db_name = parsed.path.lstrip("/")
    user = parsed.username
    password = parsed.password
    host = parsed.hostname
    port = parsed.port or 5432

    # ‚úÖ Ensure the database exists
    ensure_database_exists(user, password, host, port, db_name)

    # ‚úÖ Now safe to connect to it
    try:
        conn = psycopg2.connect(db_url)
        logger.info(f"‚úÖ Connected to database: {db_name}")
        return conn
    except Exception as e:
        logger.exception("‚ùå Failed to connect to target database.")
        raise


def ensure_database_exists(user, password, host, port, db_name):
    try:
        logger.info(f"üü¢ Checking if database '{db_name}' exists...")
        conn = psycopg2.connect(
            dbname="postgres",
            user=user,
            password=password,
            host=host,
            port=port,
        )
        conn.autocommit = True
        cur = conn.cursor()

        cur.execute("SELECT 1 FROM pg_database WHERE datname = %s", (db_name,))
        exists = cur.fetchone()
        if not exists:
            cur.execute(sql.SQL("CREATE DATABASE {}").format(sql.Identifier(db_name)))
            logger.info(f"‚úÖ Created missing database: {db_name}")
        else:
            logger.info(f"‚úÖ Database '{db_name}' already exists.")

        cur.close()
        conn.close()
    except Exception as e:
        logger.error(f"‚ùå Could not check/create database '{db_name}': {e}")
        raise



# database/db_handler.py

"""
DatabaseHandler Class and CLI Tool for PostgreSQL Database Management
This module provides a `DatabaseHandler` class to manage PostgreSQL database operations
such as creating tables, backing up data, restoring data, and populating the database
with sample data. It also includes a CLI tool for interacting with the database.
Classes:
--------
- DatabaseHandler:
Functions:
----------
- json_serializer(obj):
    Serializes datetime and memoryview objects for JSON compatibility.
- main_cli():
    Command-line interface for managing the PostgreSQL database.
DatabaseHandler Methods:
------------------------
- __init__():
    Initializes the database connection and cursor.
- backup_user_profiles(backup_dir=None):
    Backs up all user profiles from the 'users' table into a JSON file.
- restore_user_profiles(backup_dir=None, latest_only=True):
    Restores user profiles from the most recent backup.
- backup_database(backup_dir=DB_BACKUP_PATH):
    Creates a backup of the entire database using `pg_dump`.
- print_status():
    Logs the status of all tables in the database, including row counts.
- create_tables():
    Creates all tables defined in the schema.
- create_indexes():
    Creates all indexes defined in the schema.
- update_table_schemas():
    Placeholder for schema validation and dynamic alteration (not implemented).
- clear_tables():
    Truncates all tables and resets their identities.
- drop_all_tables():
    Drops all tables in the database.
- clear_camera_vision():
    Deletes all rows from the `camera_vision` table.
- populate_database():
    Populates the database with sample data using the `populate_db` module.
- close():
    Closes the database connection and cursor.
CLI Arguments:
--------------
- --clear:
    Truncate all tables and reset identities.
- --drop:
    Drop all tables.
- --create:
    Create all tables.
- --populate:
    Populate tables with sample data.
- --reset:
    Drop, create, and populate all tables (default action if no arguments are provided).
- --backup:
    Backup the database before making changes.
- --status:
    Show table row counts and status.
Usage:
------
Run the script with the desired CLI arguments to perform database operations.
If no arguments are provided, the script defaults to the `--reset` operation.
Example:
--------
$ python db_handler.py --reset
"""
import argparse
import binascii
import json
import logging
import os
import pickle
import subprocess
import sys
from datetime import datetime

import psycopg2
from dotenv import load_dotenv
from psycopg2 import Error as Psycopg2Error

from config.app_config import DB_BACKUP_PATH, PROFILE_BACKUP_PATH, setup_logging
from mini_project.database import populate_db, schema_sql
from mini_project.database.connection import get_connection

# Load .env variables
load_dotenv()


# Initialize logging with desired level (optional)
setup_logging(level=logging.INFO)
logger = logging.getLogger("PgDBaseHandler")


def json_serializer(obj):
    if isinstance(obj, datetime):
        return obj.isoformat()
    if isinstance(obj, memoryview):
        return obj.tobytes().hex()  # Hex string for JSON-safe backup
    raise TypeError(f"Type {type(obj)} not serializable")


class DatabaseHandler:
    """
    A class to handle database operations using PostgreSQL.
    """

    def __init__(self):
        try:
            self.conn = get_connection()
            self.conn.autocommit = False
            self.cursor = self.conn.cursor()
        except Psycopg2Error as e:
            logger.error(f"Error connecting to PostgreSQL database: {e}")
            raise

    def backup_user_profiles(self, backup_dir=None):
        """Backs up all user profiles from the 'users' table into a JSON file."""

        # ‚úÖ Check if 'users' table exists
        self.cursor.execute(
            """
            SELECT EXISTS (
                SELECT FROM information_schema.tables
                WHERE table_name = 'users'
            );
        """
        )
        exists = self.cursor.fetchone()[0]
        if not exists:
            logger.info(f"üî¥ 'users' table does not exist. Skipping profile backup.")
            return

        # Proceed with backup if table exists
        backup_dir = PROFILE_BACKUP_PATH
        backup_dir.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_path = backup_dir / f"user_profile_backup_{timestamp}.json"

        self.cursor.execute("SELECT * FROM users")
        rows = self.cursor.fetchall()
        colnames = [desc[0] for desc in self.cursor.description]

        users = [dict(zip(colnames, row)) for row in rows]

        with open(backup_path, "w", encoding="utf-8") as f:
            json.dump(users, f, indent=2, default=json_serializer)

        logger.info(f"‚úÖ Backed up {len(users)} users to: {backup_path}")

    def restore_user_profiles(self, backup_dir=None, latest_only=True):
        """Restores user profiles from the most recent backup."""
        backup_dir = PROFILE_BACKUP_PATH
        if not backup_dir.exists():
            logger.info(f"‚ö†Ô∏è No backup folder found.")
            return

        backup_files = sorted(
            backup_dir.glob("user_profile_backup_*.json"), reverse=True
        )
        if not backup_files:
            logger.info(f"‚ö†Ô∏è No backup files found.")
            return

        backup_path = backup_files[0] if latest_only else backup_files[-1]
        with open(backup_path, "r", encoding="utf-8") as f:
            users = json.load(f)

        restored_users = []
        for user in users:
            # üîÅ Decode binary fields
            if "face_encoding" in user and isinstance(user["face_encoding"], str):
                user["face_encoding"] = psycopg2.Binary(
                    binascii.unhexlify(user["face_encoding"])
                )

            if "voice_embedding" in user and isinstance(user["voice_embedding"], str):
                user["voice_embedding"] = psycopg2.Binary(
                    binascii.unhexlify(user["voice_embedding"])
                )

            # ‚ùå Drop user_id so PostgreSQL can auto-generate it
            user.pop("user_id", None)

            placeholders = ", ".join(["%s"] * len(user))
            columns = ", ".join(user.keys())
            values = list(user.values())
            # sql = f"INSERT INTO users ({columns}) VALUES ({placeholders}) ON CONFLICT (liu_id) DO NOTHING"
            sql = f"""
            INSERT INTO users ({columns}) VALUES ({placeholders})
            ON CONFLICT (liu_id) DO UPDATE SET
                face_encoding = EXCLUDED.face_encoding,
                voice_embedding = EXCLUDED.voice_embedding,
                preferences = EXCLUDED.preferences,
                profile_image_path = EXCLUDED.profile_image_path,
                interaction_memory = EXCLUDED.interaction_memory,
                last_updated = CURRENT_TIMESTAMP
            """
            self.cursor.execute(sql, values)
            restored_users.append(
                user["liu_id"]
            )  # ‚úÖ Collect ID instead of logging every one
        self.conn.commit()
        # ‚úÖ One neat log line:
        logger.info(
            f"‚úÖ Restored {len(restored_users)} user profile(s): {', '.join(restored_users)}"
        )

    def backup_database(self, backup_dir=DB_BACKUP_PATH):

        os.makedirs(backup_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%y%m%d_%H%M%S")
        backup_file = os.path.join(backup_dir, f"backup_{timestamp}.sql")

        db_url = os.getenv("DATABASE_URL")
        if not db_url:
            logger.error("DATABASE_URL not set in environment.")
            return

        try:
            logger.info(f"üóÉÔ∏è  Backing up database to {backup_file}...")
            subprocess.run(["pg_dump", db_url, "-f", backup_file], check=True)
            logger.info("‚úÖ Backup completed successfully.")
        except Exception as e:
            logger.error("Database backup failed: %s", e, exc_info=True)

    def print_status(self):
        logger.info("üß™ Checking database table status...")

        self.cursor.execute(
            """
            SELECT table_name
            FROM information_schema.tables
            WHERE table_schema = 'public'
            ORDER BY table_name;
        """
        )
        tables = [row[0] for row in self.cursor.fetchall()]

        for table in tables:
            self.cursor.execute(f"SELECT COUNT(*) FROM {table}")
            count = self.cursor.fetchone()[0]
            logger.info(f"üí° {table}: {count} rows")

    def create_tables(self):
        try:
            for create_sql in schema_sql.tables.values():
                self.cursor.execute(create_sql)
            self.conn.commit()
            logger.info("‚úÖ All tables created successfully.")
        except Psycopg2Error as e:
            logger.error(f"Error creating tables: {e}")
            self.conn.rollback()
            raise

    def create_indexes(self):
        try:
            for index in schema_sql.indexes:
                self.cursor.execute(index)
            self.conn.commit()
        except Psycopg2Error as e:
            logger.error(f"Error creating indexes: {e}")
            self.conn.rollback()
            raise

    def update_table_schemas(self):
        """
        Schema validation and dynamic alteration is not implemented for PostgreSQL.
        Consider using a migration tool like Alembic.
        """
        logger.info(
            "Skipping schema update. Use migrations for PostgreSQL schema changes."
        )

    def clear_tables(self):
        try:
            tables = list(schema_sql.tables.keys())
            truncate_query = (
                "TRUNCATE TABLE " + ", ".join(tables) + " RESTART IDENTITY CASCADE;"
            )
            self.cursor.execute(truncate_query)
            self.conn.commit()
            logger.info("‚úÖ All tables cleared successfully.")
        except Psycopg2Error as e:
            logger.error(f"Error clearing tables: {e}")
            self.conn.rollback()
            raise

    def drop_all_tables(self):
        try:

            self.cursor.execute(
                """
                DO $$ DECLARE
                    r RECORD;
                BEGIN
                    FOR r IN (SELECT tablename FROM pg_tables WHERE schemaname = 'public') LOOP
                        EXECUTE 'DROP TABLE IF EXISTS ' || quote_ident(r.tablename) || ' CASCADE';
                    END LOOP;
                END $$;
            """
            )
            self.conn.commit()
            logger.info("‚úÖ All tables dropped successfully.")
        except Psycopg2Error as e:
            logger.error(f"Error dropping tables: {e}")
            self.conn.rollback()
            raise

    def clear_camera_vision(self):
        try:
            # Delete all rows from the camera_vision table
            self.cursor.execute("DELETE FROM camera_vision;")
            self.conn.commit()
            logger.info("‚úÖ [DB] Cleared camera_vision table.")
        except Exception as e:
            logger.error(f"Error clearing camera_vision table: {e}")
            self.conn.rollback()

    def populate_database(self):
        try:
            self.clear_tables()

            populator = populate_db.DatabasePopulator(self.cursor)

            populator.populate_usd_data()
            populator.populate_users()
            populator.populate_sequence_library()
            populator.populate_operation_library()
            populator.populate_gesture_library()
            populator.populate_isaac_sim_gui()
            populator.populate_task_templates()
            populator.populate_skills()
            populator.populate_instructions()
            populator.populate_states()
            # populator.populate_operation_sequence()
            # populator.populate_sort_order()
            populator.populate_task_preferences()
            populator.populate_interaction_memory()
            populator.populate_simulation_results()

            populator.populate_manual_operations()

            self.conn.commit()
            logger.info("‚úÖ Database populated successfully.")
        except Psycopg2Error as e:
            logger.error(f"Database population failed: {e}")
            self.conn.rollback()

    def close(self):
        if self.cursor:
            self.cursor.close()
        if self.conn:
            self.conn.close()


def main_cli():
    parser = argparse.ArgumentParser(description="Manage PostgreSQL database.")

    parser.add_argument(
        "--clear", action="store_true", help="Truncate all tables and reset identities."
    )
    parser.add_argument("--drop", action="store_true", help="Drop all tables.")
    parser.add_argument("--create", action="store_true", help="Create all tables.")
    parser.add_argument(
        "--populate", action="store_true", help="Populate tables with sample data."
    )
    parser.add_argument(
        "--reset",
        action="store_true",
        help="Drop, create, and populate all tables.",
    )
    parser.add_argument(
        "--backup",
        action="store_true",
        help="Backup the database before making changes.",
    )

    parser.add_argument(
        "--status", action="store_true", help="Show table row counts and status."
    )

    args = parser.parse_args()

    # Show help if no arguments are passed
    if not any(vars(args).values()):
        parser.print_help()
        print("No arguments provided ‚Äî defaulting to --reset.\n")
        args.reset = True

    try:
        db = DatabaseHandler()

        if args.reset:
            logger.info(
                f"üß† Resetting the database (backup, drop, create, populate)..."
            )
            db.backup_user_profiles()  # üîê Backup BEFORE dropping
            db.backup_database()
            db.drop_all_tables()
            db.create_tables()
            db.create_indexes()
            db.populate_database()
            db.restore_user_profiles()  # ‚ôªÔ∏è Restore users after everything else
            db.print_status()
            print("\nüéâ All Done! Database is healthy and ready.\n")

        else:
            if args.backup:
                print("Backing up database...")
                db.backup_database()
            if args.status:
                db.print_status()
                return
            if args.drop:
                print("Dropping all tables...")
                db.drop_all_tables()
            if args.create:
                print("Creating tables...")
                db.create_tables()
                db.create_indexes()
            if args.clear:
                print("Clearing tables...")
                db.clear_tables()
            if args.populate:
                print("Populating tables...")
                db.populate_database()

        db.close()

    except Exception as e:
        logger.exception("Unexpected error during CLI execution")
        print(f"‚ùå Error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main_cli()



# database/populate_db.py
"""This module contains the `DatabasePopulator` class, which is responsible for populating various database tables
with predefined data. The class provides methods to populate tables such as `sequence_library`, `usd_data`,
`operation_library`, `users`, `skills`, `instructions`, `states`, `operation_sequence`, `sort_order`,
`task_templates`, `task_preferences`, `interaction_memory`, `simulation_results`, and `manual_operations`.
Each method in the class corresponds to a specific table in the database and inserts predefined data into it.
The methods ensure that data is inserted only when necessary, avoiding duplication by checking existing records
where applicable.
Classes:
    - DatabasePopulator: A class that provides methods to populate database tables with predefined data.
Methods:
    - populate_sequence_library: Populates the `sequence_library` table with predefined sequences.
    - populate_usd_data: Populates the `usd_data` table with predefined USD data.
    - populate_operation_library: Populates the `operation_library` table with predefined operations.
    - populate_users: Populates the `users` table with predefined user data.
    - populate_skills: Populates the `skills` table with predefined skill data.
    - populate_instructions: Populates the `instructions` table with predefined instruction data.
    - populate_states: Populates the `states` table with predefined state data.
    - populate_operation_sequence: Populates the `operation_sequence` table with predefined operation sequences.
    - populate_sort_order: Populates the `sort_order` table with predefined object sorting orders.
    - populate_task_templates: Populates the `task_templates` table with predefined task templates.
    - populate_task_preferences: Populates the `task_preferences` table with predefined user task preferences.
    - populate_interaction_memory: Populates the `interaction_memory` table with predefined interaction data.
    - populate_simulation_results: Populates the `simulation_results` table with predefined simulation results.
    - populate_manual_operations: Populates various operation parameter tables with predefined manual operation data.

"""
import logging
from config.app_config import setup_logging

setup_logging(level=logging.INFO)
logger = logging.getLogger("DBasePopulator")


class DatabasePopulator:
    def __init__(self, cursor):
        self.cursor = cursor

    def populate_sequence_library(self):
        """
        Populate sequence_library table with provided data.
        """
        sequence_library = [
            (
                "pick",
                "PickBlockRd",
                "Pick up an object",
                "gripper is clear",
                "object in gripper",
                1,
                "aaa",
                False,  # Boolean value for is_runnable_exit
            ),
            (
                "travel",
                "ReachToPlacementRd",
                "Move to the target location",
                "object in gripper",
                "at target location",
                1,
                "aaa",
                False,
            ),
            (
                "drop",
                "DropRd",
                "Drop the object",
                "at target location",
                "object dropped",
                1,
                "aaa",
                False,
            ),
            (
                "screw",
                "ScrewRd",
                "Screw the object two times",
                "task complete",
                "robot at home position",
                1,
                "thresh_met and self.context.gripper_has_block",
                True,
            ),
            (
                "rotate",
                "RotateRd",
                "Rotate the object once",
                "task complete",
                "robot at home position",
                1,
                "thresh_met and self.context.gripper_has_block",
                True,
            ),
            (
                "go_home",
                "GoHome",
                "Return to the home position",
                "task complete",
                "robot at home position",
                1,
                "aaa",
                False,
            ),
        ]
        insert_query = """
            INSERT INTO sequence_library
            (sequence_name, node_name, description, conditions, post_conditions, is_runnable_count, is_runnable_condition, is_runnable_exit)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s);
        """
        try:
            self.cursor.executemany(insert_query, sequence_library)
            logger.info(
                "‚úÖ Successfully populated the sequence_library table with data!"
            )
        except Exception as e:
            logger.error(f"‚ùå Error inserting sequence_library: {e}")
            self.cursor.connection.rollback()

    def populate_gesture_library(self):
        """
        Populate gesture_library table with predefined gesture data.
        """
        gesture_data = [
            (
                "thumbs_up",
                "Approval",
                "The thumb is raised above the index finger.",
                '{"threshold": 0.0}',
            ),
            (
                "open_hand",
                "Stop",
                "All fingers are extended, signaling stop.",
                '{"threshold": 0.0}',
            ),
            (
                "pointing",
                "Select Object",
                "The index finger is extended while other fingers are curled.",
                '{"threshold": 0.0}',
            ),
            (
                "closed_fist",
                "Grab",
                "The hand is clenched into a fist.",
                '{"threshold": 0.0}',
            ),
            (
                "victory",
                "Confirm",
                "The hand forms a V-shape with the index and middle fingers extended.",
                '{"threshold": 0.0}',
            ),
            (
                "ok_sign",
                "OK",
                "The thumb and index finger are touching to form a circle.",
                '{"threshold": 0.05}',
            ),
        ]

        insert_query = """
            INSERT INTO gesture_library (gesture_type, gesture_text, natural_description, config)
            VALUES (%s, %s, %s, %s);
        """
        try:
            self.cursor.executemany(insert_query, gesture_data)
            logger.info(
                "‚úÖ Successfully populated the gesture_library table with data!"
            )
        except Exception as e:
            logger.error(f"‚ùå Error inserting gesture data: {e}")
            self.cursor.connection.rollback()

    def populate_usd_data(self):
        """
        Populate usd_data table with provided data.
        """
        usd_data = [
            (
                1,
                "Fixture.usd",
                "GeometryPrim",
                "/fixture_description/Slide_Fixture.usd",
                0.0,
                0.0,
                0.1,
                0.1,
                0.1,
                "/World/fixtureprim",
                0.20,
                -0.07,
                0.094,
                False,
            ),
            (
                2,
                "Slide_Holder.usd",
                "GeometryPrim",
                "/fixture_description/Slide_Holder.usd",
                0.0,
                0.0,
                0.1,
                0.1,
                0.1,
                "/World/fixtureprim",
                40,
                17,
                8,
                False,
            ),
            (
                3,
                "Slide.usd",
                "RigidPrim",
                "/fixture_description/Slide1.usd",
                0.002,
                0.016,
                1,
                1,
                0.06,
                "/World/fixtureprim/Fixture",
                0.0,
                0.0,
                0.0,
                True,
            ),
            (
                4,
                "Cuboid.usd",
                "RigidPrim",
                "/yumi_basic_shapes/cuboid.usd",
                0.025,
                0.015,
                0.1,
                0.11,
                0.1,
                "/World/fixtureprim",
                0.55475,
                -0.116,
                0.113,
                True,
            ),
            (
                5,
                "Cylinder.usd",
                "RigidPrim",
                "/yumi_basic_shapes/cylinder.usd",
                0.025,
                0.015,
                0.1,
                0.1,
                0.1,
                "/World/fixtureprim",
                0.41475,
                -0.116,
                0.113,
                True,
            ),
            (
                6,
                "Cube.usd",
                "RigidPrim",
                "/yumi_basic_shapes/cube.usd",
                0.025,
                0.015,
                0.1,
                0.1,
                0.1,
                "/World/fixtureprim",
                0.34475,
                -0.116,
                0.113,
                True,
            ),
            (
                7,
                "Hexagon.usd",
                "RigidPrim",
                "/yumi_basic_shapes/hexagon.usd",
                0.025,
                0.015,
                0.1,
                0.1,
                0.1,
                "/World/fixtureprim",
                0.48475,
                -0.116,
                0.113,
                True,
            ),
        ]
        insert_query = """
            INSERT INTO usd_data (
                    sequence_id, usd_name, type_of_usd, repository,block_height,block_pick_height, scale_x, scale_y, scale_z,
                    prim_path, initial_pos_x, initial_pos_y, initial_pos_z, register_obstacle
                )
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        """
        try:
            self.cursor.executemany(insert_query, usd_data)
            logger.info("‚úÖ Successfully populated the usd_data table with data!")
        except Exception as e:
            logger.error(f"‚ùå Error inserting usd_data: {e}")
            self.cursor.connection.rollback()

    def populate_operation_library(self):
        self.cursor.execute("SELECT COUNT(*) FROM operation_library")
        count = self.cursor.fetchone()[0]
        if count > 0:
            return
        operation_library = [
            (
                "slide_sorting",  # operation_name	Internal task name
                "pick, travel, drop",  # List of phases like pick/travel/drop
                "Sort slides by shape and color into trays",  # Human-readable explanation
                [
                    "sort",
                    "slides",
                    "slide",
                    "sorting",
                    "tray",
                    "sort slides",
                ],  # trigger_keywords	Voice-trigger words
                "detect_slides_pgSQL.py",  # Path to run on vision system
                True,  # is_triggerable, Whether LLM can set trigger = TRUE
                False,  # trigger,  Defaults to FALSE (used as flag)
                "idle",  # state, 'idle' initially
                None,  # last_triggered_time, None until first run
            ),
            (
                "shape_stacking",
                "pick, travel, drop",
                "Stack blocks of shapes based on their type and color",
                ["stack", "stacking", "shapes", "shape", "shape stacking"],
                "detect_shapes_pgSQL.py",
                True,
                False,
                "idle",
                None,
            ),
        ]
        insert_query = """
            INSERT INTO operation_library (
                operation_name, task_order, description, trigger_keywords,
                script_path, is_triggerable, trigger, state, last_triggered
            )
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s);
        """
        try:
            self.cursor.executemany(insert_query, operation_library)
            logger.info(
                "‚úÖ Successfully populated the operation_library table with data!"
            )
        except Exception as e:
            logger.error(f"‚ùå Error inserting operation_library: {e}")
            self.cursor.connection.rollback()

    def populate_users(self):
        self.cursor.execute("SELECT COUNT(*) FROM users")
        count = self.cursor.fetchone()[0]
        if count > 0:
            return
        users = [
            (
                "Yumi",
                "Robot",
                "yumi100",
                "yumi100@lab.liu.ai",
                '{"likes": ["AI", "Robotics"]}',
                "/images/yumi001.jpg",
                '{"last_task": "Assistance", "successful_tasks": 100}',
                "robot",
            ),
            (
                "Oscar",
                "Ikechukwu",
                "oscik559",
                "oscik559@student.liu.se",
                '{"likes": ["AI", "Robotics"]}',
                "/images/oscik559.jpg",
                '{"last_task": "Pick object", "successful_tasks": 5}',
                "admin",
            ),
            (
                "Rahul",
                "Chiramel",
                "rahch515",
                "rahch515@student.liu.se",
                '{"likes": ["Aeroplanes", "Automation"]}',
                "/images/rahch515.jpg",
                '{"last_task": "Screw object", "successful_tasks": 10}',
                "admin",
            ),
            (
                "Sanjay",
                "Nambiar",
                "sanna58",
                "sanjay.nambiar@liu.se",
                '{"likes": ["Programming", "Machine Learning"]}',
                "/images/sanna58.jpg",
                '{"last_task": "Slide object", "successful_tasks": 7}',
                "admin",
            ),
            (
                "Mehdi",
                "Tarkian",
                "mehta77",
                "mehdi.tarkian@liu.se",
                '{"likes": ["Running", "Cats"]}',
                "/images/mehta77.jpg",
                '{"last_task": "Drop object", "successful_tasks": 2}',
                "team",
            ),
            (
                "Marie",
                "Jonsson",
                "marjo33",
                "marie.s.jonsson@liu.se",
                '{"likes": ["Robots", "Composites"]}',
                "/images/marjo33.jpg",
                '{"last_task": "Fix robot battery", "successful_tasks": 2}',
                "team",
            ),
            (
                "Aref",
                "Aghaee",
                "areag806",
                "areag806@student.liu.se",
                '{"likes": ["CATIA", "Turbine Blades"]}',
                "/images/areag806.jpg",
                '{"last_task": "Pick object", "successful_tasks": 5}',
                "guest",
            ),
            (
                "Thomson",
                "Kalliyath",
                "thoka981",
                "thoka981@student.liu.se",
                '{"likes": ["Omniverse", "Aeronautics"]}',
                "/images/thoka981.jpg",
                '{"last_task": "Pick object", "successful_tasks": 5}',
                "guest",
            ),
            (
                "Hamideh",
                "Pourrasoul",
                "hampo845",
                "hampo845@student.liu.se",
                '{"likes": ["CATIA", "Turbine Blades"]}',
                "/images/hampo845.jpg",
                '{"last_task": "Pick object", "successful_tasks": 5}',
                "guest",
            ),
            (
                "John",
                "Ashish",
                "johas759",
                "johas759@student.liu.se",
                '{"likes": ["python", "aircraft wings"]}',
                "/images/johas759.jpg",
                '{"last_task": "Pick object", "successful_tasks": 5}',
                "guest",
            ),
            (
                "Danial",
                "Nikpey",
                "danni741",
                "danni741@student.liu.se",
                '{"likes": ["vb.net", "aircraft wings"]}',
                "/images/danni741.jpg",
                '{"last_task": "Pick object", "successful_tasks": 5}',
                "guest",
            ),
        ]
        insert_query = """
            INSERT INTO users (first_name, last_name, liu_id, email, preferences, profile_image_path, interaction_memory, role)
            VALUES (%s, %s, %s, %s, %s,%s,%s, %s);
        """
        try:
            self.cursor.executemany(insert_query, users)
            logger.info("‚úÖ Successfully populated the users table with data!")
        except Exception as e:
            logger.error(f"‚ùå Error inserting users: {e}")
            self.cursor.connection.rollback()

    def populate_skills(self):
        skills = [
            (
                "pick",
                "Pick up object",
                '{"gripper_force": 0.5}',
                '{"gripper": true}',
                2.5,
            ),
            ("place", "Place object", '{"precision": 0.01}', '{"vision": true}', 3.0),
        ]
        insert_query = """
            INSERT INTO skills (skill_name, description, parameters, required_capabilities, average_duration)
            VALUES (%s, %s, %s, %s, %s);
        """
        self.cursor.executemany(insert_query, skills)

    def populate_instructions(self):
        """
        Populate instructions table with provided data.
        This ensures that foreign key references in child tables (e.g., interaction_memory, simulation_results)
        will find matching instruction rows.
        """
        instructions = [
            (1, "voice", "en", "command", False, "Pick up object", None, 0.95),
            (2, "text", "en", "command", False, "Place object", None, 0.90),
        ]
        insert_query = """
            INSERT INTO instructions (user_id, modality, language, instruction_type, processed, content, sync_id, confidence)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s);
        """
        self.cursor.executemany(insert_query, instructions)

    def populate_states(self):
        states = [
            (
                "LiftState",
                "Lift an object vertically",
                "gripper is clear",
                "object in gripper",
                1,
            ),
            (
                "SlideState",
                "Slide an object along X-axis",
                "object in gripper",
                "at target location",
                1,
            ),
        ]
        insert_query = """
            INSERT INTO states (task_name, description, conditions, post_conditions, sequence_id)
            VALUES (%s, %s, %s, %s, %s);
        """
        self.cursor.executemany(insert_query, states)

    def populate_operation_sequence(self):
        operation_sequence = [
            (1, 1, "pick", "Slide_1"),
            (2, 2, "travel", "Slide_1"),
            (3, 3, "drop", "Slide_1"),
            (4, 1, "pick", "Slide_2"),
            (5, 2, "travel", "Slide_2"),
            (6, 3, "drop", "Slide_2"),
            (7, 1, "pick", "Slide_3"),
            (8, 2, "travel", "Slide_3"),
            (9, 3, "drop", "Slide_3"),
            (10, 6, "go_home", ""),
        ]

        insert_query = """
            INSERT INTO operation_sequence (
                operation_id, sequence_id, sequence_name, object_name
            ) VALUES (%s, %s, %s, %s)
        """
        self.cursor.executemany(insert_query, operation_sequence)

    def populate_sort_order(self):
        sort_order = [
            ("Slide_1", "Green"),
            ("Slide_2", "Orange"),
            ("Slide_3", "Pink"),
        ]
        insert_query = """
            INSERT INTO sort_order (object_name, object_color)
            VALUES (%s, %s);
        """
        self.cursor.executemany(insert_query, sort_order)

    def populate_task_templates(self):
        task_templates = [
            (
                "sort",
                "Default sorting task: pick, move, drop",
                ["pick", "travel", "drop"],
            ),
            (
                "assemble",
                "Assembly involves pick and screw",
                ["pick", "travel", "screw", "go_home"],
            ),
            (
                "inspect",
                "Inspect task involves scan and return",
                ["travel", "inspect", "go_home"],
            ),
            (
                "cleanup",
                "Cleanup task involves pick, rotate, drop",
                ["pick", "rotate", "drop"],
            ),
        ]
        insert_query = """
            INSERT INTO task_templates (task_name, description, default_sequence)
            VALUES (%s, %s, %s);
        """
        self.cursor.executemany(insert_query, task_templates)

    def populate_task_preferences(self):
        task_preferences = [
            (1, "Pick Object", '{"time": "morning", "location": "shelf"}'),
            (2, "Place Object", '{"time": "afternoon", "location": "table"}'),
            (1, "Inspect Object", '{"tools": ["camera", "gripper"]}'),
        ]
        insert_query = """
            INSERT INTO task_preferences (user_id, task_name, preference_data)
            VALUES (%s, %s, %s);
        """
        self.cursor.executemany(insert_query, task_preferences)

    def populate_interaction_memory(self):
        interactions = [
            (
                1,
                1,
                "task_query",
                '{"task": "Pick Object"}',
                "2023-10-01 09:00:00",
                "2023-10-01 17:00:00",
            ),
            (
                2,
                1,
                "preference_update",
                '{"preference": {"time": "morning"}}',
                "2023-10-01 09:00:00",
                "2023-10-01 17:00:00",
            ),
            (
                1,
                2,
                "task_execution",
                '{"status": "success", "task": "Place Object"}',
                "2023-10-02 09:00:00",
                "2023-10-02 17:00:00",
            ),
        ]
        insert_query = """
            INSERT INTO interaction_memory (user_id, instruction_id, interaction_type, data, start_time, end_time)
            VALUES (%s, %s, %s, %s, %s, %s);
        """
        self.cursor.executemany(insert_query, interactions)

    def populate_simulation_results(self):
        results = [
            (1, True, '{"accuracy": 0.95, "time_taken": 2.5}', "No errors"),
            (2, False, '{"accuracy": 0.8, "time_taken": 3.0}', "Gripper misalignment"),
        ]
        insert_query = """
            INSERT INTO simulation_results (instruction_id, success, metrics, error_log)
            VALUES (%s, %s, %s, %s);
        """
        self.cursor.executemany(insert_query, results)

    def populate_manual_operations(self):
        self.cursor.execute(
            "SELECT COUNT(*) FROM camera_vision WHERE usd_name = 'Slide.usd'"
        )
        slide_usd_count = self.cursor.fetchone()[0]
        if slide_usd_count > 0:

            # -- Screw Operation Parameters
            self.cursor.execute(
                "SELECT sequence_id, object_name FROM operation_sequence WHERE sequence_name = 'screw'"
            )
            screw_data = self.cursor.fetchall()
            screw_op_parameters = [
                (i + 1, seq_id, obj_name, i % 2 == 0, 3, 0, False)
                for i, (seq_id, obj_name) in enumerate(screw_data)
            ]
            self.cursor.executemany(
                """
                INSERT INTO screw_op_parameters (
                    operation_order, sequence_id, object_id,
                    rotation_dir, number_of_rotations,
                    current_rotation, operation_status
                )
                VALUES (%s, %s, %s, %s, %s, %s, %s)
                """,
                screw_op_parameters,
            )

            # -- Rotate State Parameters
            self.cursor.execute(
                "SELECT sequence_id, operation_order, object_id FROM screw_op_parameters"
            )
            rotate_data = self.cursor.fetchall()
            rotate_state_parameters = [
                (seq_id, operation_order, obj_id, 90, False)
                for (seq_id, operation_order, obj_id) in rotate_data
            ]
            self.cursor.executemany(
                """
                INSERT INTO rotate_state_parameters (
                    sequence_id, operation_order, object_id,
                    rotation_angle, operation_status
                )
                VALUES (%s, %s, %s, %s, %s)
                """,
                rotate_state_parameters,
            )

            # -- Pick Operation Parameters
            self.cursor.execute(
                "SELECT sequence_id, object_name FROM operation_sequence WHERE sequence_name = 'pick'"
            )
            pick_data = self.cursor.fetchall()
            pick_op_parameters = [
                (i + 1, obj_name, False, "y", 0.01, False)
                for i, (seq_id, obj_name) in enumerate(pick_data)
            ]
            self.cursor.executemany(
                """
                INSERT INTO pick_op_parameters (
                    operation_order, object_id, slide_state_status, slide_direction, distance_travel, operation_status
                )
                VALUES (%s, %s, %s, %s, %s, %s)

                """,
                pick_op_parameters,
            )

            # -- Travel Operation Parameters
            self.cursor.execute(
                "SELECT sequence_id, object_name FROM operation_sequence WHERE sequence_name = 'travel'"
            )
            travel_data = self.cursor.fetchall()
            travel_op_parameters = [
                (i + 1, obj_name, 0.085, "y-axis", False)
                for i, (_, obj_name) in enumerate(travel_data)
            ]

            self.cursor.executemany(
                """
                INSERT INTO travel_op_parameters (
                    operation_order, object_id, travel_height, gripper_rotation, operation_status
                )
                VALUES (%s, %s, %s, %s, %s)
                """,
                travel_op_parameters,
            )

            # -- Drop Operation Parameters
            self.cursor.execute(
                "SELECT sequence_id, object_name FROM operation_sequence WHERE sequence_name = 'drop'"
            )
            drop_data = self.cursor.fetchall()

            drop_op_parameters = [
                (i + 1, obj_name, -0.003, False)
                for i, (_, obj_name) in enumerate(drop_data)
            ]

            self.cursor.executemany(
                """
                INSERT INTO drop_op_parameters (
                    operation_order, object_id, drop_height, operation_status
                )
                VALUES (%s, %s, %s, %s)
                """,
                drop_op_parameters,
            )
        else:
            self.cursor.execute(
                "SELECT sequence_id, object_name FROM operation_sequence WHERE sequence_name = 'screw'"
            )
            screw_data = self.cursor.fetchall()
            screw_op_parameters = [
                (i + 1, seq_id, obj_name, i % 2 == 0, 3, 0, False)
                for i, (seq_id, obj_name) in enumerate(screw_data)
            ]
            self.cursor.executemany(
                """
                INSERT INTO screw_op_parameters (
                    operation_order, sequence_id, object_id,
                    rotation_dir, number_of_rotations,
                    current_rotation, operation_status
                )
                VALUES (%s, %s, %s, %s, %s, %s, %s)
                """,
                screw_op_parameters,
            )

            # -- Rotate State Parameters
            self.cursor.execute(
                "SELECT sequence_id, operation_order, object_id FROM screw_op_parameters"
            )
            rotate_data = self.cursor.fetchall()
            rotate_state_parameters = [
                (seq_id, operation_order, obj_id, 90, False)
                for (seq_id, operation_order, obj_id) in rotate_data
            ]
            self.cursor.executemany(
                """
                INSERT INTO rotate_state_parameters (
                    sequence_id, operation_order, object_id,
                    rotation_angle, operation_status
                )
                VALUES (%s, %s, %s, %s, %s)
                """,
                rotate_state_parameters,
            )

            # -- Pick Operation Parameters
            self.cursor.execute(
                "SELECT sequence_id, object_name FROM operation_sequence WHERE sequence_name = 'pick'"
            )
            pick_data = self.cursor.fetchall()
            pick_op_parameters = [
                (i + 1, obj_name, False, "y", 0.01, False)
                for i, (seq_id, obj_name) in enumerate(pick_data)
            ]
            self.cursor.executemany(
                """
                INSERT INTO pick_op_parameters (
                    operation_order, object_id, slide_state_status, slide_direction, distance_travel, operation_status
                )
                VALUES (%s, %s, %s, %s, %s, %s)

                """,
                pick_op_parameters,
            )

            # -- Travel Operation Parameters
            self.cursor.execute(
                "SELECT sequence_id, object_name FROM operation_sequence WHERE sequence_name = 'travel'"
            )
            travel_data = self.cursor.fetchall()
            travel_op_parameters = [
                (i + 1, obj_name, 0.085, "z-axis", False)
                for i, (_, obj_name) in enumerate(travel_data)
            ]

            self.cursor.executemany(
                """
                INSERT INTO travel_op_parameters (
                    operation_order, object_id, travel_height, gripper_rotation, operation_status
                )
                VALUES (%s, %s, %s, %s, %s)
                """,
                travel_op_parameters,
            )

            # -- Drop Operation Parameters
            self.cursor.execute(
                "SELECT sequence_id, object_name FROM operation_sequence WHERE sequence_name = 'drop'"
            )
            drop_data = self.cursor.fetchall()

            drop_op_parameters = [
                (i + 1, obj_name, -0.003, False)
                for i, (_, obj_name) in enumerate(drop_data)
            ]

            self.cursor.executemany(
                """
                INSERT INTO drop_op_parameters (
                    operation_order, object_id, drop_height, operation_status
                )
                VALUES (%s, %s, %s, %s)
                """,
                drop_op_parameters,
            )

    def populate_isaac_sim_gui(self):
        isaac_sim_gui = [("Start", False), ("Reset", False), ("Load", False)]
        insert_query = """
            INSERT INTO isaac_sim_gui (gui_feature, operation_status)
            VALUES (%s, %s);
        """
        try:
            self.cursor.executemany(insert_query, isaac_sim_gui)
            logger.info(
                "‚úÖ Successfully populated the isaac_sim_gui table with data!"
            )
        except Exception as e:
            logger.error(f"‚ùå Error inserting isaac_sim_gui: {e}")
            self.cursor.connection.rollback()



# database/schema_sql.py
"""This module defines the database schema and indexes for a project. It uses an
OrderedDict to organize the SQL statements for creating tables and indexes.
Tables:
- `users`: Stores user information, including personal details, preferences, and roles.
- `usd_data`: Contains data related to USD (Universal Scene Description) objects.
- `isaac_sim_gui`: Tracks GUI features and their operation statuses.
- `sequence_library`: Stores sequences with metadata, conditions, and runnability.
- `operation_library`: Defines operations, their metadata,
"""
from collections import OrderedDict

tables = OrderedDict(
    [
        (
            "users",
            """
            CREATE TABLE IF NOT EXISTS users (
                user_id SERIAL PRIMARY KEY,
                first_name TEXT NOT NULL,
                last_name TEXT NOT NULL,
                liu_id TEXT UNIQUE,
                email TEXT UNIQUE,
                preferences TEXT,
                profile_image_path TEXT,
                interaction_memory TEXT,
                face_encoding BYTEA,
                voice_embedding BYTEA,
                role TEXT CHECK(role IN ('robot','team', 'guest', 'admin')) DEFAULT 'guest',
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );
    """,
        ),
        (
            "usd_data",
            """
            CREATE TABLE IF NOT EXISTS usd_data (
                sequence_id INTEGER PRIMARY KEY,
                usd_name TEXT NOT NULL,
                type_of_usd TEXT NOT NULL,
                repository TEXT NOT NULL,
                block_height FLOAT NOT NULL,
                block_pick_height FLOAT NOT NULL,
                scale_x FLOAT NOT NULL,
                scale_y FLOAT NOT NULL,
                scale_z FLOAT NOT NULL,
                prim_path TEXT NOT NULL,
                initial_pos_x FLOAT NOT NULL,
                initial_pos_y FLOAT NOT NULL,
                initial_pos_z FLOAT NOT NULL,
                register_obstacle BOOLEAN NOT NULL
            );
    """,
        ),
        (
            "isaac_sim_gui",
            """
            CREATE TABLE IF NOT EXISTS isaac_sim_gui (
                sequence_id SERIAL PRIMARY KEY,
                gui_feature TEXT NOT NULL,
                operation_status BOOLEAN DEFAULT FALSE
            );
    """,
        ),
        (
            "sequence_library",
            """
            CREATE TABLE IF NOT EXISTS sequence_library (
                sequence_id SERIAL PRIMARY KEY,
                sequence_name TEXT NOT NULL,
                skill_name TEXT,
                node_name TEXT,
                description TEXT,
                conditions TEXT,
                post_conditions TEXT,
                is_runnable_count INTEGER,
                is_runnable_condition TEXT,
                is_runnable_exit BOOLEAN
            );
    """,
        ),
        (
            "operation_library",
            """
            CREATE TABLE IF NOT EXISTS operation_library (
                id SERIAL PRIMARY KEY,

                -- Core operation metadata
                operation_name TEXT UNIQUE NOT NULL,       -- e.g., 'tray_holder_detection'
                task_order TEXT,                           -- e.g., 'detect, pick, place'
                description TEXT,                          -- Human-readable label

                -- Script & trigger metadata
                trigger_keywords TEXT[],                   -- Words that trigger this operation
                script_path TEXT,                          -- e.g., 'camera_vision_pgSQL_rs.py'
                is_triggerable BOOLEAN DEFAULT TRUE,       -- Can be triggered from LLM

                -- Trigger state tracking
                trigger BOOLEAN DEFAULT FALSE,             -- Used by LLM to trigger script
                state TEXT DEFAULT 'idle',                -- idle | triggered | running
                last_triggered TIMESTAMP                   -- When it was last set to TRUE
            );
    """,
        ),
        (
            "access_logs",
            """
            CREATE TABLE IF NOT EXISTS access_logs (
                log_id INTEGER PRIMARY KEY,
                user_id INTEGER NOT NULL,
                action_type TEXT NOT NULL,
                target_table TEXT,
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (user_id) REFERENCES users(user_id)
            );
    """,
        ),
        (
            "skills",
            """
            CREATE TABLE IF NOT EXISTS skills (
                skill_id SERIAL PRIMARY KEY,
                skill_name TEXT NOT NULL UNIQUE,
                description TEXT,
                parameters TEXT,
                required_capabilities TEXT,
                average_duration REAL
            );
    """,
        ),
        (
            "instructions",
            """
            CREATE TABLE IF NOT EXISTS instructions (
                id INTEGER GENERATED ALWAYS AS IDENTITY PRIMARY KEY,
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                user_id INTEGER,
                modality TEXT CHECK(modality IN ('voice', 'gesture', 'text')),
                language TEXT NOT NULL,
                instruction_type TEXT NOT NULL,
                processed BOOLEAN DEFAULT FALSE,
                content TEXT,
                sync_id INTEGER UNIQUE,
                confidence REAL CHECK(confidence BETWEEN 0 AND 1),
                FOREIGN KEY (user_id) REFERENCES users(user_id) ON DELETE CASCADE
            );
    """,
        ),
        (
            "states",
            """
            CREATE TABLE IF NOT EXISTS states (
                task_id SERIAL PRIMARY KEY,
                task_name TEXT NOT NULL,
                description TEXT,
                conditions TEXT,
                post_conditions TEXT,
                sequence_id INTEGER,
                FOREIGN KEY (sequence_id) REFERENCES sequence_library(sequence_id) ON DELETE CASCADE
            );
    """,
        ),
        (
            "screw_op_parameters",
            """
            CREATE TABLE IF NOT EXISTS screw_op_parameters (
                sequence_id SERIAL PRIMARY KEY,
                operation_order INTEGER NOT NULL,
                object_id TEXT NOT NULL,
                rotation_dir BOOLEAN NOT NULL,
                number_of_rotations INTEGER NOT NULL,
                current_rotation INTEGER NOT NULL,
                operation_status BOOLEAN NOT NULL
            );
    """,
        ),
        (
            "pick_op_parameters",
            """
            CREATE TABLE IF NOT EXISTS pick_op_parameters (
                sequence_id SERIAL PRIMARY KEY,
                operation_order INTEGER NOT NULL,
                object_id TEXT NOT NULL,
                slide_state_status BOOLEAN NOT NULL,
                slide_direction TEXT NOT NULL,
                distance_travel FLOAT NOT NULL,
                operation_status BOOLEAN NOT NULL
            );
    """,
        ),
        (
            "drop_op_parameters",
            """
                CREATE TABLE IF NOT EXISTS drop_op_parameters (
                sequence_id SERIAL PRIMARY KEY,
                operation_order INTEGER NOT NULL,
                object_id TEXT NOT NULL,

                drop_height FLOAT NOT NULL,

                -- ‚úÖ New columns for specifying drop location
                drop_pos_x FLOAT DEFAULT NULL,
                drop_pos_y FLOAT DEFAULT NULL,
                drop_pos_z FLOAT DEFAULT NULL,

                operation_status BOOLEAN NOT NULL
            );
    """,
        ),
        (
            "travel_op_parameters",
            """
            CREATE TABLE IF NOT EXISTS travel_op_parameters (
                sequence_id SERIAL PRIMARY KEY,
                operation_order INTEGER NOT NULL,
                object_id TEXT NOT NULL,
                travel_height FLOAT NOT NULL,
                gripper_rotation TEXT NOT NULL,
                operation_status BOOLEAN NOT NULL
            );
    """,
        ),
        (
            "lift_state_parameters",
            """
            CREATE TABLE IF NOT EXISTS lift_state_parameters (
                sequence_id SERIAL PRIMARY KEY,
                operation_order INTEGER NOT NULL,
                object_id TEXT NOT NULL,
                lift_height FLOAT NOT NULL,
                operation_status BOOLEAN NOT NULL
            );
    """,
        ),
        (
            "slide_state_parameters",
            """
            CREATE TABLE IF NOT EXISTS slide_state_parameters (
                sequence_id SERIAL PRIMARY KEY,
                operation_order INTEGER NOT NULL,
                object_id TEXT NOT NULL,
                lift_distance FLOAT NOT NULL,
                pos_x FLOAT NOT NULL,
                pos_y FLOAT NOT NULL,
                pos_z FLOAT NOT NULL,
                operation_status BOOLEAN NOT NULL
            );
    """,
        ),
        (
            "rotate_state_parameters",
            """
            CREATE TABLE IF NOT EXISTS rotate_state_parameters (
                sequence_id SERIAL PRIMARY KEY,
                operation_order INTEGER NOT NULL,
                object_id TEXT NOT NULL,
                rotation_angle FLOAT NOT NULL,
                operation_status BOOLEAN NOT NULL
            );
    """,
        ),
        (
            "camera_vision",
            """
            CREATE TABLE IF NOT EXISTS camera_vision (
                object_id SERIAL PRIMARY KEY,
                object_name TEXT NOT NULL,
                object_color TEXT NOT NULL,
                color_code FLOAT8[],
                pos_x DOUBLE PRECISION NOT NULL,
                pos_y DOUBLE PRECISION NOT NULL,
                pos_z DOUBLE PRECISION NOT NULL,
                rot_x DOUBLE PRECISION NOT NULL,
                rot_y DOUBLE PRECISION NOT NULL,
                rot_z DOUBLE PRECISION NOT NULL,
                usd_name TEXT NOT NULL,
                last_detected TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );
    """,
        ),
        (
            "task_templates",
            """
            CREATE TABLE IF NOT EXISTS task_templates (
                task_id SERIAL PRIMARY KEY,
                task_name TEXT UNIQUE NOT NULL,
                description TEXT,
                default_sequence TEXT[]
            );
    """,
        ),
        (
            "sort_order",
            """
            CREATE TABLE IF NOT EXISTS sort_order (
                order_id SERIAL PRIMARY KEY,
                object_name TEXT,
                object_color TEXT
            );
    """,
        ),
        (
            "interaction_memory",
            """
            CREATE TABLE IF NOT EXISTS interaction_memory (
                interaction_id SERIAL PRIMARY KEY,
                user_id INTEGER,
                instruction_id INTEGER,
                interaction_type TEXT,
                data TEXT,
                start_time TIMESTAMP,
                end_time TIMESTAMP,
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (user_id) REFERENCES users(user_id) ON DELETE CASCADE,
                FOREIGN KEY (instruction_id) REFERENCES instructions(id) ON DELETE CASCADE
            );
    """,
        ),
        (
            "simulation_results",
            """
            CREATE TABLE IF NOT EXISTS simulation_results (
                simulation_id SERIAL PRIMARY KEY,
                instruction_id INTEGER,
                success BOOLEAN,
                metrics TEXT,
                error_log TEXT,
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (instruction_id) REFERENCES instructions(id) ON DELETE CASCADE
            );
    """,
        ),
        (
            "task_preferences",
            """
            CREATE TABLE IF NOT EXISTS task_preferences (
                preference_id SERIAL PRIMARY KEY,
                user_id INTEGER,
                task_id TEXT,
                task_name TEXT,
                preference_data TEXT,
                FOREIGN KEY (user_id) REFERENCES users(user_id) ON DELETE CASCADE
            );
    """,
        ),
        (
            "unified_instructions",
            """
            CREATE TABLE IF NOT EXISTS unified_instructions (
                id INTEGER GENERATED ALWAYS AS IDENTITY PRIMARY KEY,
                session_id TEXT,
                timestamp TIMESTAMP,
                liu_id TEXT,
                voice_command TEXT,
                gesture_command TEXT,
                unified_command TEXT,
                confidence FLOAT CHECK(confidence BETWEEN 0 AND 1),
                processed BOOLEAN DEFAULT FALSE,
                FOREIGN KEY (liu_id) REFERENCES users(liu_id) ON DELETE CASCADE
            );
    """,
        ),
        (
            "operation_sequence",
            """
            CREATE TABLE IF NOT EXISTS operation_sequence (
                id SERIAL PRIMARY KEY,
                operation_id INTEGER NOT NULL,         -- order of execution
                sequence_id INTEGER NOT NULL,          -- FK to sequence_library
                sequence_name TEXT NOT NULL,           -- redundant but helpful for readability
                object_name TEXT,
                command_id INTEGER,                    -- FK to unified_instructions(id)
                processed BOOLEAN DEFAULT FALSE,       -- ‚úÖ track if step is completed
                execution_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

                FOREIGN KEY (sequence_id) REFERENCES sequence_library(sequence_id),
                FOREIGN KEY (command_id) REFERENCES unified_instructions(id)
            );
    """,
        ),
        (
            "gesture_library",
            """
            CREATE TABLE IF NOT EXISTS gesture_library (
                id INTEGER GENERATED ALWAYS AS IDENTITY PRIMARY KEY,
                gesture_type TEXT UNIQUE NOT NULL,
                gesture_text TEXT NOT NULL,
                natural_description TEXT,
                config JSONB

            );
    """,
        ),
        (
            "gesture_instructions",
            """
            CREATE TABLE IF NOT EXISTS gesture_instructions (
                id INTEGER GENERATED ALWAYS AS IDENTITY PRIMARY KEY,
                session_id TEXT NOT NULL,
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                gesture_text TEXT NOT NULL,
                natural_description TEXT,
                confidence REAL,
                hand_label TEXT,
                processed BOOLEAN DEFAULT FALSE
            );
    """,
        ),
        (
            "voice_instructions",
            """
            CREATE TABLE IF NOT EXISTS voice_instructions (
                id INTEGER GENERATED ALWAYS AS IDENTITY PRIMARY KEY,
                session_id TEXT NOT NULL,
                transcribed_text TEXT NOT NULL,
                confidence REAL,
                language TEXT NOT NULL,
                processed BOOLEAN DEFAULT FALSE,
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );
    """,
        ),
        (
            "instruction_operation_sequence",
            """
            CREATE TABLE IF NOT EXISTS instruction_operation_sequence (
                task_id SERIAL PRIMARY KEY,
                instruction_id INTEGER,
                skill_id INTEGER,
                skill_name TEXT,
                sequence_id INTEGER,
                sequence_name TEXT NOT NULL,
                object_id INTEGER,
                object_name TEXT,
                status TEXT CHECK(status IN ('pending', 'in_progress', 'completed', 'failed')) DEFAULT 'pending',
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (instruction_id) REFERENCES instructions(id) ON DELETE CASCADE,
                FOREIGN KEY (skill_id) REFERENCES skills(skill_id),
                FOREIGN KEY (sequence_id) REFERENCES sequence_library(sequence_id),
                FOREIGN KEY (object_id) REFERENCES camera_vision(object_id)
            );
        """,
        ),
        (
            "task_history",
            """
            CREATE TABLE IF NOT EXISTS task_history (
                id SERIAL PRIMARY KEY,
                command_text TEXT,
                generated_plan JSONB,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );
        """,
        ),
    ]
)

indexes = [
    "CREATE INDEX IF NOT EXISTS idx_users_liu_id ON users(liu_id);",
    "CREATE INDEX IF NOT EXISTS idx_users_email ON users(email);",
    "CREATE INDEX IF NOT EXISTS idx_interaction_memory_user_id ON interaction_memory(user_id);",
    "CREATE INDEX IF NOT EXISTS idx_task_preferences_user_id ON task_preferences(user_id);",
    "CREATE INDEX IF NOT EXISTS idx_conv_memory_instruction ON interaction_memory(instruction_id);",
    "CREATE INDEX IF NOT EXISTS idx_skills_name ON skills(skill_name);",
    "CREATE INDEX IF NOT EXISTS idx_simulation_instruction ON simulation_results(instruction_id);",
    "CREATE INDEX IF NOT EXISTS idx_operation_sequence_object ON instruction_operation_sequence(object_id);",
    "CREATE INDEX IF NOT EXISTS idx_camera_vision_last_detected ON camera_vision(last_detected);",
    "CREATE INDEX IF NOT EXISTS idx_user_prefs_task ON task_preferences(user_id, task_id);",
    "CREATE INDEX IF NOT EXISTS idx_voice_session_id ON voice_instructions(session_id);",
    "CREATE INDEX IF NOT EXISTS idx_voice_processed ON voice_instructions(processed);",
    "CREATE INDEX IF NOT EXISTS idx_task_templates_name ON task_templates(task_name);",
    "CREATE INDEX IF NOT EXISTS idx_task_templates_sequence ON task_templates(default_sequence);",
    "CREATE INDEX IF NOT EXISTS idx_unified_instructions_session_id ON unified_instructions(session_id);",
]



# config/app_config.py

"""
Configuration module for the mini_project application.

This module defines various configuration settings required for the application,
including paths, thresholds, and validation patterns.

- for environment variables and the prompt template.
"""

import logging.config
import os
import tempfile
from pathlib import Path

import yaml

# Define the base directory
BASE_DIR = Path(__file__).resolve().parent.parent  # mini_project_repo/ directory path

# Use network share or local database file: be aware of potential issues with file locking on a network share.
# DB_PATH = Path(r"\\ad.liu.se\coop\i\industrialrobotsetup\sequences.db")
DB_PATH = BASE_DIR / "assets" / "db_data" / "sequences.db"

# Use postgreSQL database.
# DB_URL = "dbname=sequences_db user=oscar password=oscik559 host=localhost"
DB_URL = "postgresql://oscar:oscik559@localhost:5432/sequences_db"

DB_BACKUP_PATH = BASE_DIR / "assets" / "db_backups"
PROFILE_BACKUP_PATH = BASE_DIR / "assets" / "db_user_backups"

# Face recognition utilities
FACIAL_DATA_PATH = BASE_DIR / "assets" / "face_encodings"
FACE_CAPTURE_PATH = BASE_DIR / "assets" / "face_capture"
IDENTIFICATION_FRAMES = (
    2  # Constant to control how many frames are used for identification averaging.
)
TIMEDELAY = 0.5  # Time delay between frames for face detection

# Face recognition parameters
FACE_MATCH_THRESHOLD = 0.6  # Threshold for face matching
MAX_ENCODINGS_PER_USER = 5  # Maximum number of encodings per user
AUTO_CAPTURE_FRAME_COUNT = (
    5  # Number of consecutive frames with detected face for auto-capture
)

# Voice recognition parameters
VOICE_DATA_PATH = BASE_DIR / "assets" / "voice_embeddings"
VOICE_CAPTURE_PATH = BASE_DIR / "assets" / "voice_capture"
TRANSCRIPTION_SENTENCE = "Artificial intelligence enables machines to recognize patterns, process language, and make decisions."
MAX_RETRIES = 3
VOICE_MATCH_THRESHOLD = 0.7  # Cosine similarity threshold for identification.

TEMP_AUDIO_PATH = BASE_DIR / "assets" / "temp_audio"

# Camera vision utilities
CAMERA_DATA_PATH = BASE_DIR / "assets" / "camera_data"


# Email and ID validation patterns
LIU_ID_PATTERN = r"^[a-z]{3,5}\d{3}$"  # Example: oscik559
EMAIL_PATTERN = r"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+$"

# === Voice Processing Configurations ===
VOICE_PROCESSING_CONFIG = {
    "recording": {
        "temp_audio_path": str(
            TEMP_AUDIO_PATH / "voice_recording.wav"
        ),  # Path to save recorded audio
        "sampling_rate": 16000,  # Audio sample rate (Hz)
        "max_duration": 60,  # Maximum recording duration (seconds)
        "initial_silence_duration": 15,  # Silence allowed before speech starts (seconds)
        "post_speech_silence_duration": 2,  # Silence allowed after speech ends (seconds)
        "calibration_duration": 0.5,  # Duration for ambient noise calibration (seconds)
        "amplitude_margin": 100,  # Margin above noise floor for speech detection
        "frame_duration": 0.03,  # Duration of each audio frame (seconds)
    },
    "whisper": {
        "model": "large-v3",  # Whisper model to use
        "device": "cuda",  # Device for Whisper (e.g., "cuda" or "cpu")
        "compute_type": "float16",  # Compute type for Whisper
    },
    "database": {
        "db_path": str(DB_PATH),  # Path to the SQLite database
    },
}
MAX_TRANSCRIPTION_RETRIES = 1
MIN_DURATION_SEC = 1.5
VOICE_TTS_SETTINGS = {
    "speed": 165,
    "use_gtts": True,
    "ping_sound_path": str(BASE_DIR / "assets" / "sound_effects" / "ping.wav"),
    "ding_sound_path": str(BASE_DIR / "assets" / "sound_effects" / "ding.wav"),
    "voice_index": 2,  # 0 = Hazel, 1 = David, 2 = Zira (example for Windows)
}
import json

NOISE_CACHE_PATH = BASE_DIR / "assets" / "config_cache" / "noise_floor_cache.json"
CHAT_MEMORY_FOLDER = BASE_DIR / "assets" / "chat_memory"
WAKEWORD_PATH = BASE_DIR / "assets" / "robot_wakewords" / "hey_yummy.ppn"


# LLM Settings


# === Gesture Recognition Settings ===
MIN_DETECTION_CONFIDENCE = 0.7
MIN_TRACKING_CONFIDENCE = 0.5
MAX_NUM_HANDS = 2
FRAME_SKIP = 2

# === synchronizer LLM Settings ===
BATCH_SIZE = int(os.getenv("BATCH_SIZE", 1000))
LLM_MAX_RETRIES = int(os.getenv("LLM_MAX_RETRIES", 3))
LLM_MODEL = os.getenv("LLM_MODEL", "mistral:latest")

UNIFY_PROMPT_TEMPLATE = (
    "Role: Command Unifier. Combine voice commands (primary) with gesture cues (supplementary).\n"
    "Rules:\n"
    "1. Preserve ALL details from the voice command.\n"
    "2. Integrate gestures ONLY if they add context (e.g., direction, emphasis).\n"
    "3. NEVER omit voice content unless the gesture explicitly contradicts it.\n"
    "4. Output format: Plain text, no markdown or JSON.\n\n"
    "Examples:\n"
    "- Voice: 'Turn right', Gesture: 'Pointing up' ‚Üí 'Turn right upward'\n"
    "- Voice: 'Stop', Gesture: '' ‚Üí 'Stop'\n\n"
    "Voice Instruction: {voice_text}\n"
    "Gesture Instruction: {gesture_text}\n"
    "Unified Command:"
)


# Function to validate paths at runtime
def validate_paths() -> None:
    """
    Validates that all defined paths exist or can be created.
    Raises an exception if any path is invalid.
    """
    paths_to_check = [
        FACIAL_DATA_PATH,
        FACE_CAPTURE_PATH,
        VOICE_DATA_PATH,
        TEMP_AUDIO_PATH,
        CAMERA_DATA_PATH,
        DB_PATH.parent,
        NOISE_CACHE_PATH,
        DB_BACKUP_PATH,
        PROFILE_BACKUP_PATH,
    ]

    for path in paths_to_check:
        if not path.exists():
            try:
                path.mkdir(parents=True, exist_ok=True)
                logger.info(f"Created missing directory: {path}")
            except OSError as e:
                logger.error(f"Failed to create directory {path}: {e}", exc_info=True)
                raise RuntimeError(f"Failed to create directory {path}: {e}")


def setup_logging(level: int = logging.INFO) -> None:
    logging.basicConfig(
        level=level,
        # format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
        format="[%(levelname)s] %(name)s: %(message)s",
        # datefmt="%Y-%m-%d %H:%M:%S",
        datefmt="%H:%M:%S",
    )


def load_logging_config():
    config_path = Path(__file__).parent / "logging_config.yaml"
    if config_path.exists():
        with open(config_path, "r") as f:
            config = yaml.safe_load(f)
        logging.config.dictConfig(config)
    else:
        setup_logging()


# Set up logging
load_logging_config()
logger = logging.getLogger("App_CONFIG")

# Automatically validate paths when this module is imported
# validate_paths()


if __name__ == "__main__":
    setup_logging()
    logger.info("üü° Checking paths...")
    try:
        validate_paths()
        logger.info(f"‚úÖ All paths are valid.")
    except RuntimeError as e:
        logger.error(f"Path validation failed: {e}")


# config/constants.py


# Synchronizer Constants
VOICE_TABLE = "voice_instructions"
GESTURE_TABLE = "gesture_instructions"
PROCESSED_COL = "processed"
UNIFIED_TABLE = "unified_instructions"

# Voice Processor Constants
WHISPER_LANGUAGE_NAMES = {
    "en": "english",
    "zh": "chinese",
    "de": "german",
    "es": "spanish",
    "ru": "russian",
    "ko": "korean",
    "fr": "french",
    "ja": "japanese",
    "pt": "portuguese",
    "tr": "turkish",
    "pl": "polish",
    "ca": "catalan",
    "nl": "dutch",
    "ar": "arabic",
    "sv": "swedish",
    "it": "italian",
    "id": "indonesian",
    "hi": "hindi",
    "fi": "finnish",
    "vi": "vietnamese",
    "he": "hebrew",
    "uk": "ukrainian",
    "el": "greek",
    "ms": "malay",
    "cs": "czech",
    "ro": "romanian",
    "da": "danish",
    "hu": "hungarian",
    "ta": "tamil",
    "no": "norwegian",
    "th": "thai",
    "ur": "urdu",
    "hr": "croatian",
    "bg": "bulgarian",
    "lt": "lithuanian",
    "la": "latin",
    "mi": "maori",
    "ml": "malayalam",
    "cy": "welsh",
    "sk": "slovak",
    "te": "telugu",
    "fa": "persian",
    "lv": "latvian",
    "bn": "bengali",
    "sr": "serbian",
    "az": "azerbaijani",
    "sl": "slovenian",
    "kn": "kannada",
    "et": "estonian",
    "mk": "macedonian",
    "br": "breton",
    "eu": "basque",
    "is": "icelandic",
    "hy": "armenian",
    "ne": "nepali",
    "mn": "mongolian",
    "bs": "bosnian",
    "kk": "kazakh",
    "sq": "albanian",
    "sw": "swahili",
    "gl": "galician",
    "mr": "marathi",
    "pa": "punjabi",
    "si": "sinhala",
    "km": "khmer",
    "sn": "shona",
    "yo": "yoruba",
    "so": "somali",
    "af": "afrikaans",
    "oc": "occitan",
    "ka": "georgian",
    "be": "belarusian",
    "tg": "tajik",
    "sd": "sindhi",
    "gu": "gujarati",
    "am": "amharic",
    "yi": "yiddish",
    "lo": "lao",
    "uz": "uzbek",
    "fo": "faroese",
    "ht": "haitian creole",
    "ps": "pashto",
    "tk": "turkmen",
    "nn": "nynorsk",
    "mt": "maltese",
    "sa": "sanskrit",
    "lb": "luxembourgish",
    "my": "myanmar",
    "bo": "tibetan",
    "tl": "tagalog",
    "mg": "malagasy",
    "as": "assamese",
    "tt": "tatar",
    "haw": "hawaiian",
    "ln": "lingala",
    "ha": "hausa",
    "ba": "bashkir",
    "jw": "javanese",
    "su": "sundanese",
    "yue": "cantonese",
}


WAKE_RESPONSES = [
    "yes?",
    "I'm listening",
    "what's up?",
    "go ahead.",
    "at your service.",
    "hello?",
    "I'm here!",
    "you called?",
    "what do you want?",
    "I'm listening.",
    "hi?",
    "what is it?",
]
GENERAL_TRIGGERS = {
    "weather",
    "who is",
    "link√∂ping",
    "university",
    "say something",
    "remind",
    "recap",
    "explain",
    "lab",
    "appreciate",
    "motivate",
    "how are we doing",
    "tell us about",
    "introduce",
    "location",
    "where is",
    "project",
    "working on",
    "colleague",
    "summary",
}
TASK_VERBS = {
    "sort",
    "move",
    "place",
    "assemble",
    "pick",
    "drop",
    "grab",
    "stack",
    "push",
    "pull",
}
QUESTION_WORDS = {
    "what",
    "where",
    "which",
    "who",
    "how many",
    "is there",
    "are there",
}
CONFIRM_WORDS = {
    "yes",
    "sure",
    "okay",
    "go ahead",
    "absolutely",
    "yep",
    "definitely",
    "please do",
}
CANCEL_WORDS = {"no", "cancel", "not now", "stop", "never mind", "don't"}
TRIGGER_WORDS = {
    "detect",
    "refresh",
    "capture",
    "scan",
    "trigger",
}


version: 1
disable_existing_loggers: False

formatters:
  standard:
    format: "[%(asctime)s] %(levelname)s - %(name)s - %(message)s"
    # datefmt: "%Y-%m-%d %H:%M:%S"
    datefmt: "%H:%M:%S"

handlers:
  console:
    class: logging.StreamHandler
    level: DEBUG
    formatter: standard
    stream: ext://sys.stdout

loggers:
  mini_project:
    level: DEBUG
    handlers: [console]
    propagate: no

root:
  level: INFO
  handlers: [console]


# authentication/face_auth.py
"""
Classes:
    FaceUtils:
        A utility class for face detection, bounding box drawing, and encoding selection.
    FaceAuthSystem:
        The main class for managing face authentication, including:
        - Preloading and managing face encodings.
        - Building and refreshing a FAISS index for similarity search.
        - Capturing faces in automatic, manual, or multi-face modes.
        - Registering new users with face and voice data.
        - Identifying users based on captured face data.
Functions:
    VideoCaptureContext(index: int = 0):
        A context manager for handling video capture resources.
    main():
        Entry point for the FaceAuthSystem application. Initializes directories and starts the system.
Constants:
    AUTO_CAPTURE_FRAME_COUNT: Number of consecutive frames required for automatic face capture.
    EMAIL_PATTERN: Regular expression pattern for validating email addresses.
    FACE_CAPTURE_PATH: Path for saving captured face images.
    FACE_MATCH_THRESHOLD: Threshold for face similarity matching.
    FACIAL_DATA_PATH: Path for storing facial data.
    IDENTIFICATION_FRAMES: Number of frames to process during user identification.
    LIU_ID_PATTERN: Regular expression pattern for validating LIU IDs.
    MAX_ENCODINGS_PER_USER: Maximum number of face encodings to store per user.
    TEMP_AUDIO_PATH: Path for storing temporary audio files.
    TIMEDELAY: Delay between frames during identification.
    VOICE_DATA_PATH: Path for storing voice data.
"""


# import sys
import logging
import pickle
import re
import time
from contextlib import contextmanager
from typing import Dict, List, Optional, Tuple

import cv2
import face_recognition
import faiss  # For fast similarity search
import numpy as np
import psycopg2

from config.app_config import (
    AUTO_CAPTURE_FRAME_COUNT,
    EMAIL_PATTERN,
    FACE_CAPTURE_PATH,
    FACE_MATCH_THRESHOLD,
    FACIAL_DATA_PATH,
    IDENTIFICATION_FRAMES,
    LIU_ID_PATTERN,
    MAX_ENCODINGS_PER_USER,
    TEMP_AUDIO_PATH,
    TIMEDELAY,
    VOICE_DATA_PATH,
    setup_logging,
)
from mini_project.authentication._voice_auth import VoiceAuth
from mini_project.database.connection import get_connection

logger = logging.getLogger("FaceAuthSystem")


# Context manager for handling an video session.
@contextmanager
def VideoCaptureContext(index: int = 0):
    cap = cv2.VideoCapture(index)
    try:
        yield cap
    finally:
        cap.release()


class FaceUtils:

    @staticmethod
    def detect_faces(frame: np.ndarray) -> Tuple[List[tuple], List[np.ndarray]]:
        """
        Detect faces and compute encodings in a frame.

        Returns:
            A tuple of (face_locations, face_encodings).
        """
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        face_locations = face_recognition.face_locations(rgb_frame)
        face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)
        return face_locations, face_encodings

    @staticmethod
    def draw_bounding_boxes(
        frame: np.ndarray, face_locations: List[tuple]
    ) -> np.ndarray:
        """Draw bounding boxes around detected faces."""
        for top, right, bottom, left in face_locations:
            cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)
        return frame

    @staticmethod
    def draw_text(
        frame: np.ndarray,
        text: str,
        position: Tuple[int, int] = (10, 30),
        font_scale: float = 0.7,
        color: Tuple[int, int, int] = (0, 255, 0),
        thickness: int = 2,
    ) -> None:
        """Overlay text on the frame at the given position."""
        cv2.putText(
            frame,
            text,
            position,
            cv2.FONT_HERSHEY_SIMPLEX,
            font_scale,
            color,
            thickness,
        )

    @staticmethod
    def select_best_face(
        face_encodings: List[np.ndarray], face_locations: List[tuple]
    ) -> int:
        """
        Select the largest face when multiple faces are detected.

        Returns:
            The index of the face with the largest bounding box.
        """
        face_sizes = [
            (bottom - top) * (right - left)
            for (top, right, bottom, left) in face_locations
        ]
        return int(np.argmax(face_sizes))


class FaceAuthSystem:
    def __init__(self) -> None:
        self.conn = get_connection()
        self.cursor = self.conn.cursor()
        self.face_utils = FaceUtils()
        self.known_encodings: Dict[str, dict] = self._preload_encodings()
        self.faiss_index: Optional[faiss.IndexFlatL2] = self._build_faiss_index()

    def _preload_encodings(self) -> Dict[str, dict]:
        """Preload all face encodings from the database."""
        encodings: Dict[str, dict] = {}
        try:
            self.cursor.execute(
                "SELECT user_id, first_name, last_name, liu_id, face_encoding, voice_embedding FROM users"
            )
            users = self.cursor.fetchall()
            for user in users:
                if user[4]:
                    encodings[user[3]] = {
                        "user_id": user[0],
                        "first_name": user[1],
                        "last_name": user[2],
                        "liu_id": user[3],
                        "encodings": pickle.loads(user[4].tobytes()),
                        "voice_embedding": (
                            pickle.loads(user[5].tobytes())[0] if user[5] else None
                        ),
                    }
        except psycopg2.Error as e:
            logger.error("üî¥ Database error during encoding preload: %s", e)
        return encodings

    def _build_faiss_index(self) -> Optional[faiss.IndexFlatL2]:
        """Build a FAISS index for fast face encoding search."""
        if not self.known_encodings:
            logger.debug("No known encodings to build FAISS index.")
            return None

        all_encodings: List[np.ndarray] = []
        self.user_ids: List[str] = []  # Maps FAISS index entries to LIU IDs.
        for liu_id, user in self.known_encodings.items():
            for encoding in user["encodings"]:
                all_encodings.append(encoding)
                self.user_ids.append(liu_id)
        if not all_encodings:
            logger.debug("No encodings found after processing known_encodings.")
            return None

        all_encodings_np = np.array(all_encodings, dtype=np.float32)
        index = faiss.IndexFlatL2(all_encodings_np.shape[1])
        index.add(all_encodings_np)
        logger.debug("FAISS index built with %d encodings.", all_encodings_np.shape[0])
        return index

    def _refresh_index(self) -> None:
        """Refresh the in-memory known encodings and rebuild the FAISS index."""
        self.known_encodings = self._preload_encodings()
        self.faiss_index = self._build_faiss_index()
        logger.info("‚úÖ FAISS index and known encodings refreshed.")

    def _validate_user_input(self, liu_id: str, email: str) -> bool:
        """Validate LIU ID and email formats."""
        if not re.match(LIU_ID_PATTERN, liu_id):
            logger.error("üî¥ Invalid LIU ID format. Expected format: abc123")
            return False
        if not re.match(EMAIL_PATTERN, email):
            logger.error("üî¥ Invalid email format")
            return False
        return True

    def _process_frame(
        self, cap: cv2.VideoCapture, instruction: str
    ) -> Tuple[Optional[np.ndarray], List[tuple], List[np.ndarray]]:
        """
        Read a frame from the camera, process it (face detection and bounding boxes),
        and overlay instruction text.

        Returns:
            A tuple of (frame, face_locations, face_encodings) or (None, [], []) if frame read fails.
        """
        ret, frame = cap.read()
        if not ret:
            logger.debug("No frame retrieved from camera.")
            return None, [], []
        face_locations, face_encodings = self.face_utils.detect_faces(frame)
        frame = self.face_utils.draw_bounding_boxes(frame, face_locations)
        self.face_utils.draw_text(frame, instruction)
        return frame, face_locations, face_encodings

    def _capture_face_auto(self) -> Optional[Tuple[np.ndarray, np.ndarray]]:
        """
        Capture a face in automatic mode.
        Uses a consecutive detection counter to ensure stable detection.
        """
        with VideoCaptureContext(0) as cap:
            if not cap.isOpened():
                logger.error("üî¥ Error: Camera not accessible.")
                return None
            consecutive_detections = (
                0  # Count of consecutive frames with exactly one face.
            )
            captured_frame: Optional[np.ndarray] = None
            captured_encoding: Optional[np.ndarray] = None

            while True:
                frame, _, face_encodings = self._process_frame(
                    cap, "Face detected, capturing..."
                )
                if frame is None:
                    logger.debug("Frame processing failed; exiting auto capture loop.")
                    break

                if len(face_encodings) == 1:
                    consecutive_detections += 1
                    self.face_utils.draw_text(
                        frame,
                        f"Detected: {consecutive_detections}/{AUTO_CAPTURE_FRAME_COUNT}",
                        position=(10, 60),
                    )
                    if consecutive_detections >= AUTO_CAPTURE_FRAME_COUNT:
                        captured_frame = frame
                        captured_encoding = face_encodings[0]
                        logger.debug("Auto capture threshold reached; capturing face.")
                        break
                else:
                    # Reset the counter if no face or multiple faces are detected.
                    consecutive_detections = 0

                cv2.imshow("Face Capture - Auto", frame)
                key = cv2.waitKey(1) & 0xFF
                if key == ord("q"):
                    logger.info("Auto capture aborted by user.")
                    break

            cv2.destroyAllWindows()
            if captured_frame is not None and captured_encoding is not None:
                return captured_frame, captured_encoding
            return None

    def _capture_face_manual(self) -> Optional[Tuple[np.ndarray, np.ndarray]]:
        """
        Capture a face in manual mode.
        The user must press 's' to save when a single face is detected.
        """
        with VideoCaptureContext(0) as cap:
            if not cap.isOpened():
                logger.error("üî¥ Error: Camera not accessible.")
                return None

            captured_frame: Optional[np.ndarray] = None
            captured_encoding: Optional[np.ndarray] = None

            while True:
                frame, _, face_encodings = self._process_frame(
                    cap, "Press 's' to save, 'q' to quit"
                )
                if frame is None:
                    logger.debug(
                        "Frame processing failed; exiting manual capture loop."
                    )
                    break

                if len(face_encodings) >= 1:
                    self.face_utils.draw_text(
                        frame, "Face detected!", position=(10, 60)
                    )

                cv2.imshow("Face Capture - Manual", frame)
                key = cv2.waitKey(1) & 0xFF
                if key == ord("q"):
                    logger.info("Manual capture aborted by user.")
                    break
                if key == ord("s"):
                    if len(face_encodings) == 1:
                        captured_frame = frame
                        captured_encoding = face_encodings[0]
                        logger.debug("Face captured in manual mode.")
                        break
                    else:
                        logger.warning(
                            "No face or multiple faces detected; cannot capture."
                        )

            cv2.destroyAllWindows()
            if captured_frame is not None and captured_encoding is not None:
                return captured_frame, captured_encoding
            return None

    def _capture_face(
        self, capture_mode: str = "auto"
    ) -> Optional[Tuple[np.ndarray, np.ndarray]]:
        """Capture a face using the specified mode (auto/manual)."""
        if capture_mode == "auto":
            return self._capture_face_auto()
        else:
            return self._capture_face_manual()

    def _capture_face_multi(
        self,
    ) -> Optional[Tuple[np.ndarray, List[np.ndarray], List[tuple]]]:
        """
        Capture a single frame and return all detected face encodings and locations.
        This mode is used for identification whether one or multiple faces are present.
        """
        with VideoCaptureContext(0) as cap:
            if not cap.isOpened():
                logger.error("üî¥ Error: Camera not accessible.")
                return None
            ret, frame = cap.read()
            if not ret:
                logger.error("üî¥ Failed to capture frame for multi-face detection.")
                return None
            face_locations, face_encodings = self.face_utils.detect_faces(frame)
            return frame, face_encodings, face_locations

    def register_user(self) -> None:
        """
        Registers a new user or updates an existing user's face encoding in the database.
        This method captures a user's face, collects their personal details, and stores
        the information in the database. If the user already exists (based on LIU ID or email),
        it prompts the user to confirm whether to update their face encoding.
        Steps:
        1. Captures the user's face encoding.
        2. Collects user details: first name, last name, LIU ID, and email.
        3. Validates the user input.
        4. Checks if the user already exists in the database:
           - If the user exists, prompts for confirmation to update the face encoding.
           - If the user does not exist, creates a new user record.
        5. Updates or inserts the user's face encoding and other details in the database.
        6. Refreshes the face recognition index.
        Returns:
            None
        Raises:
            psycopg2.Error: If there is a database error during the registration process.
        Logs:
            - Logs success or failure messages for each step of the process.
            - Logs errors if face capture fails or if required inputs are missing.
        Notes:
            - The maximum number of face encodings stored per user is enforced.
            - The face image is saved to a predefined path for the user's profile.
        """
        frame_encoding = self._capture_face("manual")
        if not frame_encoding:
            logger.error("üî¥ Face capture failed during registration.")
            return False

        frame, encoding = frame_encoding
        logger.info("üü¢ Face captured for registration.")

        # Gather registration details once.
        first_name = input("üö´ Enter your first name: ").strip()
        last_name = input("üö´ Enter your last name: ").strip()
        liu_id = input("üö´ Enter your LIU ID (e.g. abcxy123): ").strip()
        email = input("üö´ Enter your Email: ").strip()

        if not first_name or not last_name:
            logger.error("üî¥ First name and last name cannot be empty.")
            return False

        if not self._validate_user_input(liu_id, email):
            return False

        try:
            with self.conn:
                cursor = self.conn.cursor()
                # Query for an existing user with the same LIU ID or email.
                cursor.execute(
                    "SELECT user_id, face_encoding FROM users WHERE liu_id = %s OR email = %s",
                    (liu_id, email),
                )
                existing = cursor.fetchone()

                if existing:
                    logger.info(
                        "üü¢ User already exists. Prompting for update confirmation..."
                    )
                    confirm = input("üö´ Update face encoding? (y/n): ").strip().lower()
                    if confirm != "y":
                        logger.info("üî¥ Registration aborted by user.")
                        return False
                    else:
                        logger.info("‚úÖ User confirmed update of face encoding.")
                        user_id, existing_encoding_bytea = existing
                        existing_encodings = (
                            pickle.loads(existing_encoding_bytea)
                            if existing_encoding_bytea
                            else []
                        )
                        existing_encodings.append(encoding)
                        # Enforce maximum stored encodings.
                        existing_encodings = existing_encodings[
                            -MAX_ENCODINGS_PER_USER:
                        ]
                        cursor.execute(
                            "UPDATE users SET face_encoding = %s WHERE user_id = %s",
                            (
                                psycopg2.Binary(pickle.dumps(existing_encodings)),
                                user_id,
                            ),
                        )
                        logger.info(
                            "‚úÖ Face encoding updated for %s %s", first_name, last_name
                        )
                        # self._refresh_index()
                        # return
                        user_row_id = user_id

                else:
                    profile_image_path = str(FACE_CAPTURE_PATH / f"{liu_id}.jpg")
                    preferences = "{}"
                    interaction_memory = "[]"
                    face_blob = psycopg2.Binary(pickle.dumps([encoding]))
                    cursor.execute(
                        """INSERT INTO users
                           (first_name, last_name, liu_id, email, face_encoding,
                           preferences, profile_image_path, interaction_memory)
                           VALUES (%s, %s, %s, %s, %s, %s, %s, %s)""",
                        (
                            first_name,
                            last_name,
                            liu_id,
                            email,
                            face_blob,
                            preferences,
                            profile_image_path,
                            interaction_memory,
                        ),
                    )
                    cv2.imwrite(profile_image_path, frame)
                    user_row_id = cursor.lastrowid
                    logger.info(
                        "‚úÖ User %s %s registered successfully with LIU ID: %s",
                        first_name,
                        last_name,
                        liu_id,
                    )
            self._refresh_index()
        except psycopg2.Error as e:
            self.conn.rollback()
            logger.error("üî¥ Registration failed: %s", e)
            raise
        # On success:
        self._refresh_index()
        logger.info("‚úÖ User registered successfully.")
        return True

    def identify_user(self) -> None:
        """
        Capture several frames in multi-face mode and combine the results:
          - For each frame, perform identification on each detected face.
          - A dictionary tallies recognized users (using their LIU ID) across frames.
          - If any frame produces an unknown face (i.e. no match), an unknown flag is set.

        After processing:
          - Recognized users are welcomed.
          - If any unknown face is detected, the system explains that the capture from identification
            is not used for registration and calls register_user to re-capture the face.

        Inline Comment: The system uses multiple frames to improve reliability.
        """

        logger.info("üü¢ Starting face identification. Please look at the camera..")
        recognized_users = {}  # key: liu_id, value: count across frames
        unknown_found = False

        for i in range(IDENTIFICATION_FRAMES):
            result = self._capture_face_multi()
            if result is None:
                logger.error(
                    "üî¥ Frame %d: Failed to capture frame for identification.", i + 1
                )
                continue
            _, face_encodings, _ = result
            if not face_encodings:
                logger.debug("Frame %d: No faces detected.", i + 1)
                continue

            for encoding in face_encodings:
                query_encoding = np.array([encoding], dtype=np.float32)
                if self.faiss_index is not None:
                    distances, indices = self.faiss_index.search(query_encoding, k=1)
                    if distances[0][0] <= FACE_MATCH_THRESHOLD:
                        liu_id = self.user_ids[indices[0][0]]
                        recognized_users[liu_id] = recognized_users.get(liu_id, 0) + 1
                    else:
                        unknown_found = True
                else:
                    unknown_found = True
            time.sleep(TIMEDELAY)  # Slight delay between frames

        # Welcome recognized users (if recognized in any frame)
        if recognized_users:
            for liu_id in recognized_users:
                best_liu = max(recognized_users, key=recognized_users.get)
                user = self.known_encodings.get(best_liu)
                if user:
                    logger.info(
                        f"‚úÖ Welcome back, {user['first_name']} {user['last_name']}!"
                    )
                    return user
        else:
            logger.info("üî¥ No known faces detected.")

        # Prompt if any unknown face was found.
        if unknown_found:
            logger.info(
                "ü§ù A detected face is unknown, the system will re-capture it during registration."
            )
            response = (
                input("üö´ Would you like to register a user? (y/n): ").strip().lower()
            )
            if response == "y":
                self.register_user()

                # ‚úÖ Re-identify after registration and return the user
                user = self.identify_user()
                if user:
                    logger.info(
                        f"‚úÖ User Authenticated after registration. Welcome {user['first_name']} {user['last_name']}, (liu_id: {user['liu_id']})"
                    )
                    return user
            else:
                logger.info("‚ùå Registration declined. Authentication aborted.")
                return None
        return None

    def run(self) -> None:
        """
        Main application loop in auto-identification mode.
        The system continuously calls the identify function so that users are identified (and welcomed) immediately.
        After each identification round, the operator can re-run identification or quit.
        """
        logger.info(f"üü¢ Auto-identification mode enabled. Press Ctrl+C to exit.")
        try:
            while True:
                self.identify_user()
                choice = (
                    input("üü¢ Press Enter to re-run identification or 'q' to quit: ")
                    .strip()
                    .lower()
                )
                if choice == "q":
                    self.close()
                    logger.info("üü¢ Exiting...")
                    break
        except KeyboardInterrupt:
            self.close()
            logger.info("Exiting auto-identification mode.")

    def close(self):
        self.cursor.close()
        self.conn.close()


def main() -> None:
    """Entry point for the FaceAuthSystem application."""
    FACIAL_DATA_PATH.mkdir(parents=True, exist_ok=True)
    FACE_CAPTURE_PATH.mkdir(parents=True, exist_ok=True)
    auth_system = FaceAuthSystem()
    auth_system.run()


if __name__ == "__main__":
    main()


# try:
#                         # Call the public method from the separate VoiceAuth module.
#                         self.voice_auth.register_voice_for_user(
#                             first_name, last_name, liu_id
#                         )
#                         logger.info(
#                             "Voice registration completed for user %s %s",
#                             first_name,
#                             last_name,
#                         )
#                     except Exception as e:
#                         logger.error(
#                             "üî¥ Voice registration failed after face registration: %s",
#                             e,
#                         )
#                     logger.info(
#                         "User %s %s registered successfully", first_name, last_name
#                     )




# authentication/voice_auth.py
"""
This module provides a voice authentication system using PostgreSQL as the database backend.
It includes functionalities for user registration, voice embedding storage, and voice-based user verification.
Classes:
    VoiceAuth:
        A class for handling voice authentication, including:
        - Transcription using Google Speech Recognition.
        - Voice embedding generation using the Resemblyzer library.
        - Storing embeddings in a PostgreSQL database and as pickle files.
        - User registration and login via voice verification.
Functions:
    audio_session():
        A context manager for managing audio recording sessions.
Constants:
    MAX_RETRIES: int
        Maximum number of retries for transcription matching during registration.
    TEMP_AUDIO_PATH: str
        Path to store temporary audio files.
    TRANSCRIPTION_SENTENCE: str
        The sentence users must read for transcription and verification.
    VOICE_DATA_PATH: str
        Path to store voice embedding pickle files.
    VOICE_MATCH_THRESHOLD: float
        Threshold for cosine similarity to determine voice match.
"""


import logging
import os
import pickle
import re
import string
import warnings
from contextlib import contextmanager
from typing import List

import psycopg2
import sounddevice as sd
from resemblyzer import VoiceEncoder, preprocess_wav
from scipy.io.wavfile import write
from sklearn.metrics.pairwise import cosine_similarity
from speech_recognition import AudioFile, Recognizer, RequestError, UnknownValueError

from config.app_config import (
    MAX_RETRIES,
    TEMP_AUDIO_PATH,
    TRANSCRIPTION_SENTENCE,
    VOICE_DATA_PATH,
    VOICE_MATCH_THRESHOLD,
    setup_logging,
)
from mini_project.database.connection import get_connection

# Suppress warnings if desired
warnings.filterwarnings("ignore", category=FutureWarning)
logger = logging.getLogger("VoiceAutSystem")


# Context manager for handling an audio recording session.
@contextmanager
def audio_session():
    try:
        yield
    finally:
        sd.stop()


class VoiceAuth:
    """
    A class for voice authentication which handles:
    - Audio recording.
    - Transcription.
    - Voice embedding capture.
    - Storing embeddings in a SQLite database and on disk.
    """

    def __init__(
        self,
        temp_audio_path: str = TEMP_AUDIO_PATH,
        voice_data_path: str = VOICE_DATA_PATH,
    ) -> None:
        self.temp_audio_path = temp_audio_path
        self.voice_data_path = voice_data_path
        self.encoder = VoiceEncoder()
        self._create_directories()
        self.conn = get_connection()
        self.cursor = self.conn.cursor()

    def _create_directories(self) -> None:
        """Ensure that the directories for voice data and temporary audio exist."""
        os.makedirs(self.voice_data_path, exist_ok=True)
        os.makedirs(self.temp_audio_path, exist_ok=True)
        logger.info("üü¢ Directories ensured for voice data and temporary audio.")

    def _record_audio(
        self, filename: str, prompt: str, duration: int = 5, sampling_rate: int = 16000
    ) -> None:
        """
        Record audio from the microphone and save it to a WAV file.

        Args:
            filename (str): The file path to save the recorded audio.
            prompt (str): The message to display to the user before recording.
            duration (int): Duration of the recording in seconds.
            sampling_rate (int): Sampling rate for the audio recording.
        """
        logger.info(prompt)
        try:
            with audio_session():
                audio = sd.rec(
                    int(duration * sampling_rate),
                    samplerate=sampling_rate,
                    channels=1,
                    dtype="int16",
                )
                sd.wait()
            write(filename, sampling_rate, audio)
            logger.info(f"‚úÖ Audio recorded and saved to {filename}")
        except Exception as e:
            msg = f"üî¥ Error during audio recording: {e}"
            logger.error(msg)
            raise Exception(msg)

    def _transcribe_audio(self, filename: str) -> str:
        """
        Transcribe recorded audio to text using Google Speech Recognition.

        Args:
            filename (str): The file path of the audio file.

        Returns:
            str: The transcribed text.

        Raises:
            Exception: If transcription fails.
        """
        recognizer = Recognizer()
        try:
            with AudioFile(filename) as source:
                audio = recognizer.record(source)
                text = recognizer.recognize_google(audio)
                logger.info(f"üü¢ Transcription: {text}")
                return text
        except UnknownValueError:
            msg = "üî¥ Audio transcription failed: speech was unintelligible."
            logger.info(msg)
            raise Exception(msg)
        except RequestError as e:
            msg = f"üî¥ Audio transcription failed: API error: {e}"
            logger.info(msg)
            raise Exception(msg)
        except Exception as e:
            msg = f"üî¥ An unexpected error occurred during transcription: {e}"
            logger.info(msg)
            raise Exception(msg)

    def _capture_voice_embedding(self, audio_path: str) -> List[float]:
        """
        Capture a voice embedding from the recorded audio.

        Args:
            audio_path (str): Path to the audio file.

        Returns:
            List[float]: The voice embedding vector.
        """
        try:
            wav = preprocess_wav(audio_path)
            embedding = self.encoder.embed_utterance(wav)
            logger.info(f"‚úÖ Voice embedding captured, shape: {embedding.shape}")
            return embedding.tolist()
        except Exception as e:
            msg = f"üî¥ Error capturing voice embedding: {e}"
            logger.error(msg)
            raise Exception(msg)

    @staticmethod
    def _validate_liu_id(liu_id: str) -> bool:
        """
        Validate the LIU ID format.

        Args:
            liu_id (str): The LIU ID to validate.

        Returns:
            bool: True if the LIU ID is valid; False otherwise.
        """
        pattern = r"^[a-z]{5}[0-9]{3}$"
        return bool(re.match(pattern, liu_id))

    def _save_voice_embedding(
        self, liu_id: str, voice_embedding: List[float], first_name: str, last_name: str
    ) -> None:
        """
        Save the voice embedding in the database (using upsert) and as a pickle file.

        Args:
            liu_id (str): The LIU ID of the user.
            voice_embedding (List[float]): The voice embedding vector.
            first_name (str): The user's first name.
            last_name (str): The user's last name.
        """
        voice_file = os.path.join(self.voice_data_path, f"{liu_id}_voice.pkl")
        try:
            self.cursor.execute(
                """
                    INSERT INTO users (liu_id, voice_embedding, first_name, last_name)
                    VALUES (%s, %s, %s, %s)
                    ON CONFLICT(liu_id) DO UPDATE SET voice_embedding = EXCLUDED.voice_embedding
                """,
                (
                    liu_id,
                    psycopg2.Binary(pickle.dumps([voice_embedding])),
                    first_name,
                    last_name,
                ),
            )
            self.conn.commit()

            with open(voice_file, "wb") as file:
                pickle.dump(voice_embedding, file)

            logger.info(f"‚úÖ Voice embedding saved for LIU ID: {liu_id}")
        except psycopg2.Error as e:
            self.conn.rollback()
            logger.error(f"üî¥ Error saving voice embedding to database: {e}")
            raise
        except Exception as e:
            logger.error(f"üî¥ Error saving voice embedding to file: {e}")
            raise

    def register_user(self) -> None:
        """
        Register a new user using voice authentication.

        This method:
        - Collects user details.
        - Records a voice statement.
        - Transcribes the statement.
        - Captures the voice embedding.
        - Saves the embedding in the database and as a file.
        """
        logger.info("üü° Starting voice-driven user registration...")
        try:
            first_name = input("üü° Enter your first name: ").strip()
            if not first_name:
                raise Exception("First name cannot be empty.")

            last_name = input("üü° Enter your last name: ").strip()
            if not last_name:
                raise Exception("Last name cannot be empty.")

            liu_id = input("üü° Enter your LIU ID (e.g. abcxy123): ").strip()
            if not self._validate_liu_id(liu_id):
                raise Exception("Invalid LIU ID format.")

            # Record a voice statement.
            statement_audio = os.path.join(
                self.temp_audio_path, f"{liu_id}_statement.wav"
            )

            # Retry loop
            max_attempts = MAX_RETRIES
            for attempt in range(1, max_attempts + 1):
                self._record_audio(
                    statement_audio,
                    f"üü° Please read the following sentence clearly: '{TRANSCRIPTION_SENTENCE}'",
                    duration=12,
                )

                # Transcribe the audio.
                transcription = self._transcribe_audio(statement_audio)
                if not transcription:
                    logger.info("üî¥ Audio Transcription failed. Trying again...")
                    continue

                # Compare transcription
                if self._normalize_text(transcription) == self._normalize_text(
                    TRANSCRIPTION_SENTENCE
                ):
                    break
                else:
                    logger.info(
                        f"üî¥ Your transcription didn't match the expected sentence. Attempt {attempt}/{max_attempts}"
                    )
            else:
                raise Exception(
                    "üî¥ Maximum attempts reached. Registration failed due to mismatched transcription."
                )

            # Capture the voice embedding.
            embedding = self._capture_voice_embedding(statement_audio)
            if not embedding:
                raise Exception("Failed to capture voice embedding.")

            # Save the voice embedding, passing first_name and last_name.
            self._save_voice_embedding(liu_id, embedding, first_name, last_name)
            logger.info(
                f"‚úÖ Registration complete for {first_name} {last_name} (LIU ID: {liu_id})."
            )
        except Exception as e:
            logger.exception("üî¥ Registration failed.")
            logger.info(f"üî¥ Registration failed: {e}")

    def _normalize_text(self, text: str) -> str:
        """Lowercase, remove punctuation, and normalize whitespace."""
        # return "".join(
        #     char for char in text.lower() if char not in string.punctuation
        # ).strip()
        return "".join(
            text.lower().translate(str.maketrans("", "", string.punctuation)).split()
        )

    def register_voice_for_user(
        self, first_name: str, last_name: str, liu_id: str, duration: int = 8
    ) -> None:
        """
        Record a voice statement for an already-registered user.
        This method does not prompt for personal details, instead it uses the provided first_name,
        last_name, and liu_id. It records audio, transcribes (if desired), captures the voice embedding,
        and updates the user record with the voice embedding.
        """
        try:
            # Construct the file path for the voice statement.
            voice_statement_audio = os.path.join(
                self.temp_audio_path, f"{liu_id}_voice.wav"
            )

            # Record a voice statement.
            self._record_audio(
                voice_statement_audio,
                "Please speak a short voice statement for registration:",
                duration=duration,
                sampling_rate=16000,  # or use your configured sampling_rate
            )

            # (Optional) Transcribe the audio and log the transcription.
            transcription = self._transcribe_audio(voice_statement_audio)
            logger.info("üü¢ Voice transcription: %s", transcription)

            # Capture the voice embedding.
            embedding = self._capture_voice_embedding(voice_statement_audio)
            if not embedding:
                raise Exception("Failed to capture voice embedding.")

            # Save the voice embedding: update the database and save the pickle file.
            self._save_voice_embedding(liu_id, embedding, first_name, last_name)
            logger.info("‚úÖ Voice authentication register for: %s", liu_id)
        except Exception as e:
            logger.error("üî¥ Voice registration for user failed: %s", e)
            raise

    def verify_user_by_voice(self, liu_id: str, audio_path: str) -> bool:
        """
        Verifies whether the voice in the provided audio matches the registered user's embedding.


        Args:
            liu_id (str): The user's LIU ID.
            audio_path (str): Path to the newly recorded voice sample.

        Returns:
            bool: True if match passes the threshold; False otherwise.
        """
        try:
            # Load stored embedding
            stored_path = os.path.join(self.voice_data_path, f"{liu_id}_voice.pkl")
            if not os.path.exists(stored_path):
                raise FileNotFoundError(
                    f"üî¥ No stored voice data found for LIU ID: {liu_id}"
                )

            with open(stored_path, "rb") as f:
                stored_embedding = pickle.load(f)

            # Capture new embedding from input audio
            new_embedding = self._capture_voice_embedding(audio_path)

            # Compare using cosine similarity
            similarity = cosine_similarity([new_embedding], [stored_embedding])[0][0]
            logger.info(f"üü¢ Voice similarity score: {similarity:.4f}")
            return similarity >= VOICE_MATCH_THRESHOLD
        except Exception as e:
            logger.error(f"üî¥ Voice verification failed: {e}")
            return False

    def close(self):
        if self.cursor:
            self.cursor.close()

    def login_user(self) -> None:
        try:
            liu_id = input("Enter your LIU ID: ").strip()
            if not self._validate_liu_id(liu_id):
                raise Exception("Invalid LIU ID format.")

            # Record new audio for verification
            login_audio = os.path.join(self.temp_audio_path, f"{liu_id}_login.wav")
            self._record_audio(
                login_audio,
                f"üì£ Please read your verification sentence: {TRANSCRIPTION_SENTENCE}",
                duration=10,
            )

            if self.verify_user_by_voice(liu_id, login_audio):
                logger.info(f"‚úÖ Verification successful for {liu_id}. Welcome back!")
            else:
                logger.info(
                    f"‚ùå Verification failed. The voice does not match our records."
                )
        except Exception as e:
            logger.exception("üî¥ Login failed.")
            logger.error(f"üî¥ Login failed: {e}")


if __name__ == "__main__":
    auth = VoiceAuth(TEMP_AUDIO_PATH, VOICE_DATA_PATH)
    # auth.register_user()
    # auth.close()

    print("Choose Action:")
    print("1. Register User")
    print("2. Login via Voice")

    choice = input("Enter 1 or 2: ").strip()

    if choice == "1":
        auth.register_user()
    elif choice == "2":
        auth.login_user()
    else:
        print("Invalid choice.")

