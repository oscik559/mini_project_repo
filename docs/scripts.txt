# camera_vision_tray_and_holder_detection_pgSQL.py
"""
camera_vision_tray_and_holder_detection_pgSQL.py
This module provides functions for processing camera streams from an Intel D453i camera using pyrealsense2,
detecting trays, holders, and slides in the captured images, and updating a PostgreSQL database with the
detected object positions and orientations. The detection pipeline includes color-based object detection,
contour analysis, geometric calculations, and database upsert/cleanup operations.
Main Functions:
---------------
- process_image(image_path, wait_key, db_handler):
    Captures color and depth frames from the camera, processes each frame for object detection,
    computes positions and angles of trays and holders, matches detected slides to tray midpoints,
- color_image_process(image, wait_key, depth_frame, intrinsics):
    Processes a single color image to detect trays, holders, and slides using contour and color analysis.
    Calculates geometric properties and returns positions, angles, and matched slide information.
- calculate_midpoints_and_draw(image, coordinates, color=(0, 255, 0)):
    Calculates and draws midpoints between consecutive coordinates on the image.
- detect_colored_objects_in_roi(image, roi):
    Detects colored objects (green, orange, pink) within a specified region of interest (ROI) in the image.
- get_normalized_color(image, bounding_box):
    Computes the normalized average RGB color for a region of interest.
- match_objects_to_midpoints(detected_objects, midpoints):
    Matches each detected object center to the closest tray midpoint.
- calculate_distance(point1, point2):
    Calculates the Euclidean distance between two points.
- detect_screw_location(image):
    Detects orange and blue screws in the image based on color segmentation.
- calculate_relative_angle(screw_center_orange, screw_center_blue, dx1, dy1):
    Calculates the relative angle between two screw centers and a reference vector.
- depth_image_process(point0, ref_point, scaling_factor):
    Calculates the relative position of a point with respect to a reference point using a scaling factor.
- display_hsv_image(image, roi):
    Converts a region of the image to HSV for visualization (utility function).
- vector_changed(new_vector, old_vector, tolerance=5.0):
    Checks if a vector has changed significantly compared to a previous value.
Database Utility Functions:
--------------------------
- update_camera_vision_database(db_handler, tray_position, tray_orientation, holder_position, holder_orientation, slide_detections):
    Updates the camera_vision table with detection results for trays, holders, and slides.
- upsert_camera_vision_record(db_handler, ...):
    Inserts or updates a record in the camera_vision table, including color code and pose information.
- cleanup_camera_vision_records(db_handler, active_object_names):
Usage:
------
Run this module as a script to start processing camera images and updating the database.
 """
import math
import time

import cv2
import matplotlib.pyplot as plt
import numpy as np
import pyrealsense2 as rs
from config.app_config import *

from mini_project.database.db_handler import DatabaseHandler


def process_image(image_path, wait_key, db_handler):
    """
    Process the camera stream from the Intel D453i using pyrealsense2.
    Captures color and depth frames, processes each frame for object detection,
    and updates the database with the results.
    """
    # ================ Initialize RealSense pipeline =====================================
    pipeline = rs.pipeline()
    config = rs.config()
    config.enable_stream(rs.stream.color, 1280, 720, rs.format.bgr8, 30)
    config.enable_stream(rs.stream.depth, 1280, 720, rs.format.z16, 30)
    profile = pipeline.start(config)

    # Get the depth intrinsics for any depth-to-real-world conversions.
    depth_stream = profile.get_stream(rs.stream.depth).as_video_stream_profile()
    intrinsics = depth_stream.get_intrinsics()

    depth_sensor = profile.get_device().first_depth_sensor()
    depth_scale = depth_sensor.get_depth_scale()
    print(depth_scale)

    # Initialize the old vector to compare with new values
    old_tray_position = [1000, 1000, 1000]
    old_holder_position = [1000, 1000, 1000]

    old_tray_angle_with_x = None
    old_holder_angle_with_x = None

    old_matched_slides = []

    tray_rel_position = []
    holder_rel_position = []

    tray_angle_with_x = None
    holder_angle_with_x = None

    try:
        while True:
            # --- Capture frames ---
            frames = pipeline.wait_for_frames()
            color_frame = frames.get_color_frame()
            depth_frame = frames.get_depth_frame()
            if not color_frame or not depth_frame:
                continue

            color_image = np.asanyarray(color_frame.get_data())
            if color_image is None:
                print(f"Error: No image frames retrieved.")
                return

            # --- Process the color image ---
            (
                tray_rel_position,
                holder_rel_position,
                matched_slides,
                tray_angle_with_x,
                holder_angle_with_x,
            ) = color_image_process(color_image, wait_key, depth_frame, intrinsics)

            # Convert numpy.float64 to native floats
            tray_rel_position = [float(value) for value in tray_rel_position]
            holder_rel_position = [float(value) for value in holder_rel_position]

            # Process matched objects if they changed.
            if matched_slides != old_matched_slides:
                old_matched_slides = matched_slides

            # Prepare data for the database
            Slide_index = [
                (match["closest_midpoint_index"], match["object_color"])
                for match in matched_slides
            ]

            if (
                vector_changed(tray_rel_position, old_tray_position)
                or tray_angle_with_x != old_tray_angle_with_x
            ):
                old_tray_position = tray_rel_position
                old_tray_angle_with_x = tray_angle_with_x

            if (
                vector_changed(holder_rel_position, old_holder_position)
                or holder_angle_with_x != old_holder_angle_with_x
            ):
                old_holder_position = holder_rel_position
                old_holder_angle_with_x = holder_angle_with_x

            merged_data_1 = f"ðŸŸ¢ Fixture_and_slide_position: {tray_rel_position}; {tray_angle_with_x}; {Slide_index};"
            merged_data_2 = (
                f"ðŸŸ¢ Holder_position: {holder_rel_position}; {holder_angle_with_x};"
            )
            print(merged_data_1)
            print(merged_data_2)

            # Check if tray_rel_position is empty and decide what to do:
            if not tray_rel_position:

                print("Warning: Tray relative position is empty, using default values.")
                tray_rel_position = [50.0, 150.0, 10.0]
                tray_angle_with_x = 90.0

            if not holder_rel_position:

                print(
                    "Warning: Holder relative position is empty, using default values."
                )
                holder_rel_position = [170.0, 440.0, 0.0]
                holder_angle_with_x = 90.0

            # --- Update the database ---
            update_camera_vision_database(
                db_handler,
                tray_rel_position,
                tray_angle_with_x,
                holder_rel_position,
                holder_angle_with_x,
                matched_slides,
            )
            # Exit if the user presses 'q'
            if cv2.waitKey(wait_key) & 0xFF == ord("q"):
                break
    finally:
        cv2.destroyAllWindows()


def color_image_process(image, wait_key, depth_frame, intrinsics):
    """
    function for processing the input color image.
    Replace this with the actual object detection and processing logic.

    Args:
        color_image (np.ndarray): Input color image.

    Returns:
        tuple: (tray_relative_position, matched_objects, angle_with_x)
    """
    # Example output (replace with actual detection results)
    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    _, thresh_image = cv2.threshold(
        cv2.GaussianBlur(gray_image, (5, 5), 0), 100, 255, cv2.THRESH_BINARY_INV
    )

    # Detect contours
    contours, _ = cv2.findContours(
        thresh_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE
    )

    tray_rel_position = []
    holder_rel_position = []
    matched_slides = []
    tray_angle_with_x_axis = None
    holder_angle_with_x_axis = None
    closest_idx = None
    scaling_factor = None

    for contour in contours:
        if 50000 < cv2.contourArea(contour) < 170000:
            rect = cv2.minAreaRect(contour)
            box = cv2.boxPoints(rect).astype(np.int64)

            # Draw tray bounding box
            cv2.polylines(image, [box], isClosed=True, color=(0, 255, 0), thickness=2)

            # Detect red regions within the bounding box
            x, y, w, h = cv2.boundingRect(contour)
            hsv_roi = cv2.cvtColor(image[y : y + h, x : x + w], cv2.COLOR_BGR2HSV)
            red_mask = cv2.inRange(
                hsv_roi, np.array([0, 70, 50]), np.array([10, 255, 255])
            ) + cv2.inRange(hsv_roi, np.array([170, 70, 50]), np.array([180, 255, 255]))

            # Find red contours and identify the closest tray corner
            for red_contour in cv2.findContours(
                red_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE
            )[0]:
                if cv2.contourArea(red_contour) > 500:
                    red_center = np.mean(
                        cv2.boxPoints(cv2.minAreaRect(red_contour)), axis=0
                    ) + [
                        x,
                        y,
                    ]
                    closest_idx = np.argmin(
                        [np.linalg.norm(corner - red_center) for corner in box]
                    )

                    # Draw bounding box for the red contour
                    box_red = cv2.boxPoints(cv2.minAreaRect(red_contour)).astype(
                        np.int64
                    )
                    pts_red = np.array(
                        [
                            (box_red[0]) + (x, y),
                            (box_red[1]) + (x, y),
                            (box_red[2]) + (x, y),
                            (box_red[3]) + (x, y),
                        ]
                    ).astype(np.int64)
                    image = cv2.polylines(
                        image, [pts_red], isClosed=True, color=(0, 0, 255), thickness=2
                    )

                    # Determine tray points
                    tray_points = [box[closest_idx]]
                    next_idx = (closest_idx + 1) % 4
                    prev_idx = (closest_idx - 1) % 4
                    longer_corner_idx = (
                        next_idx
                        if np.linalg.norm(box[closest_idx] - box[next_idx])
                        > np.linalg.norm(box[closest_idx] - box[prev_idx])
                        else prev_idx
                    )

                    # Add 'POINT 1' and the remaining unmarked corners
                    tray_points.append(box[longer_corner_idx])
                    # shorter_corner_idx = (set(range(4)) - {closest_idx, longer_corner_idx}).pop()
                    remaining_indices = set(range(4)) - {closest_idx, longer_corner_idx}
                    shorter_corner_idx = min(
                        remaining_indices,
                        key=lambda idx: np.linalg.norm(box[closest_idx] - box[idx]),
                    )
                    tray_points.append(box[shorter_corner_idx])

                    # Determine the final unmarked corner as 'point3'
                    point3_idx = (
                        set(range(4))
                        - {closest_idx, longer_corner_idx, shorter_corner_idx}
                    ).pop()
                    tray_points.append(box[point3_idx])

                    # Draw tray points and labels
                    for i, pt in enumerate(tray_points):
                        cv2.circle(image, tuple(pt), 1, (0, 0, 255), -1)
                        cv2.putText(
                            image,
                            f"point{i}",
                            tuple(pt),
                            cv2.FONT_HERSHEY_SIMPLEX,
                            0.4,
                            (255, 0, 0),
                            2,
                        )

                    # Find the angle between 'POINT 0' and 'POINT 1' with respect to the x-axis
                    point0 = tray_points[0]
                    point1 = tray_points[1]
                    dx, dy = point1[0] - point0[0], point1[1] - point0[1]
                    tray_angle_with_x_axis = math.degrees(math.atan2(dy, dx))

                    # Draw the angle value next to 'POINT 0'
                    angle_text = f"Angle: {tray_angle_with_x_axis:.2f}Â°"
                    cv2.putText(
                        image,
                        angle_text,
                        (point0[0] + 50, point0[1]),
                        cv2.FONT_HERSHEY_SIMPLEX,
                        0.4,
                        (255, 0, 0),
                        2,
                    )

                    # Segment the ROI between POINT 0 and POINT 1
                    num_segments = 10
                    coordinates_top = []
                    coordinates_top.append((int(point0[0]), int(point0[1])))
                    for i in range(1, num_segments):
                        # Calculate the segment position
                        segment_ratio = i / num_segments
                        segment_x = int(
                            point0[0] + segment_ratio * (point1[0] - point0[0])
                        )
                        segment_y = int(
                            point0[1] + segment_ratio * (point1[1] - point0[1])
                        )

                        # Calculate the corresponding points on the opposite edge (point2 to point3)
                        point2 = tray_points[2]
                        point3 = tray_points[3]
                        line_start = (segment_x, segment_y)
                        line_end = (
                            int(point2[0] + segment_ratio * (point3[0] - point2[0])),
                            int(point2[1] + segment_ratio * (point3[1] - point2[1])),
                        )

                        # Draw vertical segmenting green line
                        cv2.line(image, line_start, line_end, (0, 255, 0), 1)
                        coordinates_top.append((segment_x, segment_y))

                    coordinates_top.append((int(point1[0]), int(point1[1])))
                    coordinates_bottom = []
                    coordinates_bottom.append((int(point2[0]), int(point2[1])))

                    # Calculate and append the coordinates for the divisions between POINT 2 and POINT 3
                    num_segments_edge2_to_3 = (
                        10  # Number of divisions you want along this edge
                    )
                    for i in range(1, num_segments_edge2_to_3 + 1):
                        # Calculate the segment position
                        segment_ratio = i / num_segments_edge2_to_3
                        segment_x = int(
                            point2[0] + segment_ratio * (point3[0] - point2[0])
                        )
                        segment_y = int(
                            point2[1] + segment_ratio * (point3[1] - point2[1])
                        )

                        # Append the coordinates as a tuple of integers
                        coordinates_bottom.append((segment_x, segment_y))

                    # Append point3 to the list as a tuple of integers
                    coordinates_bottom.append((int(point3[0]), int(point3[1])))

                    # Segment the ROI between POINT 0 and POINT 2 with two divisions
                    num_divisions = 2
                    for i in range(1, num_divisions):
                        # Calculate the segment position
                        division_ratio = i / (
                            num_divisions
                        )  # +1 to adjust for two segments
                        segment_x = int(
                            point0[0] + division_ratio * (point2[0] - point0[0])
                        )
                        segment_y = int(
                            point0[1] + division_ratio * (point2[1] - point0[1])
                        )

                        # Calculate the corresponding points on the opposite edge (point1 to point3)
                        line_start = (segment_x, segment_y)
                        line_end = (
                            int(point1[0] + division_ratio * (point3[0] - point1[0])),
                            int(point1[1] + division_ratio * (point3[1] - point1[1])),
                        )

                        # Draw horizontal segmenting green line
                        cv2.line(image, line_start, line_end, (0, 255, 0), 1)

                    # Draw midpoints for coordinates_top and coordinates_bottom
                    merged_midpoints = calculate_midpoints_and_draw(
                        image, coordinates_top
                    ) + calculate_midpoints_and_draw(image, coordinates_bottom)

                    # detect colored objects in ROI
                    roi = (x, y, w, h)
                    detected_objects = detect_colored_objects_in_roi(image, roi)

                    # Match objects to midpoints
                    matched_slides = match_objects_to_midpoints(
                        detected_objects, merged_midpoints
                    )

                    # for match in matched_objects:
                    #     print(f"Object color: {match['object_color']}")
                    #     print(f"Closest midpoint index: {match['closest_midpoint_index']}")

                    # Detect screw location
                    detected_screw = detect_screw_location(image)

                    screw_center_orange = []
                    screw_center_blue = []

                    for color, objects in detected_screw.items():
                        if color == "Screw_orange" and objects:
                            screw_center_orange = objects[0][
                                "center"
                            ]  # Assuming we take the first detected center
                        elif color == "Screw_blue" and objects:
                            screw_center_blue = objects[0]["center"]

                    # ======================= DEFINE ACTUAL DISTANCE BETWEEN REFERENCES =======================
                    Actual_distance_p0_p1 = 320  # in mm

                    # ======================= CALCULATE SCALING FACTOR =======================
                    pixel_distance_p0_p1 = math.sqrt(
                        (int(point1[0]) - int(point0[0])) ** 2
                        + (int(point1[1]) - int(point0[1])) ** 2
                    )
                    scaling_factor = Actual_distance_p0_p1 / pixel_distance_p0_p1

                    # ============== Calculate the relative angle between the screw centers =================
                    relative_angle_btw_screw_centers = calculate_relative_angle(
                        screw_center_orange, screw_center_blue, dx, dy
                    )

                    # ============== Calculate the relative position of the tray w.r.t. the detected screws =================
                    if (
                        screw_center_orange is not None
                        and len(screw_center_orange) != 0
                    ):
                        tray_rel_position = depth_image_process(
                            point0,
                            screw_center_orange,
                            scaling_factor,
                        )
                    display_hsv_image(image, (x, y, w, h))

        # For the slide holder
        if 5000 < cv2.contourArea(contour) < 17000:
            rect_ = cv2.minAreaRect(contour)
            box_ = cv2.boxPoints(rect_).astype(np.int64)

            # Draw the tray bounding box in blue
            cv2.polylines(image, [box_], isClosed=True, color=(255, 0, 0), thickness=2)

            # Detect red regions within the bounding box
            x_, y_, w_, h_ = cv2.boundingRect(contour)
            hsv_roi_ = cv2.cvtColor(
                image[y_ : y_ + h_, x_ : x_ + w_], cv2.COLOR_BGR2HSV
            )
            red_mask_ = cv2.inRange(
                hsv_roi_, np.array([0, 70, 50]), np.array([10, 255, 255])
            ) + cv2.inRange(
                hsv_roi_, np.array([170, 70, 50]), np.array([180, 255, 255])
            )

            # Find red contours and identify the closest tray corner
            for red_contour_ in cv2.findContours(
                red_mask_, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE
            )[0]:
                if cv2.contourArea(red_contour_) > 50:
                    red_center_ = np.mean(
                        cv2.boxPoints(cv2.minAreaRect(red_contour_)), axis=0
                    ) + [
                        x_,
                        y_,
                    ]
                    closest_idx_ = np.argmin(
                        [np.linalg.norm(corner_ - red_center_) for corner_ in box_]
                    )

                    # Draw bounding box for the red contour
                    box_red_ = cv2.boxPoints(cv2.minAreaRect(red_contour_)).astype(
                        np.int64
                    )
                    pts_red_ = np.array(
                        [
                            (box_red_[0]) + (x_, y_),
                            (box_red_[1]) + (x_, y_),
                            (box_red_[2]) + (x_, y_),
                            (box_red_[3]) + (x_, y_),
                        ]
                    ).astype(np.int64)
                    image = cv2.polylines(
                        image, [pts_red_], isClosed=True, color=(0, 0, 255), thickness=2
                    )

                    # Determine tray points
                    tray_points_ = [box_[closest_idx_]]
                    next_idx_ = (closest_idx_ + 1) % 4
                    prev_idx_ = (closest_idx_ - 1) % 4
                    longer_corner_idx_ = (
                        next_idx_
                        if np.linalg.norm(box_[closest_idx_] - box_[next_idx_])
                        > np.linalg.norm(box_[closest_idx_] - box_[prev_idx_])
                        else prev_idx_
                    )

                    # Add 'POINT 1' and the remaining unmarked corners
                    tray_points_.append(box_[longer_corner_idx_])
                    # shorter_corner_idx = (set(range(4)) - {closest_idx, longer_corner_idx}).pop()
                    remaining_indices_ = set(range(4)) - {
                        closest_idx_,
                        longer_corner_idx_,
                    }
                    shorter_corner_idx_ = min(
                        remaining_indices_,
                        key=lambda idx_: np.linalg.norm(
                            box_[closest_idx_] - box_[idx_]
                        ),
                    )
                    tray_points_.append(box_[shorter_corner_idx_])

                    # Determine the final unmarked corner as 'point3'
                    point3_idx_ = (
                        set(range(4))
                        - {closest_idx_, longer_corner_idx_, shorter_corner_idx_}
                    ).pop()
                    tray_points_.append(box_[point3_idx_])

                    for i, pt in enumerate(tray_points_):
                        cv2.circle(image, tuple(pt), 3, (255, 0, 0), -1)
                        cv2.putText(
                            image,
                            f"p{i}",
                            tuple(pt),
                            cv2.FONT_HERSHEY_SIMPLEX,
                            0.4,
                            (0, 255, 0),
                            2,
                        )

                    # Find the angle between 'POINT 0' and 'POINT 1' with respect to the x-axis
                    point0_ = tray_points_[0]
                    point1_ = tray_points_[1]
                    dx_, dy_ = point1_[0] - point0_[0], point1_[1] - point0_[1]
                    holder_angle_with_x_axis = math.degrees(math.atan2(dy_, dx_))

                    # ============== Draw the angle value next to 'POINT 0' ==============
                    angle_text_ = f"Angle: {holder_angle_with_x_axis:.2f} deg"
                    cv2.putText(
                        image,
                        angle_text_,
                        (point0_[0] + 50, point0_[1]),
                        cv2.FONT_HERSHEY_SIMPLEX,
                        0.4,
                        (0, 255, 0),
                        2,
                    )

                    # ============== Calculate relative position of holder w.r.t. detected screw ==============
                    detected_screw = detect_screw_location(image)
                    screw_center_orange = (
                        detected_screw.get("Screw_orange", [])[0]["center"]
                        if detected_screw.get("Screw_orange")
                        else []
                    )

                    # ======================= CALCULATE SCALING FACTOR =======================
                    Actual_distance_p0_p1_ = 250  # in mm
                    pixel_distance_p0_p1_ = math.sqrt(
                        (int(point1_[0]) - int(point0_[0])) ** 2
                        + (int(point1_[1]) - int(point0_[1])) ** 2
                    )
                    scaling_factor_ = Actual_distance_p0_p1_ / pixel_distance_p0_p1_

                    if len(screw_center_orange) != 0 and scaling_factor_ is not None:
                        holder_rel_position = depth_image_process(
                            point0_, screw_center_orange, scaling_factor_
                        )

    # scale_percent = 90
    # width = int(image.shape[1] * scale_percent / 100)
    # height = int(image.shape[0] * scale_percent / 100)
    # dim = (width, height)  # Resize the image
    # resized_image = cv2.resize(image, dim, interpolation=cv2.INTER_AREA)

    # Show the resized image
    # cv2.imshow("tray and holder detection", resized_image)
    cv2.imshow("tray and holder detection", image)
    cv2.waitKey(wait_key)

    ## Draw contours on the original image
    # output_image = image.copy()
    # cv2.drawContours(output_image, contours, -1, (0, 255, 0), 2)
    ## Show the image
    # cv2.imshow("Contours", output_image)

    # cv2.waitKey(0)
    # cv2.destroyAllWindows()

    return (
        tray_rel_position,
        holder_rel_position,
        matched_slides,
        tray_angle_with_x_axis,
        holder_angle_with_x_axis,
    )


def calculate_midpoints_and_draw(image, coordinates, color=(0, 255, 0)):
    midpoints = []
    # Iterate through the coordinates list to calculate midpoints
    for i in range(len(coordinates) - 1):
        # Get the current coordinate and the next coordinate
        point1, point2 = coordinates[i], coordinates[i + 1]

        # Calculate the midpoint
        midpoint = ((point1[0] + point2[0]) // 2, (point1[1] + point2[1]) // 2)
        midpoints.append(midpoint)

        # Draw a circle at the midpoint
        cv2.circle(image, midpoint, 5, color, -1)

    return midpoints


def detect_colored_objects_in_roi(image, roi):
    # Define color ranges in HSV
    color_ranges = {
        "green": (np.array([30, 45, 80]), np.array([70, 160, 190])),  # Green range
        "orange": (np.array([10, 90, 100]), np.array([25, 255, 255])),  # Orange range
        "pink": (
            np.array([150, 120, 120]),
            np.array([175, 195, 165]),
        ),
    }
    detected_objects = {}

    # Crop the image to the ROI
    x, y, w, h = roi
    roi_image = image[y : y + h, x : x + w]

    # Convert the ROI to HSV color space
    hsv_roi = cv2.cvtColor(roi_image, cv2.COLOR_BGR2HSV)

    # Loop through each color range and create masks
    for color, (lower, upper) in color_ranges.items():
        # Create a mask for the current color
        mask = cv2.inRange(hsv_roi, lower, upper)

        # Find contours of the detected objects
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        detected_objects[color] = []

        for contour in contours:
            if 500 > cv2.contourArea(contour) > 200:  # Filter by area
                # Get the bounding box and centroid
                x_contour, y_contour, w_contour, h_contour = cv2.boundingRect(contour)
                # center = (x_contour + w_contour // 2, y_contour + h_contour // 2)

                # Adjust the center coordinates back to the full image
                adjusted_center_x = x_contour + w_contour // 2 + x  # add x from ROI
                adjusted_center_y = y_contour + h_contour // 2 + y  # add y from ROI
                center = (adjusted_center_x, adjusted_center_y)

                color_code = get_normalized_color(
                    roi_image, (x_contour, y_contour, w_contour, h_contour)
                )

                # Append detected object details
                detected_objects[color].append(
                    {
                        "contour": contour,
                        "bounding_box": (x_contour, y_contour, w_contour, h_contour),
                        "center": center,
                        "color_code": color_code,
                    }
                )
                cv2.rectangle(
                    roi_image,
                    (x_contour, y_contour),
                    (x_contour + w_contour, y_contour + h_contour),
                    (255, 0, 0),
                    2,
                )
                # cv2.circle(roi_image, center, 5, (0, 0, 255), -1)  # Red center
    return detected_objects


def get_normalized_color(image, bounding_box):
    """
    Computes the normalized average color (in RGB) for a region of interest.

    Args:
        image (np.ndarray): The source image (BGR format).
        bounding_box (tuple): A tuple (x, y, w, h) defining the ROI.

    Returns:
        str: A string representation of the normalized color, e.g. "[0.0, 0.0, 0.7]".
    """
    x, y, w, h = bounding_box
    roi = image[y : y + h, x : x + w]
    # Compute the average color over the ROI. The result is in BGR order.
    avg_color_bgr = np.average(np.average(roi, axis=0), axis=0)
    # Convert BGR to RGB
    avg_color_rgb = [avg_color_bgr[2], avg_color_bgr[1], avg_color_bgr[0]]
    # Normalize each channel and round the result to 2 decimal places.
    normalized = [round(float(c) / 255.0, 2) for c in avg_color_rgb]
    return normalized  # Return as a list of Python floats


def match_objects_to_midpoints(detected_objects, midpoints):
    """Match each detected object center to the closest midpoint."""
    object_midpoint_map = []

    # Iterate over detected objects by color
    for color, objects in detected_objects.items():
        for obj in objects:
            obj_center = obj["center"]
            closest_midpoint_idx = None
            min_distance = float("inf")

            # Iterate through the midpoints to find the closest one
            for i, midpoint in enumerate(midpoints):
                distance = calculate_distance(obj_center, midpoint)
                if distance < min_distance:
                    min_distance = distance
                    closest_midpoint_idx = i

            # Append the result as a dictionary with object details and matched midpoint index
            #     'object_center': obj_center,
            #     'closest_midpoint': midpoints[closest_midpoint_idx] if closest_midpoint_idx is not None else None
            object_midpoint_map.append(
                {
                    "object_color": color,
                    "color_code": obj.get("color_code", "[0.0, 0.0, 0.0]"),
                    "closest_midpoint_index": closest_midpoint_idx,
                }
            )

    return object_midpoint_map


def calculate_distance(point1, point2):
    """Calculate Euclidean distance between two points."""
    return np.sqrt((point1[0] - point2[0]) ** 2 + (point1[1] - point2[1]) ** 2)


def detect_screw_location(image):
    # Define color ranges in HSV
    color_ranges = {
        "Screw_orange": (
            # np.array([10, 45, 140]),
            # np.array([25, 160, 200]),
            np.array([5, 50, 150]),
            np.array([35, 200, 200]),
        ),  # Orange range for screw
        "Screw_blue": (
            # np.array([100, 110, 60]),
            # np.array([120, 185, 120]),
            np.array([90, 100, 50]),
            np.array([120, 220, 110]),
        ),  # Blue range for screw
    }
    detected_screw = {}

    height, width = image.shape[:2]
    roi = 0, 0, width, height
    x, y, w, h = 0, 0, width, height
    # Convert the ROI to HSV color space
    hsv_roi = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)

    # Loop through each color range and create masks
    for color, (lower, upper) in color_ranges.items():
        # Create a mask for the current color
        mask = cv2.inRange(hsv_roi, lower, upper)
        # Find contours of the detected objects
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        detected_screw[color] = []

        for contour in contours:
            # =============== AMEND AREA TO SUIT PURPOSE ===============
            if 50 < cv2.contourArea(contour) < 150:  # Filter by area
                # Get the bounding box and centroid
                x_contour, y_contour, w_contour, h_contour = cv2.boundingRect(contour)

                # Adjust the center coordinates back to the full image
                adjusted_center_x = x_contour + w_contour // 2 + x  # add x from ROI
                adjusted_center_y = y_contour + h_contour // 2 + y  # add y from ROI
                center = (adjusted_center_x, adjusted_center_y)

                # Append detected object details
                detected_screw[color].append(
                    {
                        "contour": contour,
                        "bounding_box": (x_contour, y_contour, w_contour, h_contour),
                        "center": center,
                    }
                )
                cv2.rectangle(
                    image,
                    (x_contour, y_contour),
                    (x_contour + w_contour, y_contour + h_contour),
                    (255, 0, 0),
                    2,
                )
    return detected_screw


def calculate_relative_angle(screw_center_orange, screw_center_blue, dx1, dy1):
    angle_between_lines = None

    if screw_center_orange and screw_center_blue:
        # Calculate the direction vector for the line connecting the screw centers
        dx2, dy2 = (
            screw_center_blue[0] - screw_center_orange[0],
            screw_center_blue[1] - screw_center_orange[1],
        )

        # Calculate the angles with respect to the x-axis using atan2
        angle1 = math.atan2(dy1, dx1)
        angle2 = math.atan2(dy2, dx2)

        # Calculate the difference between the angles
        angle_between_lines = math.degrees(abs(angle1 - angle2))

        # Normalize the angle to the range [0, 180]
        if angle_between_lines > 180:
            angle_between_lines = 360 - angle_between_lines

        # print("Angle between the lines:", angle_between_lines)
    else:
        print("Error: Could not find centers for 'Screw_orange'.")

    return angle_between_lines


def depth_image_process(point0, ref_point, scaling_factor):
    point0_relative_position = []
    # depth_stream = profile.get_stream(rs.stream.depth).as_video_stream_profile()
    # intrinsics = depth_stream.get_intrinsics()
    # intrinsics = rs.intrinsics()

    # Retrieve depth values at these points in meters
    # d1 = depth_frame.get_distance(point0[0], point0[1])
    # d2 = depth_frame.get_distance(point_ref[0], point_ref[1])
    # d_center = depth_frame.get_distance(int(intrinsics.ppx), int(intrinsics.ppy))

    # d1 = 1.02
    # d2 = 1.04

    pixel_distance_p0_screw = (
        math.sqrt((ref_point[0] - point0[0]) ** 2 + (ref_point[1] - point0[1]) ** 2)
    ) * scaling_factor

    relative__pixel_vector = [
        point0[0] - ref_point[0],  # X component
        point0[1] - ref_point[1],  # Y component
    ]

    point0_relative_position = [
        relative__pixel_vector[0] * scaling_factor,
        relative__pixel_vector[1] * scaling_factor,
        0.0,
    ]

    # if d1 == 0 or d2 == 0:
    #     print("Invalid depth values at one or both points.")
    # else:
    #     point_3d = rs.rs2_deproject_pixel_to_point(intrinsics, [int(point0[0]), int(point0[1])], d1)
    #     point_3d_ref = rs.rs2_deproject_pixel_to_point(intrinsics, [int(point_ref[0]), int(point_ref[1])], d2)

    #     distance = math.sqrt((point_3d_ref[0] - point_3d[0])**2 + (point_3d_ref[1] - point_3d[1])**2 + (point_3d_ref[2] - point_3d[2])**2)
    #     print("Distance between points:", distance, "meters")

    #     # Compute the position vector relative to the reference object
    #     point0_relative_position = [point_3d[0] - point_3d_ref[0],  # X component
    #                         point_3d[1] - point_3d_ref[1],  # Y component
    #                         point_3d[2] - point_3d_ref[2]]   # Z component

    return point0_relative_position


def display_hsv_image(image, roi):
    # Crop the image to the ROI
    x, y, w, h = roi
    # roi_image = image[y:y+h, x:x+w]

    # Convert the cropped ROI to HSV
    hsv_roi = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)

    # Split the channels for visualization
    h, s, v = cv2.split(hsv_roi)


def vector_changed(new_vector, old_vector, tolerance=5.0):
    # A function to check if the relative vector has changed significantly

    return any(abs(n - o) > tolerance for n, o in zip(new_vector, old_vector))


# camera_db_utils.py
# ðŸ›‘ with postgres


def update_camera_vision_database(
    db_handler,
    tray_position,
    tray_orientation,
    holder_position,
    holder_orientation,
    slide_detections,
):
    """
    Update the camera_vision table with detection results.
    """
    active_names = []  # to keep track of all object names that are active
    DEFAULT_BLACK_CODE = [0.0, 0.0, 0.0]
    # --- Update the Tray record ---
    tray_data = {
        "object_name": "Fixture",
        "object_color": "black",  # default color name
        "color_code": DEFAULT_BLACK_CODE,  # black in normalized RGB
        "pos_x": tray_position[0],
        "pos_y": tray_position[1],
        "pos_z": tray_position[2],
        "rot_x": 180.0,
        "rot_y": 0.0,
        "rot_z": 90 - tray_orientation,
        "usd_name": "Fixture.usd",
    }
    upsert_camera_vision_record(db_handler, **tray_data)
    active_names.append(tray_data["object_name"])

    # --- Update the Holder record ---
    holder_data = {
        "object_name": "Holder",
        "object_color": "black",
        "color_code": DEFAULT_BLACK_CODE,  # default color for holder
        "pos_x": holder_position[0],
        "pos_y": holder_position[1],
        "pos_z": holder_position[2] + 8.6,
        "rot_x": 180.0,
        "rot_y": 0.0,
        "rot_z": 90 - holder_orientation,
        "usd_name": "Slide_Holder.usd",
    }
    upsert_camera_vision_record(db_handler, **holder_data)
    active_names.append(holder_data["object_name"])

    # --- Update the Slide records ---
    for index, detection in enumerate(slide_detections, start=1):
        slide_name = f"Slide_{index}"
        slide_color = detection.get("object_color", "Black")
        closest_midpoint_index = detection.get("closest_midpoint_index")
        # Calculate slide position relative to the tray (example offsets)
        # slide_pos_x = tray_position[0] + 10 * index
        # slide_pos_y = tray_position[1] + 5 * index
        # slide_pos_z = tray_position[2] - 19

        # print(slide_detections)
        # print(closest_midpoint_index)
        if closest_midpoint_index < 10:
            slide_pos_x = (22.75 + (closest_midpoint_index * 30.5)) * 10
            slide_pos_y = 460
            slide_pos_z = tray_position[2] - 19
        else:
            slide_pos_x = (22.75 + ((closest_midpoint_index - 10) * 30.5)) * 10
            slide_pos_y = 1750
            slide_pos_z = tray_position[2] - 19

        # Use the color code computed during detection; if not available, default to black.
        slide_color_code = detection.get("color_code", DEFAULT_BLACK_CODE)
        slide_data = {
            "object_name": slide_name,
            "object_color": slide_color,
            "color_code": slide_color_code,
            "pos_x": slide_pos_y,
            "pos_y": slide_pos_x,
            "pos_z": slide_pos_z,
            "rot_x": 0.0,
            "rot_y": 0.0,
            "rot_z": 90,
            "usd_name": "Slide.usd",
        }
        upsert_camera_vision_record(db_handler, **slide_data)
        active_names.append(slide_name)

    # --- Clean up any records not currently detected ---
    cleanup_camera_vision_records(db_handler, active_names)


def upsert_camera_vision_record(
    db_handler,
    object_name,
    object_color,
    color_code,
    pos_x,
    pos_y,
    pos_z,
    rot_x,
    rot_y,
    rot_z,
    usd_name,
):
    """
    Inserts a new row or updates the existing row (identified by object_name)
    in the camera_vision table, including the color_code field.
    """
    try:
        # Check if a record already exists for the given object_name
        query = "SELECT object_id FROM camera_vision WHERE object_name = %s"
        db_handler.cursor.execute(query, (object_name,))
        result = db_handler.cursor.fetchone()

        if result is None:
            # Insert a new row if no record exists
            insert_query = """
                INSERT INTO camera_vision
                (object_name, object_color, color_code, pos_x, pos_y, pos_z, rot_x, rot_y, rot_z, usd_name, last_detected)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW())
            """
            db_handler.cursor.execute(
                insert_query,
                (
                    object_name,
                    object_color,
                    color_code,
                    pos_x,
                    pos_y,
                    pos_z,
                    rot_x,
                    rot_y,
                    rot_z,
                    usd_name,
                ),
            )
        else:
            # Update the existing row
            update_query = """
                UPDATE camera_vision
                SET object_color = %s,
                    color_code = %s,
                    pos_x = %s,
                    pos_y = %s,
                    pos_z = %s,
                    rot_x = %s,
                    rot_y = %s,
                    rot_z = %s,
                    usd_name = %s,
                    last_detected = NOW()
                WHERE object_name = %s
            """
            db_handler.cursor.execute(
                update_query,
                (
                    object_color,
                    color_code,
                    pos_x,
                    pos_y,
                    pos_z,
                    rot_x,
                    rot_y,
                    rot_z,
                    usd_name,
                    object_name,
                ),
            )

        db_handler.conn.commit()

    except Exception as e:
        print(f"Database error in upsert_camera_vision_record: {e}")
        db_handler.conn.rollback()


def cleanup_camera_vision_records(db_handler, active_object_names):
    """
    Deletes any rows from camera_vision whose object_name is not in the active list.
    """
    try:
        if not active_object_names:
            # Avoid executing an invalid SQL statement
            print("Warning: No active object names provided. Skipping cleanup.")
            return

        # Create a safe query with placeholders for each item
        placeholders = ",".join(["%s"] * len(active_object_names))
        delete_query = (
            f"DELETE FROM camera_vision WHERE object_name NOT IN ({placeholders})"
        )

        db_handler.cursor.execute(delete_query, active_object_names)
        db_handler.conn.commit()

    except Exception as e:
        print(f"Database error in cleanup_camera_vision_records: {e}")
        db_handler.conn.rollback()


if __name__ == "__main__":
    db = DatabaseHandler()
    image_path = CAMERA_DATA_PATH / "image_1.png"

    wait_key = 1000
    process_image(image_path, wait_key, db_handler=db)
####################
# camera_vision_shapes_detection_pgSQL.py
"""
camera_vision_shapes_detection_pgSQL.py
This module provides functionality for detecting colored geometric shapes on a table using a RealSense camera,
processing their positions and colors, and storing/updating the results in a PostgreSQL database. It also detects
reference screws (orange and blue) to compute real-world coordinates from image pixels.
Main Features:
- Real-time video capture and processing using Intel RealSense and OpenCV.
- Detection of colored shapes (circle, square, rectangle, triangle, pentagon, hexagon) and their colors.
- Detection of reference screws for spatial calibration.
- Calculation of real-world (mm) positions of detected objects relative to reference screws.
- Database upsert (insert/update) of detected objects with position, color, and orientation.
- Visualization of detection results with annotated overlays.
- Automatic clearing of old database records for each new frame/session.
Key Functions:
- map_shape_to_object(shape_name): Maps detected shape names to object types.
- normalize_rgb_color(bgr): Normalizes BGR color to RGB [0,1] range.
- mean_shift_segmentation(image, sp, sr): Applies mean shift segmentation for color smoothing.
- get_color_name_from_ranges(hsv_value, color_ranges): Determines color name from HSV value and predefined ranges.
- detect_screw_positions(screw_roi, depth_frame, roi_origin, image): Detects orange and blue screws in the ROI.
- find_table_roi(image): Finds and extracts the table region of interest from the image.
- upsert_camera_vision_record(conn, ...): Inserts or updates a detected object record in the database.
- detect_shape(contour): Classifies a contour as a geometric shape.
- detect_colored_shapes(...): Detects colored shapes in the ROI, computes positions, and updates the database.
- clear_old_camera_vision_records(conn, expiry_seconds): Clears all records from the camera_vision table.
- visualize_shapes(image, roi_origin, annotated_roi): Overlays annotated ROI onto the original image.
- camera_pipeline(): Main loop for camera capture, detection, visualization, and database update.
Globals and Constants:
- MIN_SHAPE_AREA, MAX_SHAPE_AREA: Area thresholds for filtering shapes.
- SCREW_TO_SCREW: Real-world distance between reference screws (mm).
- SCREW_TIMEOUT_SEC: Timeout for using last-known screw positions.
- POSITION_TOLERANCE_MM: Tolerance for matching detected objects across frames.
- color_ranges: HSV color ranges for detection.
- shape_counter, last_known_screw_positions, known_objects, shape_history: State for object tracking and smoothing.
Usage:
Run this script as the main module to start the camera pipeline and begin real-time shape detection and database updates.
Dependencies:
- OpenCV (cv2)
- numpy
- matplotlib
- pyrealsense2
- psycopg2 (for database connection, via mini_project.database.connection.get_connection)
 """
import logging
from collections import Counter, defaultdict
from datetime import datetime

import cv2
import matplotlib.pyplot as plt
import numpy as np
import pyrealsense2 as rs

from mini_project.database.connection import get_connection

logger = logging.getLogger("ShapeDetection")

# ------------------- Constants -------------------
MIN_SHAPE_AREA = 200
MAX_SHAPE_AREA = 5000
SCREW_TO_SCREW = 251.0  # mm
SCREW_TIMEOUT_SEC = 3
POSITION_TOLERANCE_MM = 10

# ------------------- Globals -------------------
shape_counter = defaultdict(int)
last_known_screw_positions = {}
# last_screw_update_time = None
known_objects = {}  # {(shape_type, color): [{x, y, z, name}]}
shape_history = defaultdict(list)  # {object_name: ["Circle", "Hexagon", ...]}

# ------------------- Color Ranges -------------------
color_ranges = {
    "red1": (np.array([0, 50, 50]), np.array([8, 255, 255])),
    "red2": (np.array([170, 50, 50]), np.array([180, 255, 255])),
    "orange": (np.array([9, 50, 50]), np.array([19, 255, 255])),
    "yellow": (np.array([20, 50, 50]), np.array([30, 255, 255])),
    "green": (np.array([35, 50, 50]), np.array([85, 255, 255])),
    "blue": (np.array([90, 50, 50]), np.array([140, 255, 255])),
    "pink": (np.array([145, 50, 50]), np.array([165, 255, 255])),
    "black": (np.array([0, 0, 0]), np.array([180, 255, 50])),
}


# ------------------- Helpers -------------------
def map_shape_to_object(shape_name):
    shape_name = shape_name.lower()
    mapping = {
        "circle": "Cylinder",
        "square": "Cube",
        "rectangle": "Cuboid",
        "triangle": "Wedge",
        "pentagon": "Pentagon",
        "hexagon": "Hexagon",
    }
    return mapping.get(shape_name, "unknown")


def normalize_rgb_color(bgr):
    b, g, r = bgr
    return [round(r / 255.0, 2), round(g / 255.0, 2), round(b / 255.0, 2)]


def mean_shift_segmentation(image, sp=21, sr=51):
    return cv2.pyrMeanShiftFiltering(image, sp, sr)


def get_color_name_from_ranges(hsv_value, color_ranges):
    for color_name, (lower, upper) in color_ranges.items():
        if all(lower[i] <= hsv_value[i] <= upper[i] for i in range(3)):
            return "red" if color_name.startswith("red") else color_name
    return "Unknown"


def detect_screw_positions(screw_roi, depth_frame, roi_origin, image):
    hsv = cv2.cvtColor(screw_roi, cv2.COLOR_BGR2HSV)

    screw_colors = {
        "orange": (np.array([9, 50, 50]), np.array([19, 255, 255])),
        "blue": (np.array([90, 50, 50]), np.array([140, 255, 255])),
    }

    screw_positions = {}

    for color_name, (lower, upper) in screw_colors.items():
        mask = cv2.inRange(hsv, lower, upper)
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        if not contours:
            continue

        largest = max(contours, key=cv2.contourArea)
        area = cv2.contourArea(largest)

        # ðŸš« Area filter: only consider reasonable screw sizes
        if area < 100 or area > 1200:
            continue

        # âœ… Compute center
        M = cv2.moments(largest)
        if M["m00"] == 0:
            continue

        cX = int(M["m10"] / M["m00"])
        cY = int(M["m01"] / M["m00"])
        real_cX = roi_origin[0] + cX  # roi for the screw
        real_cY = roi_origin[1] + cY
        depth = depth_frame.get_distance(real_cX, real_cY)

        # plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
        # plt.scatter(real_cX, real_cY, color="green", s=50, marker="o")  # Green dot
        # plt.show()

        screw_positions[color_name] = (real_cX, real_cY, depth)

        # ðŸŽ¯ Mark center
        cv2.circle(screw_roi, (cX, cY), 3, (0, 0, 0), -1)  # black dot at center

        # ðŸŸ  Orange circle / ðŸ”µ Blue square
        if color_name == "orange":
            cv2.circle(screw_roi, (cX, cY), 12, (0, 140, 255), 2)
        elif color_name == "blue":
            cv2.rectangle(
                screw_roi, (cX - 12, cY - 12), (cX + 12, cY + 12), (255, 0, 0), 2
            )

        cv2.putText(
            screw_roi,
            f"{color_name} screw",
            (cX - 25, cY - 18),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.4,
            (0, 0, 0),
            1,
        )

    return screw_positions


def find_table_roi(image):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    blurred = cv2.GaussianBlur(gray, (5, 5), 0)
    _, thresh = cv2.threshold(blurred, 100, 255, cv2.THRESH_BINARY)
    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    if not contours:
        logger.warning("âš ï¸  No table detected.")
        return None, None, image

    largest_contour = max(contours, key=cv2.contourArea)
    x, y, w, h = cv2.boundingRect(largest_contour)
    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 0, 255), 2)

    roi = image[y + 20 : y + h - 150, x + 20 : x + w - 20]
    cv2.imshow("Table_ROI", roi)

    debug_image = image.copy()
    cv2.drawContours(debug_image, [largest_contour], -1, (0, 255, 0), 2)
    cv2.imshow("debug_image", debug_image)
    return roi, (x + 20, y + 20), debug_image


def upsert_camera_vision_record(
    conn,
    object_name,
    object_color,
    color_code,
    pos_x,
    pos_y,
    pos_z,
    rot_x,
    rot_y,
    rot_z,
    usd_name,
):
    try:
        cursor = conn.cursor()
        query = "SELECT object_id FROM camera_vision WHERE object_name = %s"
        cursor.execute(query, (object_name,))
        result = cursor.fetchone()

        if result is None:
            insert_query = """
                INSERT INTO camera_vision
                (object_name, object_color, color_code, pos_x, pos_y, pos_z,
                 rot_x, rot_y, rot_z, usd_name, last_detected)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW())
            """
            cursor.execute(
                insert_query,
                (
                    object_name,
                    object_color,
                    color_code,
                    pos_x,
                    pos_y,
                    pos_z,
                    rot_x,
                    rot_y,
                    rot_z,
                    usd_name,
                ),
            )
            logger.info(f"âž• Inserted {object_name} into camera_vision")
        else:
            update_query = """
                UPDATE camera_vision
                SET object_color = %s,
                    color_code = %s,
                    pos_x = %s,
                    pos_y = %s,
                    pos_z = %s,
                    rot_x = %s,
                    rot_y = %s,
                    rot_z = %s,
                    usd_name = %s,
                    last_detected = NOW()
                WHERE object_name = %s
            """
            cursor.execute(
                update_query,
                (
                    object_color,
                    color_code,
                    pos_x,
                    pos_y,
                    pos_z,
                    rot_x,
                    rot_y,
                    rot_z,
                    usd_name,
                    object_name,
                ),
            )
            logger.info(f"ðŸ” Updated {object_name} in camera_vision")
        conn.commit()

    except Exception as e:
        print(f"Database error in upsert_camera_vision_record: {e}")
        conn.rollback()


def detect_shape(contour):
    epsilon = 0.03 * cv2.arcLength(contour, True)
    approx = cv2.approxPolyDP(contour, epsilon, True)
    if len(approx) == 3:
        return "Triangle", approx
    elif len(approx) == 4:
        x, y, w, h = cv2.boundingRect(approx)
        aspect_ratio = float(w) / h
        return ("Square" if 0.95 <= aspect_ratio <= 1.2 else "Rectangle"), approx
    elif len(approx) == 5:
        return "Pentagon", approx
    elif len(approx) == 6:
        return "Hexagon", approx
    elif len(approx) > 6:
        return "Circle", approx
        # if circularity > 0.85:
        #     return "Circle", approx
        # else:
        #     return "Ellipse" or "Unknown", approx
    else:
        return "Unknown", approx


def detect_colored_shapes(
    roi,
    image,
    roi_origin,
    depth_frame,
    conn,
    origin_px,
    scaling_factor,
    screw_positions,
):

    hsv_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)
    all_mask = np.zeros_like(hsv_roi[:, :, 0])
    for lower, upper in color_ranges.values():
        all_mask |= cv2.inRange(hsv_roi, lower, upper)

    # ðŸ” Detect screws *inside* the ROI
    if "orange" not in screw_positions or "blue" not in screw_positions:
        logger.warning("ðŸŸ ðŸ”µ Missing screws! Cannot compute relative positions.")
        return [], roi

    # ðŸ“ Compute scaling factor from screw distance
    ox, oy, _ = screw_positions["orange"]
    bx, by, _ = screw_positions["blue"]
    pixel_dist = np.linalg.norm([bx - ox, by - oy])
    actual_mm = SCREW_TO_SCREW  # real-world mm between screws
    scaling_factor = actual_mm / pixel_dist
    print(scaling_factor)
    origin_px = (ox, oy)

    contours, _ = cv2.findContours(all_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    output_image = roi.copy()
    center_points = []

    # Clear the table before the new update for new frame
    conn = get_connection()
    clear_old_camera_vision_records(conn)

    for contour in contours:

        area = cv2.contourArea(contour)
        # âœ… Filter shapes by contour area (tune as needed)
        if area < MIN_SHAPE_AREA or area > MAX_SHAPE_AREA:
            continue

        shape_name, approx = detect_shape(contour)
        mask = np.zeros_like(hsv_roi[:, :, 0])
        cv2.drawContours(mask, [contour], -1, 255, thickness=cv2.FILLED)
        x, y, w, h = cv2.boundingRect(contour)
        bgr = tuple(int(v) for v in cv2.mean(cv2.bitwise_and(roi, roi, mask=mask))[:3])

        mean_hsv = tuple(map(int, cv2.mean(hsv_roi, mask=mask)[:3]))
        color_name = get_color_name_from_ranges(mean_hsv, color_ranges)

        M = cv2.moments(contour)
        if M["m00"] == 0:
            continue
        cX = int(M["m10"] / M["m00"])
        cY = int(M["m01"] / M["m00"])
        real_cX = roi_origin[0] + cX - 180
        real_cY = roi_origin[1] + cY

        # plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
        # plt.scatter(real_cX, real_cY, color="green", s=50, marker="o")  # Green dot
        # plt.show()

        depth_meters = depth_frame.get_distance(real_cX, real_cY)
        depth_mm = depth_meters * 1000  # convert to millimeters
        shape_height = 1.91

        # ðŸ§® Compute relative mm from orange screw
        # rel_x_mm = (real_cX - roi_origin[0] - ox) * scaling_factor
        # rel_y_mm = (real_cY - roi_origin[1] - oy) * scaling_factor

        rel_x_mm = (real_cX - ox) * scaling_factor
        rel_y_mm = (real_cY - oy) * scaling_factor

        mean_bgr = cv2.mean(cv2.bitwise_and(roi, roi, mask=mask))[:3]
        color_code = normalize_rgb_color(mean_bgr)
        rot_z = cv2.minAreaRect(contour)[-1]

        # shape_type = map_shape_to_object(shape_name)

        matched_name = None
        for obj in known_objects.get(color_name, []):
            dx = abs(obj["x"] - rel_x_mm)
            dy = abs(obj["y"] - rel_y_mm)
            if dx < POSITION_TOLERANCE_MM and dy < POSITION_TOLERANCE_MM:
                matched_name = obj["name"]
                obj.update({"x": rel_x_mm, "y": rel_y_mm, "z": depth_mm})
                break

        if matched_name:
            object_name = matched_name
        else:
            shape_counter[shape_name] += 1
            object_name = f"{shape_name}_{shape_counter[shape_name]}"
            known_objects.setdefault(color_name, []).append(
                {"x": rel_x_mm, "y": rel_y_mm, "z": depth_mm, "name": object_name}
            )

        # â³ Temporal shape smoothing
        shape_history[object_name].append(shape_name)
        if len(shape_history[object_name]) > 15:
            shape_history[object_name].pop(0)
        most_common_shape = Counter(shape_history[object_name]).most_common(1)[0][0]
        shape_name = most_common_shape
        shape_type = map_shape_to_object(shape_name)

        usd_name = f"{shape_type}.usd"

        upsert_camera_vision_record(
            conn,
            object_name,
            color_name,
            color_code,
            float(rel_x_mm),
            float(rel_y_mm),
            float(shape_height),
            0.0,
            0.0,
            rot_z,
            usd_name,
        )

        center_points.append((cX, cY, color_name, shape_name, round(depth_meters, 3)))
        logger.info(
            f"ðŸ§© Detected {color_name} {shape_name} â†’ stored as '{object_name}' "
            f"(x={rel_x_mm:.1f}mm, y={rel_y_mm:.1f}mm, z={depth_mm:.1f}mm)"
        )

        cv2.drawContours(output_image, [approx], -1, (0, 255, 0), 2)
        cv2.putText(
            output_image,
            f"{color_name} {shape_name}",
            (cX - 20, cY - 20),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.3,
            (0, 0, 0),
            1,
        )
        cv2.putText(
            output_image,
            f"{depth_meters:.2f}m",
            (cX - 20, cY - 8),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.3,
            (255, 0, 255),
            1,
        )
        cv2.circle(output_image, (cX, cY), 2, (255, 0, 0), -1)
    shape_counter.clear()
    return center_points, output_image


def clear_old_camera_vision_records(conn, expiry_seconds=2):
    try:
        cursor = conn.cursor()
        cursor.execute("DELETE FROM camera_vision")
        conn.commit()
        logger.info("ðŸ§¨ Deleted all camera_vision records for fresh session")
    except Exception as e:
        logger.error(f"âŒ Failed to delete all camera_vision records: {e}")
        conn.rollback()


def visualize_shapes(image, roi_origin, annotated_roi):
    x, y = roi_origin
    h, w = annotated_roi.shape[:2]
    combined = image.copy()
    combined[y : y + h, x : x + w] = annotated_roi
    return combined


def camera_pipeline():
    conn = get_connection()

    pipeline = rs.pipeline()
    config = rs.config()
    config.enable_stream(rs.stream.color, 1280, 720, rs.format.bgr8, 30)
    config.enable_stream(rs.stream.depth, 1280, 720, rs.format.z16, 30)
    profile = pipeline.start(config)

    depth_sensor = profile.get_device().first_depth_sensor()
    depth_scale = depth_sensor.get_depth_scale()
    logger.info("ðŸ” Depth scale: %s", depth_scale)

    shapes_detected = True

    try:
        while True:
            frames = pipeline.wait_for_frames()
            color_frame = frames.get_color_frame()
            depth_frame = frames.get_depth_frame()
            if not color_frame or not depth_frame:
                continue

            image = np.asanyarray(color_frame.get_data())

            roi, roi_origin, debug_image = find_table_roi(image)
            # Extract screw ROI (top edge of the main ROI)
            screw_roi_height = 100
            screw_roi = roi[0:screw_roi_height, 180:-180]
            cv2.rectangle(
                roi,
                (180, 0),
                (roi.shape[1] - 180, screw_roi_height),
                (0, 255, 0),
                2,
            )

            if roi is None:
                break

            roi = mean_shift_segmentation(roi)
            global last_known_screw_positions
            global last_screw_update_time
            screw_positions = detect_screw_positions(
                screw_roi, depth_frame, roi_origin, image
            )

            # ðŸ” Overlay annotated screw_roi back into roi so markings are visible
            roi[0 : screw_roi.shape[0], 180:-180] = screw_roi

            current_time = datetime.now()

            if "orange" in screw_positions and "blue" in screw_positions:
                last_known_screw_positions = screw_positions
                last_screw_update_time = current_time
                logger.debug("âœ… Screw positions updated")

            elif last_known_screw_positions and last_screw_update_time:
                time_since_seen = (
                    current_time - last_screw_update_time
                ).total_seconds()

                if time_since_seen > SCREW_TIMEOUT_SEC:
                    logger.warning("â° Screw positions expired â€” skipping frame")
                    continue  # Donâ€™t use outdated screws
                else:
                    screw_positions = last_known_screw_positions
                    logger.warning(
                        f"âš ï¸    Using last-known screw positions (stale for {time_since_seen:.1f}s)"
                    )
            else:
                logger.warning("âŒ No valid screw data available â€” skipping frame")
                continue

            # Extract screw positions
            orange_screw = screw_positions["orange"]
            blue_screw = screw_positions["blue"]

            # Compute pixel distance and scaling factor
            pixel_dist = np.linalg.norm(
                np.array(orange_screw[:2]) - np.array(blue_screw[:2])
            )
            scaling_factor = (
                SCREW_TO_SCREW / pixel_dist
            )  # mm is real-world screw distance

            center_points, annotated_roi = detect_colored_shapes(
                roi,
                image,
                roi_origin,
                depth_frame,
                conn,
                origin_px=orange_screw[:2],
                scaling_factor=scaling_factor,
                screw_positions=screw_positions,
            )

            conn.commit()
            combined_view = visualize_shapes(image, roi_origin, annotated_roi)

            logger.debug("ðŸ“Œ Detected: %s", center_points)
            cv2.imshow("Detected Shapes", combined_view)
            if not center_points:
                shapes_detected = False
                break

            # screw_positions_X, screw_positions_Y, _ = screw_positions["orange"]
            # cv2.circle(image, (screw_positions_X, screw_positions_X), 5, (0, 255, 0), -1)
            # plt.imshow( h)
            # plt.show()
            key = cv2.waitKey(10) & 0xFF
            if key == 27 or key == ord("q"):
                break

    finally:
        pipeline.stop()
        conn.close()
        cv2.destroyAllWindows()

    return shapes_detected


if __name__ == "__main__":
    camera_pipeline()
####################
# authentication/_face_auth.py
"""
authentication/_face_auth.py
This module provides classes and functions for face authentication using computer vision and deep learning techniques.
It supports user registration and identification based on facial features, with optional integration for voice authentication.
    - Utility class for face detection, drawing bounding boxes, overlaying text, and selecting the best face from multiple detections.
    - Main class for managing face authentication.
    - Handles:
        - Preloading and managing face encodings from a PostgreSQL database.
        - Building and refreshing a FAISS index for fast similarity search.
        - Validating user input and managing user records.
    - Context manager for handling video capture resources using OpenCV.
    - Entry point for the FaceAuthSystem application.
    - Initializes required directories and starts the authentication system.
Constants (imported from config):
    AUTO_CAPTURE_FRAME_COUNT: int
    - Number of consecutive frames required for automatic face capture.
    EMAIL_PATTERN: str
    - Regular expression pattern for validating email addresses.
    FACE_CAPTURE_PATH: Path
    - Path for saving captured face images.
    FACE_MATCH_THRESHOLD: float
    - Threshold for face similarity matching.
    FACIAL_DATA_PATH: Path
    - Path for storing facial data.
    IDENTIFICATION_FRAMES: int
    - Number of frames to process during user identification.
    LIU_ID_PATTERN: str
    - Regular expression pattern for validating LIU IDs.
    MAX_ENCODINGS_PER_USER: int
    - Maximum number of face encodings to store per user.
    TEMP_AUDIO_PATH: Path
    - Path for storing temporary audio files.
    TIMEDELAY: float
    - Delay between frames during identification.
    VOICE_DATA_PATH: Path
    - Path for storing voice data.
Usage:
    Run this module as a script to start the face authentication system.
    The system will prompt for user registration or identification as needed.
Dependencies:
    - OpenCV (cv2)
    - face_recognition
    - faiss
    - numpy
    - psycopg2
    - mini_project.authentication._voice_auth
    - mini_project.config.app_config
    - mini_project.database.connection
"""



# import sys
import logging
import pickle
import re
import time
from contextlib import contextmanager
from typing import Dict, List, Optional, Tuple

import cv2
import face_recognition
import faiss  # For fast similarity search
import numpy as np
import psycopg2

from mini_project.authentication._voice_auth import VoiceAuth
from mini_project.config.app_config import (
    AUTO_CAPTURE_FRAME_COUNT,
    EMAIL_PATTERN,
    FACE_CAPTURE_PATH,
    FACE_MATCH_THRESHOLD,
    FACIAL_DATA_PATH,
    IDENTIFICATION_FRAMES,
    LIU_ID_PATTERN,
    MAX_ENCODINGS_PER_USER,
    TEMP_AUDIO_PATH,
    TIMEDELAY,
    VOICE_DATA_PATH,
    setup_logging,
)
from mini_project.database.connection import get_connection

logger = logging.getLogger("FaceAuthSystem")


# Context manager for handling an video session.
@contextmanager
def VideoCaptureContext(index: int = 0):
    cap = cv2.VideoCapture(index)
    try:
        yield cap
    finally:
        cap.release()


class FaceUtils:

    @staticmethod
    def detect_faces(frame: np.ndarray) -> Tuple[List[tuple], List[np.ndarray]]:
        """
        Detect faces and compute encodings in a frame.

        Returns:
            A tuple of (face_locations, face_encodings).
        """
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        face_locations = face_recognition.face_locations(rgb_frame)
        face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)
        return face_locations, face_encodings

    @staticmethod
    def draw_bounding_boxes(
        frame: np.ndarray, face_locations: List[tuple]
    ) -> np.ndarray:
        """Draw bounding boxes around detected faces."""
        for top, right, bottom, left in face_locations:
            cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)
        return frame

    @staticmethod
    def draw_text(
        frame: np.ndarray,
        text: str,
        position: Tuple[int, int] = (10, 30),
        font_scale: float = 0.7,
        color: Tuple[int, int, int] = (0, 255, 0),
        thickness: int = 2,
    ) -> None:
        """Overlay text on the frame at the given position."""
        cv2.putText(
            frame,
            text,
            position,
            cv2.FONT_HERSHEY_SIMPLEX,
            font_scale,
            color,
            thickness,
        )

    @staticmethod
    def select_best_face(
        face_encodings: List[np.ndarray], face_locations: List[tuple]
    ) -> int:
        """
        Select the largest face when multiple faces are detected.

        Returns:
            The index of the face with the largest bounding box.
        """
        face_sizes = [
            (bottom - top) * (right - left)
            for (top, right, bottom, left) in face_locations
        ]
        return int(np.argmax(face_sizes))


class FaceAuthSystem:
    def __init__(self) -> None:
        self.conn = get_connection()
        self.cursor = self.conn.cursor()
        self.face_utils = FaceUtils()
        self.known_encodings: Dict[str, dict] = self._preload_encodings()
        self.faiss_index: Optional[faiss.IndexFlatL2] = self._build_faiss_index()

    def _preload_encodings(self) -> Dict[str, dict]:
        """Preload all face encodings from the database."""
        encodings: Dict[str, dict] = {}
        try:
            self.cursor.execute(
                "SELECT user_id, first_name, last_name, liu_id, face_encoding, voice_embedding FROM users"
            )
            users = self.cursor.fetchall()
            for user in users:
                if user[4]:
                    encodings[user[3]] = {
                        "user_id": user[0],
                        "first_name": user[1],
                        "last_name": user[2],
                        "liu_id": user[3],
                        "encodings": pickle.loads(user[4].tobytes()),
                        "voice_embedding": (
                            pickle.loads(user[5].tobytes())[0] if user[5] else None
                        ),
                    }
        except psycopg2.Error as e:
            logger.error("ðŸ”´ Database error during encoding preload: %s", e)
        return encodings

    def _build_faiss_index(self) -> Optional[faiss.IndexFlatL2]:
        """Build a FAISS index for fast face encoding search."""
        if not self.known_encodings:
            logger.debug("No known encodings to build FAISS index.")
            return None

        all_encodings: List[np.ndarray] = []
        self.user_ids: List[str] = []  # Maps FAISS index entries to LIU IDs.
        for liu_id, user in self.known_encodings.items():
            for encoding in user["encodings"]:
                all_encodings.append(encoding)
                self.user_ids.append(liu_id)
        if not all_encodings:
            logger.debug("No encodings found after processing known_encodings.")
            return None

        all_encodings_np = np.array(all_encodings, dtype=np.float32)
        index = faiss.IndexFlatL2(all_encodings_np.shape[1])
        index.add(all_encodings_np)
        logger.debug("FAISS index built with %d encodings.", all_encodings_np.shape[0])
        return index

    def _refresh_index(self) -> None:
        """Refresh the in-memory known encodings and rebuild the FAISS index."""
        self.known_encodings = self._preload_encodings()
        self.faiss_index = self._build_faiss_index()
        logger.info("âœ… FAISS index and known encodings refreshed.")

    def _validate_user_input(self, liu_id: str, email: str) -> bool:
        """Validate LIU ID and email formats."""
        if not re.match(LIU_ID_PATTERN, liu_id):
            logger.error("ðŸ”´ Invalid LIU ID format. Expected format: abc123")
            return False
        if not re.match(EMAIL_PATTERN, email):
            logger.error("ðŸ”´ Invalid email format")
            return False
        return True

    def _process_frame(
        self, cap: cv2.VideoCapture, instruction: str
    ) -> Tuple[Optional[np.ndarray], List[tuple], List[np.ndarray]]:
        """
        Read a frame from the camera, process it (face detection and bounding boxes),
        and overlay instruction text.

        Returns:
            A tuple of (frame, face_locations, face_encodings) or (None, [], []) if frame read fails.
        """
        ret, frame = cap.read()
        if not ret:
            logger.debug("No frame retrieved from camera.")
            return None, [], []
        face_locations, face_encodings = self.face_utils.detect_faces(frame)
        frame = self.face_utils.draw_bounding_boxes(frame, face_locations)
        self.face_utils.draw_text(frame, instruction)
        return frame, face_locations, face_encodings

    def _capture_face_auto(self) -> Optional[Tuple[np.ndarray, np.ndarray]]:
        """
        Capture a face in automatic mode.
        Uses a consecutive detection counter to ensure stable detection.
        """
        with VideoCaptureContext(0) as cap:
            if not cap.isOpened():
                logger.error("ðŸ”´ Error: Camera not accessible.")
                return None
            consecutive_detections = (
                0  # Count of consecutive frames with exactly one face.
            )
            captured_frame: Optional[np.ndarray] = None
            captured_encoding: Optional[np.ndarray] = None

            while True:
                frame, _, face_encodings = self._process_frame(
                    cap, "Face detected, capturing..."
                )
                if frame is None:
                    logger.debug("Frame processing failed; exiting auto capture loop.")
                    break

                if len(face_encodings) == 1:
                    consecutive_detections += 1
                    self.face_utils.draw_text(
                        frame,
                        f"Detected: {consecutive_detections}/{AUTO_CAPTURE_FRAME_COUNT}",
                        position=(10, 60),
                    )
                    if consecutive_detections >= AUTO_CAPTURE_FRAME_COUNT:
                        captured_frame = frame
                        captured_encoding = face_encodings[0]
                        logger.debug("Auto capture threshold reached; capturing face.")
                        break
                else:
                    # Reset the counter if no face or multiple faces are detected.
                    consecutive_detections = 0

                cv2.imshow("Face Capture - Auto", frame)
                key = cv2.waitKey(1) & 0xFF
                if key == ord("q"):
                    logger.info("Auto capture aborted by user.")
                    break

            cv2.destroyAllWindows()
            if captured_frame is not None and captured_encoding is not None:
                return captured_frame, captured_encoding
            return None

    def _capture_face_manual(self) -> Optional[Tuple[np.ndarray, np.ndarray]]:
        """
        Capture a face in manual mode.
        The user must press 's' to save when a single face is detected.
        """
        with VideoCaptureContext(0) as cap:
            if not cap.isOpened():
                logger.error("ðŸ”´ Error: Camera not accessible.")
                return None

            captured_frame: Optional[np.ndarray] = None
            captured_encoding: Optional[np.ndarray] = None

            while True:
                frame, _, face_encodings = self._process_frame(
                    cap, "Press 's' to save, 'q' to quit"
                )
                if frame is None:
                    logger.debug(
                        "Frame processing failed; exiting manual capture loop."
                    )
                    break

                if len(face_encodings) >= 1:
                    self.face_utils.draw_text(
                        frame, "Face detected!", position=(10, 60)
                    )

                cv2.imshow("Face Capture - Manual", frame)
                key = cv2.waitKey(1) & 0xFF
                if key == ord("q"):
                    logger.info("Manual capture aborted by user.")
                    break
                if key == ord("s"):
                    if len(face_encodings) == 1:
                        captured_frame = frame
                        captured_encoding = face_encodings[0]
                        logger.debug("Face captured in manual mode.")
                        break
                    else:
                        logger.warning(
                            "No face or multiple faces detected; cannot capture."
                        )

            cv2.destroyAllWindows()
            if captured_frame is not None and captured_encoding is not None:
                return captured_frame, captured_encoding
            return None

    def _capture_face(
        self, capture_mode: str = "auto"
    ) -> Optional[Tuple[np.ndarray, np.ndarray]]:
        """Capture a face using the specified mode (auto/manual)."""
        if capture_mode == "auto":
            return self._capture_face_auto()
        else:
            return self._capture_face_manual()

    def _capture_face_multi(
        self,
    ) -> Optional[Tuple[np.ndarray, List[np.ndarray], List[tuple]]]:
        """
        Capture a single frame and return all detected face encodings and locations.
        This mode is used for identification whether one or multiple faces are present.
        """
        with VideoCaptureContext(0) as cap:
            if not cap.isOpened():
                logger.error("ðŸ”´ Error: Camera not accessible.")
                return None
            ret, frame = cap.read()
            if not ret:
                logger.error("ðŸ”´ Failed to capture frame for multi-face detection.")
                return None
            face_locations, face_encodings = self.face_utils.detect_faces(frame)
            return frame, face_encodings, face_locations

    def register_user(self) -> None:
        """
        Registers a new user or updates an existing user's face encoding in the database.
        This method captures a user's face, collects their personal details, and stores
        the information in the database. If the user already exists (based on LIU ID or email),
        it prompts the user to confirm whether to update their face encoding.
        Steps:
        1. Captures the user's face encoding.
        2. Collects user details: first name, last name, LIU ID, and email.
        3. Validates the user input.
        4. Checks if the user already exists in the database:
           - If the user exists, prompts for confirmation to update the face encoding.
           - If the user does not exist, creates a new user record.
        5. Updates or inserts the user's face encoding and other details in the database.
        6. Refreshes the face recognition index.
        Returns:
            None
        Raises:
            psycopg2.Error: If there is a database error during the registration process.
        Logs:
            - Logs success or failure messages for each step of the process.
            - Logs errors if face capture fails or if required inputs are missing.
        Notes:
            - The maximum number of face encodings stored per user is enforced.
            - The face image is saved to a predefined path for the user's profile.
        """
        frame_encoding = self._capture_face("manual")
        if not frame_encoding:
            logger.error("ðŸ”´ Face capture failed during registration.")
            return False

        frame, encoding = frame_encoding
        logger.info("ðŸŸ¢ Face captured for registration.")

        # Gather registration details once.
        first_name = input("ðŸš« Enter your first name: ").strip()
        last_name = input("ðŸš« Enter your last name: ").strip()
        liu_id = input("ðŸš« Enter your LIU ID (e.g. abcxy123): ").strip()
        email = input("ðŸš« Enter your Email: ").strip()

        if not first_name or not last_name:
            logger.error("ðŸ”´ First name and last name cannot be empty.")
            return False

        if not self._validate_user_input(liu_id, email):
            return False

        try:
            with self.conn:
                cursor = self.conn.cursor()
                # Query for an existing user with the same LIU ID or email.
                cursor.execute(
                    "SELECT user_id, face_encoding FROM users WHERE liu_id = %s OR email = %s",
                    (liu_id, email),
                )
                existing = cursor.fetchone()

                if existing:
                    logger.info(
                        "ðŸŸ¢ User already exists. Prompting for update confirmation..."
                    )
                    confirm = input("ðŸš« Update face encoding? (y/n): ").strip().lower()
                    if confirm != "y":
                        logger.info("ðŸ”´ Registration aborted by user.")
                        return False
                    else:
                        logger.info("âœ… User confirmed update of face encoding.")
                        user_id, existing_encoding_bytea = existing
                        existing_encodings = (
                            pickle.loads(existing_encoding_bytea)
                            if existing_encoding_bytea
                            else []
                        )
                        existing_encodings.append(encoding)
                        # Enforce maximum stored encodings.
                        existing_encodings = existing_encodings[
                            -MAX_ENCODINGS_PER_USER:
                        ]
                        cursor.execute(
                            "UPDATE users SET face_encoding = %s WHERE user_id = %s",
                            (
                                psycopg2.Binary(pickle.dumps(existing_encodings)),
                                user_id,
                            ),
                        )
                        logger.info(
                            "âœ… Face encoding updated for %s %s", first_name, last_name
                        )
                        # self._refresh_index()
                        # return
                        user_row_id = user_id

                else:
                    profile_image_path = str(FACE_CAPTURE_PATH / f"{liu_id}.jpg")
                    preferences = "{}"
                    interaction_memory = "[]"
                    face_blob = psycopg2.Binary(pickle.dumps([encoding]))
                    cursor.execute(
                        """INSERT INTO users
                           (first_name, last_name, liu_id, email, face_encoding,
                           preferences, profile_image_path, interaction_memory)
                           VALUES (%s, %s, %s, %s, %s, %s, %s, %s)""",
                        (
                            first_name,
                            last_name,
                            liu_id,
                            email,
                            face_blob,
                            preferences,
                            profile_image_path,
                            interaction_memory,
                        ),
                    )
                    cv2.imwrite(profile_image_path, frame)
                    user_row_id = cursor.lastrowid
                    logger.info(
                        "âœ… User %s %s registered successfully with LIU ID: %s",
                        first_name,
                        last_name,
                        liu_id,
                    )
            self._refresh_index()
        except psycopg2.Error as e:
            self.conn.rollback()
            logger.error("ðŸ”´ Registration failed: %s", e)
            raise
        # On success:
        self._refresh_index()
        logger.info("âœ… User registered successfully.")
        return {
            "liu_id": liu_id,
            "first_name": first_name,
            "last_name": last_name,
            "voice_embedding": None  # or fetch from DB if needed
        }

    def identify_user(self) -> None:
        """
        Capture several frames in multi-face mode and combine the results:
          - For each frame, perform identification on each detected face.
          - A dictionary tallies recognized users (using their LIU ID) across frames.
          - If any frame produces an unknown face (i.e. no match), an unknown flag is set.

        After processing:
          - Recognized users are welcomed.
          - If any unknown face is detected, the system explains that the capture from identification
            is not used for registration and calls register_user to re-capture the face.

        Inline Comment: The system uses multiple frames to improve reliability.
        """

        logger.info("ðŸŸ¢ Starting face identification. Please look at the camera..")
        recognized_users = {}  # key: liu_id, value: count across frames
        unknown_found = False

        for i in range(IDENTIFICATION_FRAMES):
            result = self._capture_face_multi()
            if result is None:
                logger.error(
                    "ðŸ”´ Frame %d: Failed to capture frame for identification.", i + 1
                )
                continue
            _, face_encodings, _ = result
            if not face_encodings:
                logger.debug("Frame %d: No faces detected.", i + 1)
                continue

            for encoding in face_encodings:
                query_encoding = np.array([encoding], dtype=np.float32)
                if self.faiss_index is not None:
                    distances, indices = self.faiss_index.search(query_encoding, k=1)
                    if distances[0][0] <= FACE_MATCH_THRESHOLD:
                        liu_id = self.user_ids[indices[0][0]]
                        recognized_users[liu_id] = recognized_users.get(liu_id, 0) + 1
                    else:
                        unknown_found = True
                else:
                    unknown_found = True
            time.sleep(TIMEDELAY)  # Slight delay between frames

        # Welcome recognized users (if recognized in any frame)
        if recognized_users:
            for liu_id in recognized_users:
                best_liu = max(recognized_users, key=recognized_users.get)
                user = self.known_encodings.get(best_liu)
                if user:
                    logger.info(
                        f"âœ… Welcome back, {user['first_name']} {user['last_name']}!"
                    )
                    return user
        else:
            logger.info("ðŸ”´ No known faces detected.")

        # Prompt if any unknown face was found.
        if unknown_found:
            logger.info(
                "ðŸ¤ A detected face is unknown, the system will re-capture it during registration."
            )
            response = (
                input("ðŸš« Would you like to register a user? (y/n): ").strip().lower()
            )
            if response == "y":
                self.register_user()

                # âœ… Re-identify after registration and return the user
                user = self.identify_user()
                if user:
                    logger.info(
                        f"âœ… User Authenticated after registration. Welcome {user['first_name']} {user['last_name']}, (liu_id: {user['liu_id']})"
                    )
                    return user
            else:
                logger.info("âŒ Registration declined. Authentication aborted.")
                return None
        return None

    def run(self) -> None:
        """
        Main application loop in auto-identification mode.
        The system continuously calls the identify function so that users are identified (and welcomed) immediately.
        After each identification round, the operator can re-run identification or quit.
        """
        logger.info(f"ðŸŸ¢ Auto-identification mode enabled. Press Ctrl+C to exit.")
        try:
            while True:
                self.identify_user()
                choice = (
                    input("ðŸŸ¢ Press Enter to re-run identification or 'q' to quit: ")
                    .strip()
                    .lower()
                )
                if choice == "q":
                    self.close()
                    logger.info("ðŸŸ¢ Exiting...")
                    break
        except KeyboardInterrupt:
            self.close()
            logger.info("Exiting auto-identification mode.")

    def close(self):
        self.cursor.close()
        self.conn.close()


def main() -> None:
    """Entry point for the FaceAuthSystem application."""
    FACIAL_DATA_PATH.mkdir(parents=True, exist_ok=True)
    FACE_CAPTURE_PATH.mkdir(parents=True, exist_ok=True)
    auth_system = FaceAuthSystem()
    auth_system.run()


if __name__ == "__main__":
    main()



#################################
# authentication/_voice_auth.py
""" _voice_auth.py
This module provides the VoiceAuth class for handling voice-based user authentication and registration.
It includes functionality for:
- Recording audio from the user's microphone.
- Transcribing recorded audio to text using Google Speech Recognition.
- Capturing voice embeddings using the Resemblyzer library.
- Storing and retrieving voice embeddings in a PostgreSQL database and as local pickle files.
- Registering new users with voice authentication.
- Registering or updating a user's voice sample.
- Verifying a user's identity by comparing a new voice sample to the stored embedding.
- Handling audio session management and directory setup for audio data.
Classes:
    VoiceAuth: Handles all aspects of voice authentication, including registration, verification, and embedding management.
Functions:
    audio_session: Context manager for safely handling audio recording sessions.
Configuration:
    Uses application configuration for paths, retry limits, and thresholds.
Dependencies:
    - sounddevice
    - resemblyzer
    - scipy
    - speech_recognition
    - psycopg2
    - scikit-learn
    - pickle
    - logging
Usage:
    Run as a script to interactively register or authenticate users via voice.

 """

import logging
import os
import pickle
import re
import string
import warnings
from contextlib import contextmanager
from typing import List

import psycopg2
import sounddevice as sd
from resemblyzer import VoiceEncoder, preprocess_wav
from scipy.io.wavfile import write
from sklearn.metrics.pairwise import cosine_similarity
from speech_recognition import AudioFile, Recognizer, RequestError, UnknownValueError

from mini_project.config.app_config import (
    MAX_RETRIES,
    TEMP_AUDIO_PATH,
    TRANSCRIPTION_SENTENCE,
    VOICE_DATA_PATH,
    VOICE_MATCH_THRESHOLD,
    setup_logging,
)
from mini_project.database.connection import get_connection

# Suppress warnings if desired
warnings.filterwarnings("ignore", category=FutureWarning)
logger = logging.getLogger("VoiceAutSystem")


# Context manager for handling an audio recording session.
@contextmanager
def audio_session():
    try:
        yield
    finally:
        sd.stop()


class VoiceAuth:
    """
    A class for voice authentication which handles:
    - Audio recording.
    - Transcription.
    - Voice embedding capture.
    - Storing embeddings in a SQLite database and on disk.
    """

    def __init__(
        self,
        temp_audio_path: str = TEMP_AUDIO_PATH,
        voice_data_path: str = VOICE_DATA_PATH,
    ) -> None:
        self.temp_audio_path = temp_audio_path
        self.voice_data_path = voice_data_path
        self.encoder = VoiceEncoder()
        self._create_directories()
        self.conn = get_connection()
        self.cursor = self.conn.cursor()

    def _create_directories(self) -> None:
        """Ensure that the directories for voice data and temporary audio exist."""
        os.makedirs(self.voice_data_path, exist_ok=True)
        os.makedirs(self.temp_audio_path, exist_ok=True)
        logger.info("ðŸŸ¢ Directories ensured for voice data and temporary audio.")

    def _record_audio(
        self, filename: str, prompt: str, duration: int = 5, sampling_rate: int = 16000
    ) -> None:
        """
        Record audio from the microphone and save it to a WAV file.

        Args:
            filename (str): The file path to save the recorded audio.
            prompt (str): The message to display to the user before recording.
            duration (int): Duration of the recording in seconds.
            sampling_rate (int): Sampling rate for the audio recording.
        """
        logger.info(prompt)
        try:
            with audio_session():
                audio = sd.rec(
                    int(duration * sampling_rate),
                    samplerate=sampling_rate,
                    channels=1,
                    dtype="int16",
                )
                sd.wait()
            write(filename, sampling_rate, audio)
            logger.info(f"âœ… Audio recorded and saved to {filename}")
        except Exception as e:
            msg = f"ðŸ”´ Error during audio recording: {e}"
            logger.error(msg)
            raise Exception(msg)

    def _transcribe_audio(self, filename: str) -> str:
        """
        Transcribe recorded audio to text using Google Speech Recognition.

        Args:
            filename (str): The file path of the audio file.

        Returns:
            str: The transcribed text.

        Raises:
            Exception: If transcription fails.
        """
        recognizer = Recognizer()
        try:
            with AudioFile(filename) as source:
                audio = recognizer.record(source)
                text = recognizer.recognize_google(audio)
                logger.info(f"ðŸŸ¢ Transcription: {text}")
                return text
        except UnknownValueError:
            msg = "ðŸ”´ Audio transcription failed: speech was unintelligible."
            logger.info(msg)
            raise Exception(msg)
        except RequestError as e:
            msg = f"ðŸ”´ Audio transcription failed: API error: {e}"
            logger.info(msg)
            raise Exception(msg)
        except Exception as e:
            msg = f"ðŸ”´ An unexpected error occurred during transcription: {e}"
            logger.info(msg)
            raise Exception(msg)

    def _capture_voice_embedding(self, audio_path: str) -> List[float]:
        """
        Capture a voice embedding from the recorded audio.

        Args:
            audio_path (str): Path to the audio file.

        Returns:
            List[float]: The voice embedding vector.
        """
        try:
            wav = preprocess_wav(audio_path)
            embedding = self.encoder.embed_utterance(wav)
            logger.info(f"âœ… Voice embedding captured, shape: {embedding.shape}")
            return embedding.tolist()
        except Exception as e:
            msg = f"ðŸ”´ Error capturing voice embedding: {e}"
            logger.error(msg)
            raise Exception(msg)

    @staticmethod
    def _validate_liu_id(liu_id: str) -> bool:
        """
        Validate the LIU ID format.

        Args:
            liu_id (str): The LIU ID to validate.

        Returns:
            bool: True if the LIU ID is valid; False otherwise.
        """
        pattern = r"^[a-z]{5}[0-9]{3}$"
        return bool(re.match(pattern, liu_id))

    def _save_voice_embedding(
        self, liu_id: str, voice_embedding: List[float], first_name: str, last_name: str
    ) -> None:
        """
        Save the voice embedding in the database (using upsert) and as a pickle file.

        Args:
            liu_id (str): The LIU ID of the user.
            voice_embedding (List[float]): The voice embedding vector.
            first_name (str): The user's first name.
            last_name (str): The user's last name.
        """
        voice_file = os.path.join(self.voice_data_path, f"{liu_id}_voice.pkl")
        try:
            self.cursor.execute(
                """
                    INSERT INTO users (liu_id, voice_embedding, first_name, last_name)
                    VALUES (%s, %s, %s, %s)
                    ON CONFLICT(liu_id) DO UPDATE SET voice_embedding = EXCLUDED.voice_embedding
                """,
                (
                    liu_id,
                    psycopg2.Binary(pickle.dumps([voice_embedding])),
                    first_name,
                    last_name,
                ),
            )
            self.conn.commit()

            with open(voice_file, "wb") as file:
                pickle.dump(voice_embedding, file)

            logger.info(f"âœ… Voice embedding saved for LIU ID: {liu_id}")
        except psycopg2.Error as e:
            self.conn.rollback()
            logger.error(f"ðŸ”´ Error saving voice embedding to database: {e}")
            raise
        except Exception as e:
            logger.error(f"ðŸ”´ Error saving voice embedding to file: {e}")
            raise

    def register_user(self) -> None:
        """
        Register a new user using voice authentication.

        This method:
        - Collects user details.
        - Records a voice statement.
        - Transcribes the statement.
        - Captures the voice embedding.
        - Saves the embedding in the database and as a file.
        """
        logger.info("ðŸŸ¡ Starting voice-driven user registration...")
        try:
            first_name = input("ðŸŸ¡ Enter your first name: ").strip()
            if not first_name:
                raise Exception("First name cannot be empty.")

            last_name = input("ðŸŸ¡ Enter your last name: ").strip()
            if not last_name:
                raise Exception("Last name cannot be empty.")

            liu_id = input("ðŸŸ¡ Enter your LIU ID (e.g. abcxy123): ").strip()
            if not self._validate_liu_id(liu_id):
                raise Exception("Invalid LIU ID format.")

            # Record a voice statement.
            statement_audio = os.path.join(
                self.temp_audio_path, f"{liu_id}_statement.wav"
            )

            # Retry loop
            max_attempts = MAX_RETRIES
            for attempt in range(1, max_attempts + 1):
                self._record_audio(
                    statement_audio,
                    f"ðŸŸ¡ Please read the following sentence clearly: '{TRANSCRIPTION_SENTENCE}'",
                    duration=12,
                )

                # Transcribe the audio.
                transcription = self._transcribe_audio(statement_audio)
                if not transcription:
                    logger.info("ðŸ”´ Audio Transcription failed. Trying again...")
                    continue

                # Compare transcription
                if self._normalize_text(transcription) == self._normalize_text(
                    TRANSCRIPTION_SENTENCE
                ):
                    break
                else:
                    logger.info(
                        f"ðŸ”´ Your transcription didn't match the expected sentence. Attempt {attempt}/{max_attempts}"
                    )
            else:
                raise Exception(
                    "ðŸ”´ Maximum attempts reached. Registration failed due to mismatched transcription."
                )

            # Capture the voice embedding.
            embedding = self._capture_voice_embedding(statement_audio)
            if not embedding:
                raise Exception("Failed to capture voice embedding.")

            # Save the voice embedding, passing first_name and last_name.
            self._save_voice_embedding(liu_id, embedding, first_name, last_name)
            logger.info(
                f"âœ… Registration complete for {first_name} {last_name} (LIU ID: {liu_id})."
            )
        except Exception as e:
            logger.exception("ðŸ”´ Registration failed.")
            logger.info(f"ðŸ”´ Registration failed: {e}")

    def _normalize_text(self, text: str) -> str:
        """Lowercase, remove punctuation, and normalize whitespace."""
        # return "".join(
        #     char for char in text.lower() if char not in string.punctuation
        # ).strip()
        return "".join(
            text.lower().translate(str.maketrans("", "", string.punctuation)).split()
        )

    def register_voice_for_user(
        self, first_name: str, last_name: str, liu_id: str, duration: int = 8
    ) -> None:
        """
        Record a voice statement for an already-registered user.
        This method does not prompt for personal details, instead it uses the provided first_name,
        last_name, and liu_id. It records audio, transcribes (if desired), captures the voice embedding,
        and updates the user record with the voice embedding.
        """
        try:
            # Construct the file path for the voice statement.
            voice_statement_audio = os.path.join(
                self.temp_audio_path, f"{liu_id}_voice.wav"
            )

            # Record a voice statement.
            self._record_audio(
                voice_statement_audio,
                "Please speak a short voice statement for registration:",
                duration=duration,
                sampling_rate=16000,  # or use your configured sampling_rate
            )

            # (Optional) Transcribe the audio and log the transcription.
            transcription = self._transcribe_audio(voice_statement_audio)
            logger.info("ðŸŸ¢ Voice transcription: %s", transcription)

            # Capture the voice embedding.
            embedding = self._capture_voice_embedding(voice_statement_audio)
            if not embedding:
                raise Exception("Failed to capture voice embedding.")

            # Save the voice embedding: update the database and save the pickle file.
            self._save_voice_embedding(liu_id, embedding, first_name, last_name)
            logger.info("âœ… Voice authentication register for: %s", liu_id)
        except Exception as e:
            logger.error("ðŸ”´ Voice registration for user failed: %s", e)
            raise

    def verify_user_by_voice(self, liu_id: str, audio_path: str) -> bool:
        """
        Verifies whether the voice in the provided audio matches the registered user's embedding.


        Args:
            liu_id (str): The user's LIU ID.
            audio_path (str): Path to the newly recorded voice sample.

        Returns:
            bool: True if match passes the threshold; False otherwise.
        """
        try:
            # Load stored embedding
            stored_path = os.path.join(self.voice_data_path, f"{liu_id}_voice.pkl")
            if not os.path.exists(stored_path):
                raise FileNotFoundError(
                    f"ðŸ”´ No stored voice data found for LIU ID: {liu_id}"
                )

            with open(stored_path, "rb") as f:
                stored_embedding = pickle.load(f)

            # Capture new embedding from input audio
            new_embedding = self._capture_voice_embedding(audio_path)

            # Compare using cosine similarity
            similarity = cosine_similarity([new_embedding], [stored_embedding])[0][0]
            logger.info(f"ðŸŸ¢ Voice similarity score: {similarity:.4f}")
            return similarity >= VOICE_MATCH_THRESHOLD
        except Exception as e:
            logger.error(f"ðŸ”´ Voice verification failed: {e}")
            return False

    def close(self):
        if self.cursor:
            self.cursor.close()

    def login_user(self) -> None:
        try:
            liu_id = input("Enter your LIU ID: ").strip()
            if not self._validate_liu_id(liu_id):
                raise Exception("Invalid LIU ID format.")

            # Record new audio for verification
            login_audio = os.path.join(self.temp_audio_path, f"{liu_id}_login.wav")
            self._record_audio(
                login_audio,
                f"ðŸ“£ Please read your verification sentence: {TRANSCRIPTION_SENTENCE}",
                duration=10,
            )

            if self.verify_user_by_voice(liu_id, login_audio):
                logger.info(f"âœ… Verification successful for {liu_id}. Welcome back!")
            else:
                logger.info(
                    f"âŒ Verification failed. The voice does not match our records."
                )
        except Exception as e:
            logger.exception("ðŸ”´ Login failed.")
            logger.error(f"ðŸ”´ Login failed: {e}")


if __name__ == "__main__":
    auth = VoiceAuth(TEMP_AUDIO_PATH, VOICE_DATA_PATH)
    # auth.register_user()
    # auth.close()

    print("Choose Action:")
    print("1. Register User")
    print("2. Login via Voice")

    choice = input("Enter 1 or 2: ").strip()

    if choice == "1":
        auth.register_user()
    elif choice == "2":
        auth.login_user()
    else:
        print("Invalid choice.")
#######################
# database/connection.py
"""
Database connection utilities for PostgreSQL.
This module provides functions to establish a connection to a PostgreSQL database,
ensuring that the target database exists before connecting. It uses environment
variables for configuration and supports logging.
Functions:
    get_connection():
        Establishes a connection to the PostgreSQL database specified by the
        DATABASE_URL environment variable. Ensures the database exists before
        connecting. Raises an EnvironmentError if DATABASE_URL is not set.
    ensure_database_exists(user, password, host, port, db_name):
        Checks if the specified database exists on the PostgreSQL server. If it
        does not exist, creates the database. Raises an exception if the check
        or creation fails.
"""


import logging
import os
from urllib.parse import urlparse

import psycopg2
from dotenv import load_dotenv
from psycopg2 import sql

from mini_project.config.app_config import setup_logging

load_dotenv()

# Logging
debug_mode = os.getenv("DEBUG", "0") in ["1", "true", "True"]
log_level = os.getenv("LOG_LEVEL", "DEBUG" if debug_mode else "INFO").upper()
setup_logging(level=getattr(logging, log_level))
logger = logging.getLogger("PgDBConnection")


def get_connection():
    db_url = os.getenv("DATABASE_URL")
    if not db_url:
        raise EnvironmentError("âŒ DATABASE_URL not found in .env")

    parsed = urlparse(db_url)
    db_name = parsed.path.lstrip("/")
    user = parsed.username
    password = parsed.password
    host = parsed.hostname
    port = parsed.port or 5432

    # âœ… Ensure the database exists
    ensure_database_exists(user, password, host, port, db_name)

    # âœ… Now safe to connect to it
    try:
        conn = psycopg2.connect(db_url)
        logger.info(f"âœ… Connected to database: {db_name}")
        return conn
    except Exception as e:
        logger.exception("âŒ Failed to connect to target database.")
        raise


def ensure_database_exists(user, password, host, port, db_name):
    try:
        logger.info(f"ðŸŸ¢ Checking if database '{db_name}' exists...")
        conn = psycopg2.connect(
            dbname="postgres",
            user=user,
            password=password,
            host=host,
            port=port,
        )
        conn.autocommit = True
        cur = conn.cursor()

        cur.execute("SELECT 1 FROM pg_database WHERE datname = %s", (db_name,))
        exists = cur.fetchone()
        if not exists:
            cur.execute(sql.SQL("CREATE DATABASE {}").format(sql.Identifier(db_name)))
            logger.info(f"âœ… Created missing database: {db_name}")
        else:
            logger.info(f"âœ… Database '{db_name}' already exists.")

        cur.close()
        conn.close()
    except Exception as e:
        logger.error(f"âŒ Could not check/create database '{db_name}': {e}")
        raise
#################################
# database/db_handler.py
""" database/db_handler.py
This module provides the `DatabaseHandler` class and CLI utilities for managing a PostgreSQL database
used in the mini_project application. It supports operations such as creating, dropping, clearing,
and populating tables, as well as backing up and restoring user profiles and the entire database.
Classes:
    DatabaseHandler:
        Handles PostgreSQL database operations, including:
            - Connecting to the database
            - Creating and dropping tables and indexes
            - Backing up and restoring user profiles (JSON)
            - Backing up the entire database (SQL dump)
            - Clearing and populating tables with sample data
            - Printing table status and row counts
            - Closing database connections
Functions:
    json_serializer(obj):
        Serializes datetime and memoryview objects for JSON output.
    main_cli():
        Command-line interface for managing the database. Supports arguments for:
            --clear: Truncate all tables and reset identities
            --drop: Drop all tables
            --create: Create all tables
            --populate: Populate tables with sample data
            --reset: Drop, create, and populate all tables (default if no args)
            --backup: Backup the database before making changes
            --status: Show table row counts and status
Environment:
    Requires a valid DATABASE_URL in the environment or .env file.
Logging:
    Uses the application's logging configuration for status and error reporting.
Dependencies:
    - psycopg2
    - dotenv
    - mini_project.config.app_config
    - mini_project.database.connection
    - mini_project.database.populate_db
    - mini_project.database.schema_sql
 """
import argparse
import binascii
import json
import logging
import os
import pickle
import subprocess
import sys
from datetime import datetime

import psycopg2
from mini_project.config.app_config import (
    DB_BACKUP_PATH,
    PROFILE_BACKUP_PATH,
    setup_logging,
)
from dotenv import load_dotenv
from psycopg2 import Error as Psycopg2Error

from mini_project.database import populate_db, schema_sql
from mini_project.database.connection import get_connection

# Load .env variables
load_dotenv()


# Initialize logging with desired level (optional)
setup_logging(level=logging.INFO)
logger = logging.getLogger("PgDBaseHandler")


def json_serializer(obj):
    if isinstance(obj, datetime):
        return obj.isoformat()
    if isinstance(obj, memoryview):
        return obj.tobytes().hex()  # Hex string for JSON-safe backup
    raise TypeError(f"Type {type(obj)} not serializable")


class DatabaseHandler:
    """
    A class to handle database operations using PostgreSQL.
    """

    def __init__(self):
        try:
            self.conn = get_connection()
            self.conn.autocommit = False
            self.cursor = self.conn.cursor()
        except Psycopg2Error as e:
            logger.error(f"Error connecting to PostgreSQL database: {e}")
            raise

    def backup_user_profiles(self, backup_dir=None):
        """Backs up all user profiles from the 'users' table into a JSON file."""

        # âœ… Check if 'users' table exists
        self.cursor.execute(
            """
            SELECT EXISTS (
                SELECT FROM information_schema.tables
                WHERE table_name = 'users'
            );
        """
        )
        exists = self.cursor.fetchone()[0]
        if not exists:
            logger.info(f"ðŸ”´ 'users' table does not exist. Skipping profile backup.")
            return

        # Proceed with backup if table exists
        backup_dir = PROFILE_BACKUP_PATH
        backup_dir.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_path = backup_dir / f"user_profile_backup_{timestamp}.json"

        self.cursor.execute("SELECT * FROM users")
        rows = self.cursor.fetchall()
        colnames = [desc[0] for desc in self.cursor.description]

        users = [dict(zip(colnames, row)) for row in rows]

        with open(backup_path, "w", encoding="utf-8") as f:
            json.dump(users, f, indent=2, default=json_serializer)

        logger.info(f"âœ… Backed up {len(users)} users to: {backup_path}")

    def restore_user_profiles(self, backup_dir=None, latest_only=True):
        """Restores user profiles from the most recent backup."""
        backup_dir = PROFILE_BACKUP_PATH
        if not backup_dir.exists():
            logger.info(f"âš ï¸ No backup folder found.")
            return

        backup_files = sorted(
            backup_dir.glob("user_profile_backup_*.json"), reverse=True
        )
        if not backup_files:
            logger.info(f"âš ï¸ No backup files found.")
            return

        backup_path = backup_files[0] if latest_only else backup_files[-1]
        with open(backup_path, "r", encoding="utf-8") as f:
            users = json.load(f)

        restored_users = []
        for user in users:
            # ðŸ” Decode binary fields
            if "face_encoding" in user and isinstance(user["face_encoding"], str):
                user["face_encoding"] = psycopg2.Binary(
                    binascii.unhexlify(user["face_encoding"])
                )

            if "voice_embedding" in user and isinstance(user["voice_embedding"], str):
                user["voice_embedding"] = psycopg2.Binary(
                    binascii.unhexlify(user["voice_embedding"])
                )

            # âŒ Drop user_id so PostgreSQL can auto-generate it
            user.pop("user_id", None)

            placeholders = ", ".join(["%s"] * len(user))
            columns = ", ".join(user.keys())
            values = list(user.values())
            # sql = f"INSERT INTO users ({columns}) VALUES ({placeholders}) ON CONFLICT (liu_id) DO NOTHING"
            sql = f"""
            INSERT INTO users ({columns}) VALUES ({placeholders})
            ON CONFLICT (liu_id) DO UPDATE SET
                face_encoding = EXCLUDED.face_encoding,
                voice_embedding = EXCLUDED.voice_embedding,
                preferences = EXCLUDED.preferences,
                profile_image_path = EXCLUDED.profile_image_path,
                interaction_memory = EXCLUDED.interaction_memory,
                last_updated = CURRENT_TIMESTAMP
            """
            self.cursor.execute(sql, values)
            restored_users.append(
                user["liu_id"]
            )  # âœ… Collect ID instead of logging every one
        self.conn.commit()
        # âœ… One neat log line:
        logger.info(
            f"âœ… Restored {len(restored_users)} user profile(s): {', '.join(restored_users)}"
        )

    def backup_database(self, backup_dir=DB_BACKUP_PATH):

        os.makedirs(backup_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%y%m%d_%H%M%S")
        backup_file = os.path.join(backup_dir, f"backup_{timestamp}.sql")

        db_url = os.getenv("DATABASE_URL")
        if not db_url:
            logger.error("DATABASE_URL not set in environment.")
            return

        try:
            logger.info(f"ðŸ—ƒï¸  Backing up database to {backup_file}...")
            subprocess.run(["pg_dump", db_url, "-f", backup_file], check=True)
            logger.info("âœ… Backup completed successfully.")
        except Exception as e:
            logger.error("Database backup failed: %s", e, exc_info=True)

    def print_status(self):
        logger.info("ðŸ§ª Checking database table status...")

        self.cursor.execute(
            """
            SELECT table_name
            FROM information_schema.tables
            WHERE table_schema = 'public'
            ORDER BY table_name;
        """
        )
        tables = [row[0] for row in self.cursor.fetchall()]

        for table in tables:
            self.cursor.execute(f"SELECT COUNT(*) FROM {table}")
            count = self.cursor.fetchone()[0]
            logger.info(f"ðŸ’¡ {table}: {count} rows")

    def create_tables(self):
        try:
            for create_sql in schema_sql.tables.values():
                self.cursor.execute(create_sql)
            self.conn.commit()
            logger.info("âœ… All tables created successfully.")
        except Psycopg2Error as e:
            logger.error(f"Error creating tables: {e}")
            self.conn.rollback()
            raise

    def create_indexes(self):
        try:
            for index in schema_sql.indexes:
                self.cursor.execute(index)
            self.conn.commit()
        except Psycopg2Error as e:
            logger.error(f"Error creating indexes: {e}")
            self.conn.rollback()
            raise

    def update_table_schemas(self):
        """
        Schema validation and dynamic alteration is not implemented for PostgreSQL.
        Consider using a migration tool like Alembic.
        """
        logger.info(
            "Skipping schema update. Use migrations for PostgreSQL schema changes."
        )

    def clear_tables(self):
        try:
            tables = list(schema_sql.tables.keys())
            truncate_query = (
                "TRUNCATE TABLE " + ", ".join(tables) + " RESTART IDENTITY CASCADE;"
            )
            self.cursor.execute(truncate_query)
            self.conn.commit()
            logger.info("âœ… All tables cleared successfully.")
        except Psycopg2Error as e:
            logger.error(f"Error clearing tables: {e}")
            self.conn.rollback()
            raise

    def drop_all_tables(self):
        try:

            self.cursor.execute(
                """
                DO $$ DECLARE
                    r RECORD;
                BEGIN
                    FOR r IN (SELECT tablename FROM pg_tables WHERE schemaname = 'public') LOOP
                        EXECUTE 'DROP TABLE IF EXISTS ' || quote_ident(r.tablename) || ' CASCADE';
                    END LOOP;
                END $$;
            """
            )
            self.conn.commit()
            logger.info("âœ… All tables dropped successfully.")
        except Psycopg2Error as e:
            logger.error(f"Error dropping tables: {e}")
            self.conn.rollback()
            raise

    def clear_camera_vision(self):
        try:
            # Delete all rows from the camera_vision table
            self.cursor.execute("DELETE FROM camera_vision;")
            self.conn.commit()
            logger.info("âœ… [DB] Cleared camera_vision table.")
        except Exception as e:
            logger.error(f"Error clearing camera_vision table: {e}")
            self.conn.rollback()

    def populate_database(self):
        try:
            self.clear_tables()

            populator = populate_db.DatabasePopulator(self.cursor)

            populator.populate_usd_data()
            populator.populate_users()
            populator.populate_sequence_library()
            populator.populate_operation_library()
            populator.populate_gesture_library()
            populator.populate_isaac_sim_gui()
            populator.populate_task_templates()
            populator.populate_skills()
            populator.populate_instructions()
            populator.populate_states()
            # populator.populate_operation_sequence()
            # populator.populate_sort_order()
            populator.populate_task_preferences()
            populator.populate_interaction_memory()
            populator.populate_simulation_results()

            populator.populate_manual_operations()

            self.conn.commit()
            logger.info("âœ… Database populated successfully.")
        except Psycopg2Error as e:
            logger.error(f"Database population failed: {e}")
            self.conn.rollback()

    def close(self):
        if self.cursor:
            self.cursor.close()
        if self.conn:
            self.conn.close()


def main_cli():
    parser = argparse.ArgumentParser(description="Manage PostgreSQL database.")

    parser.add_argument(
        "--clear", action="store_true", help="Truncate all tables and reset identities."
    )
    parser.add_argument("--drop", action="store_true", help="Drop all tables.")
    parser.add_argument("--create", action="store_true", help="Create all tables.")
    parser.add_argument(
        "--populate", action="store_true", help="Populate tables with sample data."
    )
    parser.add_argument(
        "--reset",
        action="store_true",
        help="Drop, create, and populate all tables.",
    )
    parser.add_argument(
        "--backup",
        action="store_true",
        help="Backup the database before making changes.",
    )

    parser.add_argument(
        "--status", action="store_true", help="Show table row counts and status."
    )

    args = parser.parse_args()

    # Show help if no arguments are passed
    if not any(vars(args).values()):
        parser.print_help()
        print("No arguments provided â€” defaulting to --reset.\n")
        args.reset = True

    try:
        db = DatabaseHandler()

        if args.reset:
            logger.info(
                f"ðŸ§  Resetting the database (backup, drop, create, populate)..."
            )
            db.backup_user_profiles()  # ðŸ” Backup BEFORE dropping
            db.backup_database()
            db.drop_all_tables()
            db.create_tables()
            db.create_indexes()
            db.populate_database()
            db.restore_user_profiles()  # â™»ï¸ Restore users after everything else
            db.print_status()
            print("\nðŸŽ‰ All Done! Database is healthy and ready.\n")

        else:
            if args.backup:
                print("Backing up database...")
                db.backup_database()
            if args.status:
                db.print_status()
                return
            if args.drop:
                print("Dropping all tables...")
                db.drop_all_tables()
            if args.create:
                print("Creating tables...")
                db.create_tables()
                db.create_indexes()
            if args.clear:
                print("Clearing tables...")
                db.clear_tables()
            if args.populate:
                print("Populating tables...")
                db.populate_database()

        db.close()

    except Exception as e:
        logger.exception("Unexpected error during CLI execution")
        print(f"âŒ Error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main_cli()
############################
# database/populate_db.py
""" Module for populating the database with initial or default data for various tables.
Classes:
    DatabasePopulator:
        A utility class to insert predefined data into the application's database tables.
        Methods:
            - populate_sequence_library(): Inserts default robot sequence definitions.
            - populate_gesture_library(): Inserts predefined gesture types and descriptions.
            - populate_usd_data(): Inserts USD (Universal Scene Description) asset metadata.
            - populate_operation_library(): Inserts operation/task definitions and triggers.
            - populate_users(): Inserts user profiles with roles and preferences.
            - populate_skills(): Inserts robot skill definitions and parameters.
            - populate_instructions(): Inserts instruction records for user interactions.
            - populate_states(): Inserts state definitions for robot tasks.
            - populate_operation_sequence(): Inserts mappings of operations to sequences and objects.
            - populate_sort_order(): Inserts object sorting preferences.
            - populate_task_templates(): Inserts task templates with default sequences.
            - populate_task_preferences(): Inserts user-specific task preferences.
            - populate_interaction_memory(): Inserts records of user interactions.
            - populate_simulation_results(): Inserts simulation result records.
            - populate_manual_operations(): Inserts operation parameters for manual tasks.
            - populate_isaac_sim_gui(): Inserts GUI feature states for the Isaac Sim interface.
Logging:
    Uses the configured logger to report success or errors during data population.
Usage:
    Instantiate DatabasePopulator with a database cursor and call the desired populate_* methods.
 """
import logging

from mini_project.config.app_config import setup_logging

setup_logging(level=logging.INFO)
logger = logging.getLogger("DBasePopulator")


class DatabasePopulator:
    def __init__(self, cursor):
        self.cursor = cursor

    def populate_sequence_library(self):
        """
        Populate sequence_library table with provided data.
        """
        sequence_library = [
            (
                "pick",
                "PickBlockRd",
                "Pick up an object",
                "gripper is clear",
                "object in gripper",
                1,
                "aaa",
                False,  # Boolean value for is_runnable_exit
            ),
            (
                "travel",
                "ReachToPlacementRd",
                "Move to the target location",
                "object in gripper",
                "at target location",
                1,
                "aaa",
                False,
            ),
            (
                "drop",
                "DropRd",
                "Drop the object",
                "at target location",
                "object dropped",
                1,
                "aaa",
                False,
            ),
            (
                "screw",
                "ScrewRd",
                "Screw the object two times",
                "task complete",
                "robot at home position",
                1,
                "thresh_met and self.context.gripper_has_block",
                True,
            ),
            (
                "rotate",
                "RotateRd",
                "Rotate the object once",
                "task complete",
                "robot at home position",
                1,
                "thresh_met and self.context.gripper_has_block",
                True,
            ),
            (
                "go_home",
                "GoHome",
                "Return to the home position",
                "task complete",
                "robot at home position",
                1,
                "aaa",
                False,
            ),
        ]
        insert_query = """
            INSERT INTO sequence_library
            (sequence_name, node_name, description, conditions, post_conditions, is_runnable_count, is_runnable_condition, is_runnable_exit)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s);
        """
        try:
            self.cursor.executemany(insert_query, sequence_library)
            logger.info(
                "âœ… Successfully populated the sequence_library table with data!"
            )
        except Exception as e:
            logger.error(f"âŒ Error inserting sequence_library: {e}")
            self.cursor.connection.rollback()

    def populate_gesture_library(self):
        """
        Populate gesture_library table with predefined gesture data.
        """
        gesture_data = [
            (
                "thumbs_up",
                "Approval",
                "The thumb is raised above the index finger.",
                '{"threshold": 0.0}',
            ),
            (
                "open_hand",
                "Stop",
                "All fingers are extended, signaling stop.",
                '{"threshold": 0.0}',
            ),
            (
                "pointing",
                "Select Object",
                "The index finger is extended while other fingers are curled.",
                '{"threshold": 0.0}',
            ),
            (
                "closed_fist",
                "Grab",
                "The hand is clenched into a fist.",
                '{"threshold": 0.0}',
            ),
            (
                "victory",
                "Confirm",
                "The hand forms a V-shape with the index and middle fingers extended.",
                '{"threshold": 0.0}',
            ),
            (
                "ok_sign",
                "OK",
                "The thumb and index finger are touching to form a circle.",
                '{"threshold": 0.05}',
            ),
        ]

        insert_query = """
            INSERT INTO gesture_library (gesture_type, gesture_text, natural_description, config)
            VALUES (%s, %s, %s, %s);
        """
        try:
            self.cursor.executemany(insert_query, gesture_data)
            logger.info(
                "âœ… Successfully populated the gesture_library table with data!"
            )
        except Exception as e:
            logger.error(f"âŒ Error inserting gesture data: {e}")
            self.cursor.connection.rollback()

    def populate_usd_data(self):
        """
        Populate usd_data table with provided data.
        """
        usd_data = [
            (
                1,
                "Fixture.usd",
                "GeometryPrim",
                "/fixture_description/Slide_Fixture.usd",
                0.0,
                0.0,
                0.1,
                0.1,
                0.1,
                "/World/fixtureprim",
                0.20,
                -0.07,
                0.094,
                False,
            ),
            (
                2,
                "Slide_Holder.usd",
                "GeometryPrim",
                "/fixture_description/Slide_Holder.usd",
                0.0,
                0.0,
                0.1,
                0.1,
                0.1,
                "/World/fixtureprim",
                40,
                17,
                8,
                False,
            ),
            (
                3,
                "Slide.usd",
                "RigidPrim",
                "/fixture_description/Slide1.usd",
                0.002,
                0.016,
                1,
                1,
                0.06,
                "/World/fixtureprim/Fixture",
                0.0,
                0.0,
                0.0,
                True,
            ),
            (
                4,
                "Cuboid.usd",
                "RigidPrim",
                "/yumi_basic_shapes/cuboid.usd",
                0.025,
                0.015,
                0.1,
                0.11,
                0.1,
                "/World/fixtureprim",
                0.55475,
                -0.116,
                0.113,
                True,
            ),
            (
                5,
                "Cylinder.usd",
                "RigidPrim",
                "/yumi_basic_shapes/cylinder.usd",
                0.025,
                0.015,
                0.1,
                0.1,
                0.1,
                "/World/fixtureprim",
                0.41475,
                -0.116,
                0.113,
                True,
            ),
            (
                6,
                "Cube.usd",
                "RigidPrim",
                "/yumi_basic_shapes/cube.usd",
                0.025,
                0.015,
                0.1,
                0.1,
                0.1,
                "/World/fixtureprim",
                0.34475,
                -0.116,
                0.113,
                True,
            ),
            (
                7,
                "Hexagon.usd",
                "RigidPrim",
                "/yumi_basic_shapes/hexagon.usd",
                0.025,
                0.015,
                0.1,
                0.1,
                0.1,
                "/World/fixtureprim",
                0.48475,
                -0.116,
                0.113,
                True,
            ),
        ]
        insert_query = """
            INSERT INTO usd_data (
                    sequence_id, usd_name, type_of_usd, repository,block_height,block_pick_height, scale_x, scale_y, scale_z,
                    prim_path, initial_pos_x, initial_pos_y, initial_pos_z, register_obstacle
                )
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        """
        try:
            self.cursor.executemany(insert_query, usd_data)
            logger.info("âœ… Successfully populated the usd_data table with data!")
        except Exception as e:
            logger.error(f"âŒ Error inserting usd_data: {e}")
            self.cursor.connection.rollback()

    def populate_operation_library(self):
        self.cursor.execute("SELECT COUNT(*) FROM operation_library")
        count = self.cursor.fetchone()[0]
        if count > 0:
            return
        operation_library = [
            (
                "slide_sorting",  # operation_name	Internal task name
                "pick, travel, drop",  # List of phases like pick/travel/drop
                "Sort slides by shape and color into trays",  # Human-readable explanation
                [
                    "sort",
                    "slides",
                    "slide",
                    "sorting",
                    "tray",
                    "sort slides",
                ],  # trigger_keywords	Voice-trigger words
                "detect_slides_pgSQL.py",  # Path to run on vision system
                True,  # is_triggerable, Whether LLM can set trigger = TRUE
                False,  # trigger,  Defaults to FALSE (used as flag)
                "idle",  # state, 'idle' initially
                None,  # last_triggered_time, None until first run
            ),
            (
                "shape_stacking",
                "pick, travel, drop",
                "Stack blocks of shapes based on their type and color",
                ["stack", "stacking", "shapes", "shape", "shape stacking"],
                "detect_shapes_pgSQL.py",
                True,
                False,
                "idle",
                None,
            ),
        ]
        insert_query = """
            INSERT INTO operation_library (
                operation_name, task_order, description, trigger_keywords,
                script_path, is_triggerable, trigger, state, last_triggered
            )
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s);
        """
        try:
            self.cursor.executemany(insert_query, operation_library)
            logger.info(
                "âœ… Successfully populated the operation_library table with data!"
            )
        except Exception as e:
            logger.error(f"âŒ Error inserting operation_library: {e}")
            self.cursor.connection.rollback()

    def populate_users(self):
        self.cursor.execute("SELECT COUNT(*) FROM users")
        count = self.cursor.fetchone()[0]
        if count > 0:
            return
        users = [
            (
                "Yumi",
                "Robot",
                "yumi100",
                "yumi100@lab.liu.ai",
                '{"likes": ["AI", "Robotics"]}',
                "/images/yumi001.jpg",
                '{"last_task": "Assistance", "successful_tasks": 100}',
                "robot",
            ),
            (
                "Oscar",
                "Ikechukwu",
                "oscik559",
                "oscik559@student.liu.se",
                '{"likes": ["AI", "Robotics"]}',
                "/images/oscik559.jpg",
                '{"last_task": "Pick object", "successful_tasks": 5}',
                "admin",
            ),
            (
                "Rahul",
                "Chiramel",
                "rahch515",
                "rahch515@student.liu.se",
                '{"likes": ["Aeroplanes", "Automation"]}',
                "/images/rahch515.jpg",
                '{"last_task": "Screw object", "successful_tasks": 10}',
                "admin",
            ),
            (
                "Sanjay",
                "Nambiar",
                "sanna58",
                "sanjay.nambiar@liu.se",
                '{"likes": ["Programming", "Machine Learning"]}',
                "/images/sanna58.jpg",
                '{"last_task": "Slide object", "successful_tasks": 7}',
                "admin",
            ),
            (
                "Mehdi",
                "Tarkian",
                "mehta77",
                "mehdi.tarkian@liu.se",
                '{"likes": ["Running", "Cats"]}',
                "/images/mehta77.jpg",
                '{"last_task": "Drop object", "successful_tasks": 2}',
                "team",
            ),
            (
                "Marie",
                "Jonsson",
                "marjo33",
                "marie.s.jonsson@liu.se",
                '{"likes": ["Robots", "Composites"]}',
                "/images/marjo33.jpg",
                '{"last_task": "Fix robot battery", "successful_tasks": 2}',
                "team",
            ),
            (
                "Aref",
                "Aghaee",
                "areag806",
                "areag806@student.liu.se",
                '{"likes": ["CATIA", "Turbine Blades"]}',
                "/images/areag806.jpg",
                '{"last_task": "Pick object", "successful_tasks": 5}',
                "guest",
            ),
            (
                "Thomson",
                "Kalliyath",
                "thoka981",
                "thoka981@student.liu.se",
                '{"likes": ["Omniverse", "Aeronautics"]}',
                "/images/thoka981.jpg",
                '{"last_task": "Pick object", "successful_tasks": 5}',
                "guest",
            ),
            (
                "Hamideh",
                "Pourrasoul",
                "hampo845",
                "hampo845@student.liu.se",
                '{"likes": ["CATIA", "Turbine Blades"]}',
                "/images/hampo845.jpg",
                '{"last_task": "Pick object", "successful_tasks": 5}',
                "guest",
            ),
            (
                "John",
                "Ashish",
                "johas759",
                "johas759@student.liu.se",
                '{"likes": ["python", "aircraft wings"]}',
                "/images/johas759.jpg",
                '{"last_task": "Pick object", "successful_tasks": 5}',
                "guest",
            ),
            (
                "Danial",
                "Nikpey",
                "danni741",
                "danni741@student.liu.se",
                '{"likes": ["vb.net", "aircraft wings"]}',
                "/images/danni741.jpg",
                '{"last_task": "Pick object", "successful_tasks": 5}',
                "guest",
            ),
        ]
        insert_query = """
            INSERT INTO users (first_name, last_name, liu_id, email, preferences, profile_image_path, interaction_memory, role)
            VALUES (%s, %s, %s, %s, %s,%s,%s, %s);
        """
        try:
            self.cursor.executemany(insert_query, users)
            logger.info("âœ… Successfully populated the users table with data!")
        except Exception as e:
            logger.error(f"âŒ Error inserting users: {e}")
            self.cursor.connection.rollback()

    def populate_skills(self):
        skills = [
            (
                "pick",
                "Pick up object",
                '{"gripper_force": 0.5}',
                '{"gripper": true}',
                2.5,
            ),
            ("place", "Place object", '{"precision": 0.01}', '{"vision": true}', 3.0),
        ]
        insert_query = """
            INSERT INTO skills (skill_name, description, parameters, required_capabilities, average_duration)
            VALUES (%s, %s, %s, %s, %s);
        """
        self.cursor.executemany(insert_query, skills)

    def populate_instructions(self):
        """
        Populate instructions table with provided data.
        This ensures that foreign key references in child tables (e.g., interaction_memory, simulation_results)
        will find matching instruction rows.
        """
        instructions = [
            (1, "voice", "en", "command", False, "Pick up object", None, 0.95),
            (2, "text", "en", "command", False, "Place object", None, 0.90),
        ]
        insert_query = """
            INSERT INTO instructions (user_id, modality, language, instruction_type, processed, content, sync_id, confidence)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s);
        """
        self.cursor.executemany(insert_query, instructions)

    def populate_states(self):
        states = [
            (
                "LiftState",
                "Lift an object vertically",
                "gripper is clear",
                "object in gripper",
                1,
            ),
            (
                "SlideState",
                "Slide an object along X-axis",
                "object in gripper",
                "at target location",
                1,
            ),
        ]
        insert_query = """
            INSERT INTO states (task_name, description, conditions, post_conditions, sequence_id)
            VALUES (%s, %s, %s, %s, %s);
        """
        self.cursor.executemany(insert_query, states)

    def populate_operation_sequence(self):
        operation_sequence = [
            (1, 1, "pick", "Slide_1"),
            (2, 2, "travel", "Slide_1"),
            (3, 3, "drop", "Slide_1"),
            (4, 1, "pick", "Slide_2"),
            (5, 2, "travel", "Slide_2"),
            (6, 3, "drop", "Slide_2"),
            (7, 1, "pick", "Slide_3"),
            (8, 2, "travel", "Slide_3"),
            (9, 3, "drop", "Slide_3"),
            (10, 6, "go_home", ""),
        ]

        insert_query = """
            INSERT INTO operation_sequence (
                operation_id, sequence_id, sequence_name, object_name
            ) VALUES (%s, %s, %s, %s)
        """
        self.cursor.executemany(insert_query, operation_sequence)

    def populate_sort_order(self):
        sort_order = [
            ("Slide_1", "Green"),
            ("Slide_2", "Orange"),
            ("Slide_3", "Pink"),
        ]
        insert_query = """
            INSERT INTO sort_order (object_name, object_color)
            VALUES (%s, %s);
        """
        self.cursor.executemany(insert_query, sort_order)

    def populate_task_templates(self):
        task_templates = [
            (
                "sort",
                "Default sorting task: pick, move, drop",
                ["pick", "travel", "drop"],
            ),
            (
                "assemble",
                "Assembly involves pick and screw",
                ["pick", "travel", "screw", "go_home"],
            ),
            (
                "inspect",
                "Inspect task involves scan and return",
                ["travel", "inspect", "go_home"],
            ),
            (
                "cleanup",
                "Cleanup task involves pick, rotate, drop",
                ["pick", "rotate", "drop"],
            ),
        ]
        insert_query = """
            INSERT INTO task_templates (task_name, description, default_sequence)
            VALUES (%s, %s, %s);
        """
        self.cursor.executemany(insert_query, task_templates)

    def populate_task_preferences(self):
        task_preferences = [
            (1, "Pick Object", '{"time": "morning", "location": "shelf"}'),
            (2, "Place Object", '{"time": "afternoon", "location": "table"}'),
            (1, "Inspect Object", '{"tools": ["camera", "gripper"]}'),
        ]
        insert_query = """
            INSERT INTO task_preferences (user_id, task_name, preference_data)
            VALUES (%s, %s, %s);
        """
        self.cursor.executemany(insert_query, task_preferences)

    def populate_interaction_memory(self):
        interactions = [
            (
                1,
                1,
                "task_query",
                '{"task": "Pick Object"}',
                "2023-10-01 09:00:00",
                "2023-10-01 17:00:00",
            ),
            (
                2,
                1,
                "preference_update",
                '{"preference": {"time": "morning"}}',
                "2023-10-01 09:00:00",
                "2023-10-01 17:00:00",
            ),
            (
                1,
                2,
                "task_execution",
                '{"status": "success", "task": "Place Object"}',
                "2023-10-02 09:00:00",
                "2023-10-02 17:00:00",
            ),
        ]
        insert_query = """
            INSERT INTO interaction_memory (user_id, instruction_id, interaction_type, data, start_time, end_time)
            VALUES (%s, %s, %s, %s, %s, %s);
        """
        self.cursor.executemany(insert_query, interactions)

    def populate_simulation_results(self):
        results = [
            (1, True, '{"accuracy": 0.95, "time_taken": 2.5}', "No errors"),
            (2, False, '{"accuracy": 0.8, "time_taken": 3.0}', "Gripper misalignment"),
        ]
        insert_query = """
            INSERT INTO simulation_results (instruction_id, success, metrics, error_log)
            VALUES (%s, %s, %s, %s);
        """
        self.cursor.executemany(insert_query, results)

    def populate_manual_operations(self):
        self.cursor.execute(
            "SELECT COUNT(*) FROM camera_vision WHERE usd_name = 'Slide.usd'"
        )
        slide_usd_count = self.cursor.fetchone()[0]
        if slide_usd_count > 0:

            # -- Screw Operation Parameters
            self.cursor.execute(
                "SELECT sequence_id, object_name FROM operation_sequence WHERE sequence_name = 'screw'"
            )
            screw_data = self.cursor.fetchall()
            screw_op_parameters = [
                (i + 1, seq_id, obj_name, i % 2 == 0, 3, 0, False)
                for i, (seq_id, obj_name) in enumerate(screw_data)
            ]
            self.cursor.executemany(
                """
                INSERT INTO screw_op_parameters (
                    operation_order, sequence_id, object_id,
                    rotation_dir, number_of_rotations,
                    current_rotation, operation_status
                )
                VALUES (%s, %s, %s, %s, %s, %s, %s)
                """,
                screw_op_parameters,
            )

            # -- Rotate State Parameters
            self.cursor.execute(
                "SELECT sequence_id, operation_order, object_id FROM screw_op_parameters"
            )
            rotate_data = self.cursor.fetchall()
            rotate_state_parameters = [
                (seq_id, operation_order, obj_id, 90, False)
                for (seq_id, operation_order, obj_id) in rotate_data
            ]
            self.cursor.executemany(
                """
                INSERT INTO rotate_state_parameters (
                    sequence_id, operation_order, object_id,
                    rotation_angle, operation_status
                )
                VALUES (%s, %s, %s, %s, %s)
                """,
                rotate_state_parameters,
            )

            # -- Pick Operation Parameters
            self.cursor.execute(
                "SELECT sequence_id, object_name FROM operation_sequence WHERE sequence_name = 'pick'"
            )
            pick_data = self.cursor.fetchall()
            pick_op_parameters = [
                (i + 1, obj_name, False, "y", 0.01, False)
                for i, (seq_id, obj_name) in enumerate(pick_data)
            ]
            self.cursor.executemany(
                """
                INSERT INTO pick_op_parameters (
                    operation_order, object_id, slide_state_status, slide_direction, distance_travel, operation_status
                )
                VALUES (%s, %s, %s, %s, %s, %s)

                """,
                pick_op_parameters,
            )

            # -- Travel Operation Parameters
            self.cursor.execute(
                "SELECT sequence_id, object_name FROM operation_sequence WHERE sequence_name = 'travel'"
            )
            travel_data = self.cursor.fetchall()
            travel_op_parameters = [
                (i + 1, obj_name, 0.085, "y-axis", False)
                for i, (_, obj_name) in enumerate(travel_data)
            ]

            self.cursor.executemany(
                """
                INSERT INTO travel_op_parameters (
                    operation_order, object_id, travel_height, gripper_rotation, operation_status
                )
                VALUES (%s, %s, %s, %s, %s)
                """,
                travel_op_parameters,
            )

            # -- Drop Operation Parameters
            self.cursor.execute(
                "SELECT sequence_id, object_name FROM operation_sequence WHERE sequence_name = 'drop'"
            )
            drop_data = self.cursor.fetchall()

            drop_op_parameters = [
                (i + 1, obj_name, -0.003, False)
                for i, (_, obj_name) in enumerate(drop_data)
            ]

            self.cursor.executemany(
                """
                INSERT INTO drop_op_parameters (
                    operation_order, object_id, drop_height, operation_status
                )
                VALUES (%s, %s, %s, %s)
                """,
                drop_op_parameters,
            )
        else:
            self.cursor.execute(
                "SELECT sequence_id, object_name FROM operation_sequence WHERE sequence_name = 'screw'"
            )
            screw_data = self.cursor.fetchall()
            screw_op_parameters = [
                (i + 1, seq_id, obj_name, i % 2 == 0, 3, 0, False)
                for i, (seq_id, obj_name) in enumerate(screw_data)
            ]
            self.cursor.executemany(
                """
                INSERT INTO screw_op_parameters (
                    operation_order, sequence_id, object_id,
                    rotation_dir, number_of_rotations,
                    current_rotation, operation_status
                )
                VALUES (%s, %s, %s, %s, %s, %s, %s)
                """,
                screw_op_parameters,
            )

            # -- Rotate State Parameters
            self.cursor.execute(
                "SELECT sequence_id, operation_order, object_id FROM screw_op_parameters"
            )
            rotate_data = self.cursor.fetchall()
            rotate_state_parameters = [
                (seq_id, operation_order, obj_id, 90, False)
                for (seq_id, operation_order, obj_id) in rotate_data
            ]
            self.cursor.executemany(
                """
                INSERT INTO rotate_state_parameters (
                    sequence_id, operation_order, object_id,
                    rotation_angle, operation_status
                )
                VALUES (%s, %s, %s, %s, %s)
                """,
                rotate_state_parameters,
            )

            # -- Pick Operation Parameters
            self.cursor.execute(
                "SELECT sequence_id, object_name FROM operation_sequence WHERE sequence_name = 'pick'"
            )
            pick_data = self.cursor.fetchall()
            pick_op_parameters = [
                (i + 1, obj_name, False, "y", 0.01, False)
                for i, (seq_id, obj_name) in enumerate(pick_data)
            ]
            self.cursor.executemany(
                """
                INSERT INTO pick_op_parameters (
                    operation_order, object_id, slide_state_status, slide_direction, distance_travel, operation_status
                )
                VALUES (%s, %s, %s, %s, %s, %s)

                """,
                pick_op_parameters,
            )

            # -- Travel Operation Parameters
            self.cursor.execute(
                "SELECT sequence_id, object_name FROM operation_sequence WHERE sequence_name = 'travel'"
            )
            travel_data = self.cursor.fetchall()
            travel_op_parameters = [
                (i + 1, obj_name, 0.085, "z-axis", False)
                for i, (_, obj_name) in enumerate(travel_data)
            ]

            self.cursor.executemany(
                """
                INSERT INTO travel_op_parameters (
                    operation_order, object_id, travel_height, gripper_rotation, operation_status
                )
                VALUES (%s, %s, %s, %s, %s)
                """,
                travel_op_parameters,
            )

            # -- Drop Operation Parameters
            self.cursor.execute(
                "SELECT sequence_id, object_name FROM operation_sequence WHERE sequence_name = 'drop'"
            )
            drop_data = self.cursor.fetchall()

            drop_op_parameters = [
                (i + 1, obj_name, -0.003, False)
                for i, (_, obj_name) in enumerate(drop_data)
            ]

            self.cursor.executemany(
                """
                INSERT INTO drop_op_parameters (
                    operation_order, object_id, drop_height, operation_status
                )
                VALUES (%s, %s, %s, %s)
                """,
                drop_op_parameters,
            )

    def populate_isaac_sim_gui(self):
        isaac_sim_gui = [("Start", False), ("Reset", False), ("Load", False)]
        insert_query = """
            INSERT INTO isaac_sim_gui (gui_feature, operation_status)
            VALUES (%s, %s);
        """
        try:
            self.cursor.executemany(insert_query, isaac_sim_gui)
            logger.info("âœ… Successfully populated the isaac_sim_gui table with data!")
        except Exception as e:
            logger.error(f"âŒ Error inserting isaac_sim_gui: {e}")
            self.cursor.connection.rollback()
##########################
# database/schema_sql.py
""" This module defines the SQL schema for the application's PostgreSQL database using an OrderedDict
to store table creation statements and a list for index creation statements.
Attributes:
    tables (OrderedDict):
        An ordered mapping of table names to their corresponding SQL CREATE TABLE statements.
        Each entry defines the structure, constraints, and relationships for a specific table
        in the database, including user management, operation parameters, instructions,
        simulation results, and more.
    indexes (list of str):
        A list of SQL CREATE INDEX statements to optimize query performance on frequently
        accessed columns and relationships.
Usage:
    The `tables` and `indexes` can be iterated over to initialize or migrate the database schema.
    This module is intended to be imported and used by database setup or migration scripts.


 """
from collections import OrderedDict

tables = OrderedDict(
    [
        (
            "users",
            """
            CREATE TABLE IF NOT EXISTS users (
                user_id SERIAL PRIMARY KEY,
                first_name TEXT NOT NULL,
                last_name TEXT NOT NULL,
                liu_id TEXT UNIQUE,
                email TEXT UNIQUE,
                preferences TEXT,
                profile_image_path TEXT,
                interaction_memory TEXT,
                face_encoding BYTEA,
                voice_embedding BYTEA,
                role TEXT CHECK(role IN ('robot','team', 'guest', 'admin')) DEFAULT 'guest',
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );
    """,
        ),
        (
            "usd_data",
            """
            CREATE TABLE IF NOT EXISTS usd_data (
                sequence_id INTEGER PRIMARY KEY,
                usd_name TEXT NOT NULL,
                type_of_usd TEXT NOT NULL,
                repository TEXT NOT NULL,
                block_height FLOAT NOT NULL,
                block_pick_height FLOAT NOT NULL,
                scale_x FLOAT NOT NULL,
                scale_y FLOAT NOT NULL,
                scale_z FLOAT NOT NULL,
                prim_path TEXT NOT NULL,
                initial_pos_x FLOAT NOT NULL,
                initial_pos_y FLOAT NOT NULL,
                initial_pos_z FLOAT NOT NULL,
                register_obstacle BOOLEAN NOT NULL
            );
    """,
        ),
        (
            "isaac_sim_gui",
            """
            CREATE TABLE IF NOT EXISTS isaac_sim_gui (
                sequence_id SERIAL PRIMARY KEY,
                gui_feature TEXT NOT NULL,
                operation_status BOOLEAN DEFAULT FALSE
            );
    """,
        ),
        (
            "sequence_library",
            """
            CREATE TABLE IF NOT EXISTS sequence_library (
                sequence_id SERIAL PRIMARY KEY,
                sequence_name TEXT NOT NULL,
                skill_name TEXT,
                node_name TEXT,
                description TEXT,
                conditions TEXT,
                post_conditions TEXT,
                is_runnable_count INTEGER,
                is_runnable_condition TEXT,
                is_runnable_exit BOOLEAN
            );
    """,
        ),
        (
            "operation_library",
            """
            CREATE TABLE IF NOT EXISTS operation_library (
                id SERIAL PRIMARY KEY,

                -- Core operation metadata
                operation_name TEXT UNIQUE NOT NULL,       -- e.g., 'tray_holder_detection'
                task_order TEXT,                           -- e.g., 'detect, pick, place'
                description TEXT,                          -- Human-readable label

                -- Script & trigger metadata
                trigger_keywords TEXT[],                   -- Words that trigger this operation
                script_path TEXT,                          -- e.g., 'camera_vision_pgSQL_rs.py'
                is_triggerable BOOLEAN DEFAULT TRUE,       -- Can be triggered from LLM

                -- Trigger state tracking
                trigger BOOLEAN DEFAULT FALSE,             -- Used by LLM to trigger script
                state TEXT DEFAULT 'idle',                -- idle | triggered | running
                last_triggered TIMESTAMP                   -- When it was last set to TRUE
            );
    """,
        ),
        (
            "access_logs",
            """
            CREATE TABLE IF NOT EXISTS access_logs (
                log_id INTEGER PRIMARY KEY,
                user_id INTEGER NOT NULL,
                action_type TEXT NOT NULL,
                target_table TEXT,
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (user_id) REFERENCES users(user_id)
            );
    """,
        ),
        (
            "skills",
            """
            CREATE TABLE IF NOT EXISTS skills (
                skill_id SERIAL PRIMARY KEY,
                skill_name TEXT NOT NULL UNIQUE,
                description TEXT,
                parameters TEXT,
                required_capabilities TEXT,
                average_duration REAL
            );
    """,
        ),
        (
            "instructions",
            """
            CREATE TABLE IF NOT EXISTS instructions (
                id INTEGER GENERATED ALWAYS AS IDENTITY PRIMARY KEY,
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                user_id INTEGER,
                modality TEXT CHECK(modality IN ('voice', 'gesture', 'text')),
                language TEXT NOT NULL,
                instruction_type TEXT NOT NULL,
                processed BOOLEAN DEFAULT FALSE,
                content TEXT,
                sync_id INTEGER UNIQUE,
                confidence REAL CHECK(confidence BETWEEN 0 AND 1),
                FOREIGN KEY (user_id) REFERENCES users(user_id) ON DELETE CASCADE
            );
    """,
        ),
        (
            "states",
            """
            CREATE TABLE IF NOT EXISTS states (
                task_id SERIAL PRIMARY KEY,
                task_name TEXT NOT NULL,
                description TEXT,
                conditions TEXT,
                post_conditions TEXT,
                sequence_id INTEGER,
                FOREIGN KEY (sequence_id) REFERENCES sequence_library(sequence_id) ON DELETE CASCADE
            );
    """,
        ),
        (
            "screw_op_parameters",
            """
            CREATE TABLE IF NOT EXISTS screw_op_parameters (
                sequence_id SERIAL PRIMARY KEY,
                operation_order INTEGER NOT NULL,
                object_id TEXT NOT NULL,
                rotation_dir BOOLEAN NOT NULL,
                number_of_rotations INTEGER NOT NULL,
                current_rotation INTEGER NOT NULL,
                operation_status BOOLEAN NOT NULL
            );
    """,
        ),
        (
            "pick_op_parameters",
            """
            CREATE TABLE IF NOT EXISTS pick_op_parameters (
                sequence_id SERIAL PRIMARY KEY,
                operation_order INTEGER NOT NULL,
                object_id TEXT NOT NULL,
                slide_state_status BOOLEAN NOT NULL,
                slide_direction TEXT NOT NULL,
                distance_travel FLOAT NOT NULL,
                operation_status BOOLEAN NOT NULL
            );
    """,
        ),
        (
            "drop_op_parameters",
            """
                CREATE TABLE IF NOT EXISTS drop_op_parameters (
                sequence_id SERIAL PRIMARY KEY,
                operation_order INTEGER NOT NULL,
                object_id TEXT NOT NULL,

                drop_height FLOAT NOT NULL,

                -- âœ… New columns for specifying drop location
                drop_pos_x FLOAT DEFAULT NULL,
                drop_pos_y FLOAT DEFAULT NULL,
                drop_pos_z FLOAT DEFAULT NULL,

                operation_status BOOLEAN NOT NULL
            );
    """,
        ),
        (
            "travel_op_parameters",
            """
            CREATE TABLE IF NOT EXISTS travel_op_parameters (
                sequence_id SERIAL PRIMARY KEY,
                operation_order INTEGER NOT NULL,
                object_id TEXT NOT NULL,
                travel_height FLOAT NOT NULL,
                gripper_rotation TEXT NOT NULL,
                operation_status BOOLEAN NOT NULL
            );
    """,
        ),
        (
            "lift_state_parameters",
            """
            CREATE TABLE IF NOT EXISTS lift_state_parameters (
                sequence_id SERIAL PRIMARY KEY,
                operation_order INTEGER NOT NULL,
                object_id TEXT NOT NULL,
                lift_height FLOAT NOT NULL,
                operation_status BOOLEAN NOT NULL
            );
    """,
        ),
        (
            "slide_state_parameters",
            """
            CREATE TABLE IF NOT EXISTS slide_state_parameters (
                sequence_id SERIAL PRIMARY KEY,
                operation_order INTEGER NOT NULL,
                object_id TEXT NOT NULL,
                lift_distance FLOAT NOT NULL,
                pos_x FLOAT NOT NULL,
                pos_y FLOAT NOT NULL,
                pos_z FLOAT NOT NULL,
                operation_status BOOLEAN NOT NULL
            );
    """,
        ),
        (
            "rotate_state_parameters",
            """
            CREATE TABLE IF NOT EXISTS rotate_state_parameters (
                sequence_id SERIAL PRIMARY KEY,
                operation_order INTEGER NOT NULL,
                object_id TEXT NOT NULL,
                rotation_angle FLOAT NOT NULL,
                operation_status BOOLEAN NOT NULL
            );
    """,
        ),
        (
            "camera_vision",
            """
            CREATE TABLE IF NOT EXISTS camera_vision (
                object_id SERIAL PRIMARY KEY,
                object_name TEXT NOT NULL,
                object_color TEXT NOT NULL,
                color_code FLOAT8[],
                pos_x DOUBLE PRECISION NOT NULL,
                pos_y DOUBLE PRECISION NOT NULL,
                pos_z DOUBLE PRECISION NOT NULL,
                rot_x DOUBLE PRECISION NOT NULL,
                rot_y DOUBLE PRECISION NOT NULL,
                rot_z DOUBLE PRECISION NOT NULL,
                usd_name TEXT NOT NULL,
                last_detected TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );
    """,
        ),
        (
            "task_templates",
            """
            CREATE TABLE IF NOT EXISTS task_templates (
                task_id SERIAL PRIMARY KEY,
                task_name TEXT UNIQUE NOT NULL,
                description TEXT,
                default_sequence TEXT[]
            );
    """,
        ),
        (
            "sort_order",
            """
            CREATE TABLE IF NOT EXISTS sort_order (
                order_id SERIAL PRIMARY KEY,
                object_name TEXT,
                object_color TEXT
            );
    """,
        ),
        (
            "interaction_memory",
            """
            CREATE TABLE IF NOT EXISTS interaction_memory (
                interaction_id SERIAL PRIMARY KEY,
                user_id INTEGER,
                instruction_id INTEGER,
                interaction_type TEXT,
                data TEXT,
                start_time TIMESTAMP,
                end_time TIMESTAMP,
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (user_id) REFERENCES users(user_id) ON DELETE CASCADE,
                FOREIGN KEY (instruction_id) REFERENCES instructions(id) ON DELETE CASCADE
            );
    """,
        ),
        (
            "simulation_results",
            """
            CREATE TABLE IF NOT EXISTS simulation_results (
                simulation_id SERIAL PRIMARY KEY,
                instruction_id INTEGER,
                success BOOLEAN,
                metrics TEXT,
                error_log TEXT,
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (instruction_id) REFERENCES instructions(id) ON DELETE CASCADE
            );
    """,
        ),
        (
            "task_preferences",
            """
            CREATE TABLE IF NOT EXISTS task_preferences (
                preference_id SERIAL PRIMARY KEY,
                user_id INTEGER,
                task_id TEXT,
                task_name TEXT,
                preference_data TEXT,
                FOREIGN KEY (user_id) REFERENCES users(user_id) ON DELETE CASCADE
            );
    """,
        ),
        (
            "unified_instructions",
            """
            CREATE TABLE IF NOT EXISTS unified_instructions (
                id INTEGER GENERATED ALWAYS AS IDENTITY PRIMARY KEY,
                session_id TEXT,
                timestamp TIMESTAMP,
                liu_id TEXT,
                voice_command TEXT,
                gesture_command TEXT,
                unified_command TEXT,
                confidence FLOAT CHECK(confidence BETWEEN 0 AND 1),
                processed BOOLEAN DEFAULT FALSE,
                FOREIGN KEY (liu_id) REFERENCES users(liu_id) ON DELETE CASCADE
            );
    """,
        ),
        (
            "operation_sequence",
            """
            CREATE TABLE IF NOT EXISTS operation_sequence (
                id SERIAL PRIMARY KEY,
                operation_id INTEGER NOT NULL,         -- order of execution
                sequence_id INTEGER NOT NULL,          -- FK to sequence_library
                sequence_name TEXT NOT NULL,           -- redundant but helpful for readability
                object_name TEXT,
                command_id INTEGER,                    -- FK to unified_instructions(id)
                processed BOOLEAN DEFAULT FALSE,       -- âœ… track if step is completed
                execution_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

                FOREIGN KEY (sequence_id) REFERENCES sequence_library(sequence_id),
                FOREIGN KEY (command_id) REFERENCES unified_instructions(id)
            );
    """,
        ),
        (
            "gesture_library",
            """
            CREATE TABLE IF NOT EXISTS gesture_library (
                id INTEGER GENERATED ALWAYS AS IDENTITY PRIMARY KEY,
                gesture_type TEXT UNIQUE NOT NULL,
                gesture_text TEXT NOT NULL,
                natural_description TEXT,
                config JSONB

            );
    """,
        ),
        (
            "gesture_instructions",
            """
            CREATE TABLE IF NOT EXISTS gesture_instructions (
                id INTEGER GENERATED ALWAYS AS IDENTITY PRIMARY KEY,
                session_id TEXT NOT NULL,
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                gesture_text TEXT NOT NULL,
                natural_description TEXT,
                confidence REAL,
                hand_label TEXT,
                processed BOOLEAN DEFAULT FALSE
            );
    """,
        ),
        (
            "voice_instructions",
            """
            CREATE TABLE IF NOT EXISTS voice_instructions (
                id INTEGER GENERATED ALWAYS AS IDENTITY PRIMARY KEY,
                session_id TEXT NOT NULL,
                transcribed_text TEXT NOT NULL,
                confidence REAL,
                language TEXT NOT NULL,
                processed BOOLEAN DEFAULT FALSE,
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );
    """,
        ),
        (
            "instruction_operation_sequence",
            """
            CREATE TABLE IF NOT EXISTS instruction_operation_sequence (
                task_id SERIAL PRIMARY KEY,
                instruction_id INTEGER,
                skill_id INTEGER,
                skill_name TEXT,
                sequence_id INTEGER,
                sequence_name TEXT NOT NULL,
                object_id INTEGER,
                object_name TEXT,
                status TEXT CHECK(status IN ('pending', 'in_progress', 'completed', 'failed')) DEFAULT 'pending',
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (instruction_id) REFERENCES instructions(id) ON DELETE CASCADE,
                FOREIGN KEY (skill_id) REFERENCES skills(skill_id),
                FOREIGN KEY (sequence_id) REFERENCES sequence_library(sequence_id),
                FOREIGN KEY (object_id) REFERENCES camera_vision(object_id)
            );
        """,
        ),
        (
            "task_history",
            """
            CREATE TABLE IF NOT EXISTS task_history (
                id SERIAL PRIMARY KEY,
                command_text TEXT,
                generated_plan JSONB,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );
        """,
        ),
    ]
)

indexes = [
    "CREATE INDEX IF NOT EXISTS idx_users_liu_id ON users(liu_id);",
    "CREATE INDEX IF NOT EXISTS idx_users_email ON users(email);",
    "CREATE INDEX IF NOT EXISTS idx_interaction_memory_user_id ON interaction_memory(user_id);",
    "CREATE INDEX IF NOT EXISTS idx_task_preferences_user_id ON task_preferences(user_id);",
    "CREATE INDEX IF NOT EXISTS idx_conv_memory_instruction ON interaction_memory(instruction_id);",
    "CREATE INDEX IF NOT EXISTS idx_skills_name ON skills(skill_name);",
    "CREATE INDEX IF NOT EXISTS idx_simulation_instruction ON simulation_results(instruction_id);",
    "CREATE INDEX IF NOT EXISTS idx_operation_sequence_object ON instruction_operation_sequence(object_id);",
    "CREATE INDEX IF NOT EXISTS idx_camera_vision_last_detected ON camera_vision(last_detected);",
    "CREATE INDEX IF NOT EXISTS idx_user_prefs_task ON task_preferences(user_id, task_id);",
    "CREATE INDEX IF NOT EXISTS idx_voice_session_id ON voice_instructions(session_id);",
    "CREATE INDEX IF NOT EXISTS idx_voice_processed ON voice_instructions(processed);",
    "CREATE INDEX IF NOT EXISTS idx_task_templates_name ON task_templates(task_name);",
    "CREATE INDEX IF NOT EXISTS idx_task_templates_sequence ON task_templates(default_sequence);",
    "CREATE INDEX IF NOT EXISTS idx_unified_instructions_session_id ON unified_instructions(session_id);",
]
###########################
# mini_project/modalities/command_processor.py
""" CommandProcessor module for handling unified robotic commands, database operations, and LLM-based inference.
This module provides the CommandProcessor class, which is responsible for:
- Connecting to the database and managing persistent cursors.
- Fetching and caching available sequences and objects for validation.
- Extracting and validating operations from LLM responses.
- Inferring operation names using an LLM (Ollama).
- Generating operation sequences based on sort order and task templates.
- Populating operation-specific parameter tables for robotic tasks.
- Extracting and populating sort order from LLM output.
- Processing unified commands and updating the database accordingly.
- Managing logging and error handling throughout the command processing cycle.
Classes:
    CommandProcessor: Main class for processing commands, interacting with the database, and integrating LLM inference.
Functions:
    __init__(self, llm_model: str = OLLAMA_MODEL)
        Initialize the CommandProcessor, set up database connection, and cache available sequences/objects.
    get_available_sequences(self) -> List[str]
        Fetch available sequence names from the sequence_library table.
    fetch_column(self, table: str, column: str) -> list
        Fetch a single column from a specified table.
    get_available_objects(self) -> List[str]
        Fetch available object names from the camera_vision table.
    get_unprocessed_unified_command(self) -> Dict
        Retrieve the latest unprocessed unified command from the unified_instructions table.
    get_available_objects_with_colors(self) -> List[str]
        Fetch available objects with their colors from the camera_vision table.
    get_sort_order(self) -> List[str]
        Fetch the current sort order from the sort_order table.
    get_task_templates(self) -> Dict[str, List[str]]
        Fetch task templates from the task_templates table.
    refresh_cache(self)
        Refresh cached sequences and objects from the database.
    validate_operation(self, operation: Dict) -> bool
        Validate the structure and values of an operation.
    extract_json_array(self, raw_response: str) -> List[Dict]
        Extract a JSON array from a raw LLM response string.
    infer_operation_name_from_llm(self, command_text: str) -> str
        Use LLM to infer the operation name from a user command.
    get_task_order(self, operation_name: str) -> List[str]
        Get the task order for a given operation name.
    generate_operations_from_sort_order(self, task_order: List[str], command_id: int) -> List[Dict]
        Generate a list of operations based on the sort order and task order.
    process_command(self, unified_command: Dict) -> Tuple[bool, List[Dict]]
        Process a unified command: generate operations, update the database, and populate parameters.
    populate_operation_parameters(self)
        Populate operation-specific parameter tables based on planned sequences.
    extract_sort_order_from_llm(self, command_text: str) -> List[Tuple[str, str]]
        Use LLM to extract the sort order (object name and color) from a command.
    populate_sort_order_from_llm(self, command_text: str) -> None
        Populate the sort_order table using LLM-extracted sort order.
    run_processing_cycle(self)
        Process the latest unprocessed unified command, if available.
    close(self)
        Close the database connection.
Usage:
    Run as a script to process the latest unified command using:
        python command_processor.py
Environment Variables:
    DEBUG: Enable debug mode for logging.
    LOG_LEVEL: Set the logging level (default: DEBUG if debug mode, else INFO).
Dependencies:
    - ollama (for LLM chat)
    - psycopg2 (for PostgreSQL database access)
    - mini_project.config.app_config
    - mini_project.database.connection
    - mini_project.modalities.prompt_utils
 """

import atexit
import json
import logging
import os
from collections import defaultdict, deque
from typing import Dict, List, Tuple

import ollama
import psycopg2
from psycopg2 import Error as Psycopg2Error
from psycopg2 import sql
from psycopg2.extras import DictCursor

from mini_project.config.app_config import setup_logging
from mini_project.database.connection import get_connection
from mini_project.modalities.prompt_utils import PromptBuilder

# === Logging Setup ===
debug_mode = os.getenv("DEBUG", "0") in ["1", "true", "True"]
log_level = os.getenv("LOG_LEVEL", "DEBUG" if debug_mode else "INFO").upper()
setup_logging(level=getattr(logging, log_level))
logger = logging.getLogger("CommandProcess")

# models: "llama3.2:1b", "deepseek-r1:1.5b", "mistral:latest", "deepseek-r1:32b"
OLLAMA_MODEL = "mistral:latest"


class CommandProcessor:

    def __init__(self, llm_model: str = OLLAMA_MODEL):
        self.conn = get_connection()
        self.cursor = self.conn.cursor(cursor_factory=DictCursor)
        self.logger = logger
        self.llm_model = llm_model

        # Cache available sequences and objects from database for validation purposes
        self.available_sequences = self.get_available_sequences()
        self.available_objects = self.get_available_objects()

    def get_available_sequences(self) -> List[str]:
        """Fetch available sequence names from sequence_library in database"""
        self.cursor.execute("SELECT sequence_name FROM sequence_library")
        available_sequences = [row[0] for row in self.cursor.fetchall()]
        logger.info(f"ðŸŸ¢ Available sequences: {available_sequences}")
        return available_sequences

    def fetch_column(self, table: str, column: str) -> list:
        try:
            query = sql.SQL("SELECT {field} FROM {tbl}").format(
                field=sql.Identifier(column), tbl=sql.Identifier(table)
            )
            self.cursor.execute(query)
            return [row[0] for row in self.cursor.fetchall()]
        except Psycopg2Error as e:
            logger.error("Database error in fetch_column: %s", str(e), exc_info=True)
            raise

    def get_available_objects(self) -> List[str]:
        self.cursor.execute("SELECT object_name FROM camera_vision")
        available_objects = [row[0] for row in self.cursor.fetchall()]
        logger.info(f"ðŸŸ¢ Available objects: {available_objects}")
        return available_objects

    def get_unprocessed_unified_command(self) -> Dict:
        self.cursor.execute(
            """
            SELECT id, unified_command FROM unified_instructions
            WHERE processed = FALSE ORDER BY id DESC LIMIT 1
        """
        )
        result = self.cursor.fetchone()
        return (
            {"id": result["id"], "unified_command": result["unified_command"]}
            if result
            else None
        )

    def get_available_objects_with_colors(self) -> List[str]:
        self.cursor.execute("SELECT object_name, object_color FROM camera_vision")
        return [f"{name} ({color})" for name, color in self.cursor.fetchall()]

    def get_sort_order(self) -> List[str]:
        self.cursor.execute(
            "SELECT object_name, object_color FROM sort_order ORDER BY order_id ASC"
        )
        return [f"{name} ({color})" for name, color in self.cursor.fetchall()]

    def get_task_templates(self) -> Dict[str, List[str]]:
        self.cursor.execute("SELECT task_name, default_sequence FROM task_templates")
        return {row[0]: row[1] for row in self.cursor.fetchall()}

    def refresh_cache(self):
        """Refreshes cached sequences and objects from the database."""
        self.available_sequences = self.get_available_sequences()
        self.available_objects = self.get_available_objects()
        logger.info("ðŸŸ¢ Cache refreshed: sequences and objects updated.")

    def validate_operation(self, operation: Dict) -> bool:
        """Validate operation structure"""
        self.refresh_cache()  # Ensure we're using the latest data

        if operation["sequence_name"] not in self.available_sequences:
            logger.error(f"Invalid sequence name: {operation['sequence_name']}")
            return False

        # Allow empty object names
        if not operation.get("object_name", ""):
            return True

        if operation["object_name"] in self.available_objects:
            return True
        else:
            logger.error(f"Invalid object name: {operation['object_name']}")
            return False

    def extract_json_array(self, raw_response: str) -> List[Dict]:
        import re

        try:
            if not PromptBuilder.validate_llm_json(raw_response):
                raise ValueError(
                    "Invalid JSON format: response must start with [ and end with ]"
                )

            # Match the first complete JSON array only
            match = re.search(r"\[\s*{[\s\S]*?}\s*]", raw_response)
            if not match:
                raise ValueError("No valid JSON array found in LLM response.")

            json_str = match.group(0)
            return json.loads(json_str)

        except Exception as e:
            logger.error(
                "Failed to extract JSON array from LLM response: %s", e, exc_info=True
            )
            raise

    def infer_operation_name_from_llm(self, command_text: str) -> str:
        self.cursor.execute("SELECT operation_name FROM operation_library")
        available_operations = [row[0] for row in self.cursor.fetchall()]

        prompt = f"""
        Given the following user command:

        "{command_text}"

        Choose the most appropriate operation name from the list:
        {', '.join(available_operations)}

        Only respond with the exact operation_name string â€” no extra words or explanation.
        """

        response = ollama.chat(
            model=self.llm_model,
            messages=[
                {
                    "role": "system",
                    "content": "You are a smart classifier for robotic task types.",
                },
                {"role": "user", "content": prompt},
            ],
        )
        result = response["message"]["content"].strip()

        if result not in available_operations:
            raise ValueError(f"LLM returned invalid operation_name: {result}")

        return result

    def get_task_order(self, operation_name: str) -> List[str]:
        self.cursor.execute(
            "SELECT task_order FROM operation_library WHERE operation_name = %s",
            (operation_name,),
        )
        row = self.cursor.fetchone()
        return [s.strip() for s in row[0].split(",")] if row else []

    def generate_operations_from_sort_order(
        self, task_order: List[str], command_id: int
    ) -> List[Dict]:
        self.cursor.execute("SELECT object_name FROM sort_order ORDER BY order_id")
        objects = [row[0] for row in self.cursor.fetchall()]

        self.cursor.execute(
            "SELECT COALESCE(MAX(operation_id), 0) + 1 FROM operation_sequence"
        )
        operation_id_start = self.cursor.fetchone()[0]

        ops = []
        idx = 0

        for obj in objects:
            for seq in task_order:
                self.cursor.execute(
                    "SELECT sequence_id FROM sequence_library WHERE sequence_name = %s",
                    (seq,),
                )
                seq_id_row = self.cursor.fetchone()
                if not seq_id_row:
                    continue

                ops.append(
                    {
                        "operation_id": operation_id_start + idx,
                        "sequence_id": seq_id_row[0],
                        "sequence_name": seq,
                        "object_name": obj,
                        "command_id": command_id,
                    }
                )
                idx += 1

        # Optionally add a final "go_home"
        self.cursor.execute(
            "SELECT sequence_id FROM sequence_library WHERE sequence_name = 'go_home'"
        )
        go_home_seq = self.cursor.fetchone()
        if go_home_seq:
            ops.append(
                {
                    "operation_id": operation_id_start + idx,
                    "sequence_id": go_home_seq[0],
                    "sequence_name": "go_home",
                    "object_name": "",
                    "command_id": command_id,
                }
            )

        return ops

    def process_command(self, unified_command: Dict) -> Tuple[bool, List[Dict]]:
        try:
            self.populate_sort_order_from_llm(unified_command["unified_command"])

            operation_name = self.infer_operation_name_from_llm(
                unified_command["unified_command"]
            )
            task_order = self.get_task_order(operation_name)

            operations = self.generate_operations_from_sort_order(
                task_order, unified_command["id"]
            )

            # Wipe old unprocessed
            self.cursor.execute(
                "UPDATE operation_sequence SET processed = TRUE WHERE processed = FALSE"
            )

            # Insert new
            insert_count = 0
            for op in operations:
                self.cursor.execute(
                    """
                    INSERT INTO operation_sequence
                    (operation_id, sequence_id, sequence_name, object_name, command_id)
                    VALUES (%s, %s, %s, %s, %s)
                    """,
                    (
                        op["operation_id"],
                        op["sequence_id"],
                        op["sequence_name"],
                        op["object_name"],
                        op["command_id"],
                    ),
                )
                insert_count += 1
            logger.info(f"âœ… Inserted {insert_count} rows into operation_sequence.")

            # Mark as processed
            self.cursor.execute(
                "UPDATE unified_instructions SET processed = TRUE WHERE id = %s",
                (unified_command["id"],),
            )

            # Auto-populate operation parameter tables
            self.populate_operation_parameters()

            self.conn.commit()
            return True, operations

        except Exception as e:
            logger.error("Failed to process command: %s", e, exc_info=True)
            self.conn.rollback()
            return False, []

    def populate_operation_parameters(self):
        logger.info("Populating operation-specific parameters...")

        # Step 1: Get all unique sequence types planned
        self.cursor.execute("SELECT DISTINCT sequence_name FROM operation_sequence")
        sequence_types = [row[0] for row in self.cursor.fetchall()]

        insert_count = 0
        self.cursor.execute(
            "SELECT COUNT(*) FROM camera_vision WHERE usd_name = 'Slide.usd'"
        )
        slide_usd_count = self.cursor.fetchone()[0]
        if slide_usd_count > 0:

            if "pick" in sequence_types:
                self.cursor.execute("DELETE FROM pick_op_parameters")
                self.cursor.execute(
                    "SELECT object_name FROM operation_sequence WHERE sequence_name = 'pick'"
                )
                pick_data = self.cursor.fetchall()
                for i, (obj,) in enumerate(pick_data):
                    self.cursor.execute(
                        """
                        INSERT INTO pick_op_parameters (
                            operation_order, object_id, slide_state_status, slide_direction, distance_travel, operation_status
                        ) VALUES (%s, %s, %s, %s, %s, %s)
                        """,
                        (i + 1, obj, False, "y", 0.01, False),
                    )
                    insert_count += 1
                logger.info(f"âœ… Inserted {insert_count} rows into pick_op_parameters.")

            if "travel" in sequence_types:
                self.cursor.execute("DELETE FROM travel_op_parameters")
                self.cursor.execute(
                    "SELECT object_name FROM operation_sequence WHERE sequence_name = 'travel'"
                )
                travel_data = self.cursor.fetchall()
                for i, (obj,) in enumerate(travel_data):
                    self.cursor.execute(
                        """
                        INSERT INTO travel_op_parameters (
                            operation_order, object_id, travel_height, gripper_rotation, operation_status
                        ) VALUES (%s, %s, %s, %s, %s)
                        """,
                        (i + 1, obj, 0.085, "y-axis", False),
                    )
                    insert_count += 1
                logger.info(
                    f"âœ… Inserted {insert_count} rows into travel_op_parameters."
                )

            if "drop" in sequence_types:
                self.cursor.execute("DELETE FROM drop_op_parameters")
                self.cursor.execute(
                    "SELECT object_name FROM operation_sequence WHERE sequence_name = 'drop'"
                )
                drop_data = self.cursor.fetchall()
                for i, (obj,) in enumerate(drop_data):
                    self.cursor.execute(
                        """
                        INSERT INTO drop_op_parameters (
                            operation_order, object_id, drop_height, operation_status
                        ) VALUES (%s, %s, %s, %s)
                        """,
                        (i + 1, obj, 0.0, False),
                    )
                    insert_count += 1
                logger.info(f"âœ… Inserted {insert_count} rows into drop_op_parameters.")

            if "screw" in sequence_types:
                self.cursor.execute("DELETE FROM screw_op_parameters")
                self.cursor.execute(
                    "SELECT sequence_id, object_name FROM operation_sequence WHERE sequence_name = 'screw'"
                )
                screw_data = self.cursor.fetchall()
                for i, (seq_id, obj) in enumerate(screw_data):
                    self.cursor.execute(
                        """
                        INSERT INTO screw_op_parameters (
                            operation_order, sequence_id, object_id,
                            rotation_dir, number_of_rotations,
                            current_rotation, operation_status
                        ) VALUES (%s, %s, %s, %s, %s, %s, %s)
                        """,
                        (i + 1, seq_id, obj, i % 2 == 0, 3, 0, False),
                    )
                    insert_count += 1
                logger.info(f"Inserted {insert_count} rows into screw_op_parameters.")

            if "screw" in sequence_types:
                self.cursor.execute("DELETE FROM rotate_state_parameters")
                self.cursor.execute(
                    "SELECT sequence_id, operation_order, object_id FROM screw_op_parameters"
                )
                rotate_data = self.cursor.fetchall()
                for seq_id, op_order, obj in rotate_data:
                    self.cursor.execute(
                        """
                        INSERT INTO rotate_state_parameters (
                            sequence_id, operation_order, object_id,
                            rotation_angle, operation_status
                        ) VALUES (%s, %s, %s, %s, %s)
                        """,
                        (seq_id, op_order, obj, 90, False),
                    )
                    insert_count += 1
                logger.info(
                    f"Inserted {insert_count} rows into rotate_state_parameters."
                )
        else:
            if "pick" in sequence_types:
                self.cursor.execute("DELETE FROM pick_op_parameters")
                self.cursor.execute(
                    "SELECT object_name FROM operation_sequence WHERE sequence_name = 'pick'"
                )
                pick_data = self.cursor.fetchall()
                for i, (obj,) in enumerate(pick_data):
                    self.cursor.execute(
                        """
                        INSERT INTO pick_op_parameters (
                            operation_order, object_id, slide_state_status, slide_direction, distance_travel, operation_status
                        ) VALUES (%s, %s, %s, %s, %s, %s)
                        """,
                        (i + 1, obj, False, "y", 0.01, False),
                    )
                    insert_count += 1
                logger.info(f"âœ… Inserted {insert_count} rows into pick_op_parameters.")

            if "travel" in sequence_types:
                self.cursor.execute("DELETE FROM travel_op_parameters")
                self.cursor.execute(
                    "SELECT object_name FROM operation_sequence WHERE sequence_name = 'travel'"
                )
                travel_data = self.cursor.fetchall()
                for i, (obj,) in enumerate(travel_data):
                    self.cursor.execute(
                        """
                        INSERT INTO travel_op_parameters (
                            operation_order, object_id, travel_height, gripper_rotation, operation_status
                        ) VALUES (%s, %s, %s, %s, %s)
                        """,
                        (i + 1, obj, 0.085, "z-axis", False),
                    )
                    insert_count += 1
                logger.info(
                    f"âœ… Inserted {insert_count} rows into travel_op_parameters."
                )

            # if "drop" in sequence_types:
            #     self.cursor.execute("DELETE FROM drop_op_parameters")
            #     self.cursor.execute(
            #         "SELECT object_name FROM operation_sequence WHERE sequence_name = 'drop'"
            #     )
            #     drop_data = self.cursor.fetchall()
            #     for i, (obj,) in enumerate(drop_data):
            #         self.cursor.execute(
            #             """
            #             INSERT INTO drop_op_parameters (
            #                 operation_order, object_id, drop_height, operation_status
            #             ) VALUES (%s, %s, %s, %s)
            #             """,
            #             (i + 1, obj, 0.0, False),
            #         )
            #         insert_count += 1
            #     logger.info(f"âœ… Inserted {insert_count} rows into drop_op_parameters.")

            if "screw" in sequence_types:
                self.cursor.execute("DELETE FROM screw_op_parameters")
                self.cursor.execute(
                    "SELECT sequence_id, object_name FROM operation_sequence WHERE sequence_name = 'screw'"
                )
                screw_data = self.cursor.fetchall()
                for i, (seq_id, obj) in enumerate(screw_data):
                    self.cursor.execute(
                        """
                        INSERT INTO screw_op_parameters (
                            operation_order, sequence_id, object_id,
                            rotation_dir, number_of_rotations,
                            current_rotation, operation_status
                        ) VALUES (%s, %s, %s, %s, %s, %s, %s)
                        """,
                        (i + 1, seq_id, obj, i % 2 == 0, 3, 0, False),
                    )
                    insert_count += 1
                logger.info(f"Inserted {insert_count} rows into screw_op_parameters.")

            if "screw" in sequence_types:
                self.cursor.execute("DELETE FROM rotate_state_parameters")
                self.cursor.execute(
                    "SELECT sequence_id, operation_order, object_id FROM screw_op_parameters"
                )
                rotate_data = self.cursor.fetchall()
                for seq_id, op_order, obj in rotate_data:
                    self.cursor.execute(
                        """
                        INSERT INTO rotate_state_parameters (
                            sequence_id, operation_order, object_id,
                            rotation_angle, operation_status
                        ) VALUES (%s, %s, %s, %s, %s)
                        """,
                        (seq_id, op_order, obj, 90, False),
                    )
                    insert_count += 1
                logger.info(
                    f"Inserted {insert_count} rows into rotate_state_parameters."
                )

        self.conn.commit()
        logger.info("âœ… Operation-specific parameter tables updated.")

    def extract_sort_order_from_llm(self, command_text: str) -> List[Tuple[str, str]]:
        try:
            logger.info("ðŸ§  Extracting sort order using LLM...")
            prompt = PromptBuilder.sort_order_prompt(command_text)
            response = ollama.chat(
                model=self.llm_model,
                messages=[
                    PromptBuilder.sort_order_system_msg(),
                    {"role": "user", "content": prompt},
                ],
            )

            parsed = self.extract_json_array(response["message"]["content"])
            if not isinstance(parsed, list):
                logger.warning("âš ï¸ Unexpected format from LLM sort extraction.")
                return []

            results = []
            for item in parsed:
                object_name = (item.get("object_name") or "").strip()
                object_color = (item.get("object_color") or "").strip()
                results.append((object_name, object_color))

            logger.info(f"âœ… Extracted sort order: {results}")
            return results

        except Exception as e:
            logger.error(
                "âŒ Failed to extract sort order using LLM: %s", str(e), exc_info=True
            )
            return []

    def populate_sort_order_from_llm(self, command_text: str) -> None:
        try:
            logger.info("ðŸ§  Asking LLM to extract sort order...")

            extracted = self.extract_sort_order_from_llm(command_text)

            if not extracted:
                logger.warning("âš ï¸ No sort order to insert.")
                return

            # Clear previous sort_order
            self.cursor.execute("DELETE FROM sort_order")

            # Fetch color-to-object_name mapping from camera_vision
            self.cursor.execute("SELECT object_name, object_color FROM camera_vision")
            camera_color_map = self.cursor.fetchall()
            color_to_names = {}
            for name, color in camera_color_map:
                color_to_names.setdefault(color.lower(), []).append(name)

            inserted = []
            for item in extracted:
                color = item[1].lower()
                name_list = color_to_names.get(color, [])
                if not name_list:
                    logger.warning(f"âš ï¸ No match in camera_vision for color: {color}")
                    continue
                obj_name = name_list.pop(0)  # Use and remove to avoid duplicates
                self.cursor.execute(
                    "INSERT INTO sort_order (object_name, object_color) VALUES (%s, %s)",
                    (obj_name, color),
                )
                inserted.append((obj_name, color))

            logger.info(f"âœ… sort_order table populated: {inserted}")

        except Exception as e:
            logger.error(
                "âŒ Failed to populate sort_order table: %s", str(e), exc_info=True
            )

    def run_processing_cycle(self):
        """
        Process the latest unprocessed unified command
        """
        logger.info("ðŸŸ¢ Checking for new unified_commands...")
        unified_command = self.get_unprocessed_unified_command()

        if unified_command:
            logger.info(f"Processing command ID: {unified_command['id']}")
            if self.process_command(unified_command):
                logger.info(
                    f"Successfully processed unified_command {unified_command['id']}"
                )
            else:
                logger.error(
                    f"Failed to process unified_command {unified_command['id']}"
                )
        else:
            logger.info("ðŸŸ¡ No unprocessed unified_commands found")

    def close(self):
        """Close the persistent SQLite connection."""
        if self.conn:
            self.conn.close()
            self.conn = None
            logger.info("ðŸŸ¢ Database connection closed.")


if __name__ == "__main__":
    processor = CommandProcessor(llm_model="mistral:latest")
    # Register the close method so it gets called when the program exits
    atexit.register(processor.close)
    processor.run_processing_cycle()
#########################
# mini_project/modalities/gesture_processor.py
""" GestureProcessor module for hand gesture detection and logging using MediaPipe and OpenCV.

Classes:
    GestureDetector:
        Detects hand gestures from video frames, logs gesture events to a PostgreSQL database,
        and provides real-time feedback via OpenCV overlays.

        Methods:
            __init__(self, min_detection_confidence, min_tracking_confidence, max_num_hands, frame_skip, session_id):
                Initializes the gesture detector, database connection, and loads gesture definitions.

            _init_db(self):
                Initializes the gesture_instructions table in the PostgreSQL database.

            _log_gesture(self, gesture_type, gesture_text, natural_description, confidence, hand_label):
                Logs detected gesture information into the database.

            _get_landmark_coords(self, landmarks, landmark_id):
                Retrieves the (x, y, z) coordinates for a specific hand landmark.

            _euclidean_distance(self, a, b):
                Calculates the Euclidean distance between two 3D points.

            _is_thumbs_up(self, landmarks):
                Determines if the hand gesture is a thumbs-up.

            _is_open_hand(self, landmarks):
                Determines if the hand gesture is an open hand.

            _is_pointing(self, landmarks):
                Determines if the hand gesture is a pointing gesture.

            _is_closed_fist(self, landmarks):
                Determines if the hand gesture is a closed fist.

            _is_victory(self, landmarks):
                Determines if the hand gesture is a victory sign.

            _is_ok_sign(self, landmarks):
                Determines if the hand gesture is an OK sign.

            _analyze_thumb(self, landmarks):
                Analyzes the thumb's position (up or down).

            _count_open_fingers(self, landmarks):
                Counts the number of open fingers.

            convert_features_to_description(self, gesture_type, hand_landmarks):
                Converts gesture features into a natural language description.

            detect_gesture(self, frame):
                Detects gestures in a given video frame and returns detection results.

            _process_frame(self, frame):
                Processes a single video frame: flips, resizes, detects gestures, overlays info, and draws landmarks.

            load_gesture_definitions(self):
                Loads gesture definitions from the gesture_library database table.

            gesture_map_functions(self):
                Returns a mapping of gesture types to their detection functions.

            process_video_stream(self, termination_event):
                Captures video from the camera, processes frames for gesture detection, and displays results in real-time.

Usage:
    Run this module directly to start gesture detection from the default camera.

 """
import logging
import threading
import time
import uuid
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple

import cv2
import mediapipe as mp
from config.app_config import *
from psycopg2 import Error as Psycopg2Error

from mini_project.database.connection import get_connection

setup_logging(level=logging.INFO)
logger = logging.getLogger("GestureProcessor")


class GestureDetector:
    def __init__(
        self,
        min_detection_confidence: float = MIN_DETECTION_CONFIDENCE,
        min_tracking_confidence: float = MIN_TRACKING_CONFIDENCE,
        max_num_hands: int = MAX_NUM_HANDS,
        frame_skip: int = FRAME_SKIP,
        session_id: Optional[str] = None,
    ):
        self.mp_hands = mp.solutions.hands
        self.hands = self.mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=max_num_hands,
            min_detection_confidence=min_detection_confidence,
            min_tracking_confidence=min_tracking_confidence,
        )
        self.mp_drawing = mp.solutions.drawing_utils
        self.mp_drawing_styles = mp.solutions.drawing_styles

        self.conn = get_connection()

        self._init_db()
        self.cursor = self.conn.cursor()

        self.session_id = session_id or str(uuid.uuid4())
        self.frame_skip = frame_skip
        self.frame_counter = 0

        loaded_gestures = self.load_gesture_definitions()
        if loaded_gestures:
            self.gesture_map = loaded_gestures

        self.last_gesture: Optional[str] = None
        self.last_log_time: float = 0
        self.min_log_interval: float = 2.0
        self.last_detection: Optional[List[Dict[str, Any]]] = None

    def _init_db(self):
        with self.conn.cursor() as cursor:
            cursor.execute(
                """
                CREATE TABLE IF NOT EXISTS gesture_instructions (
                    id SERIAL PRIMARY KEY,
                    session_id TEXT NOT NULL,
                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    gesture_text TEXT NOT NULL,
                    natural_description TEXT,
                    confidence REAL,
                    hand_label TEXT,
                    processed BOOLEAN DEFAULT FALSE
                )
                """
            )
        self.conn.commit()
        logger.info("Gesture table (PostgreSQL) initialized.")

    def _log_gesture(
        self,
        gesture_type: str,
        gesture_text: str,
        natural_description: str,
        confidence: float,
        hand_label: str,
    ):
        timestamp = datetime.now()
        try:
            with self.conn.cursor() as cursor:
                cursor.execute(
                    """
                    INSERT INTO gesture_instructions
                    (session_id, timestamp, gesture_text, natural_description, confidence, hand_label)
                    VALUES (%s, %s, %s, %s, %s, %s)
                    """,
                    (
                        self.session_id,
                        timestamp,
                        gesture_text,
                        natural_description,
                        confidence,
                        hand_label,
                    ),
                )
            self.conn.commit()

            logger.info(
                f"Gesture: [{gesture_text}], Hand: [{hand_label}], Confidence: [{confidence:.2f}], Description: [{natural_description}]"
            )
        except Psycopg2Error as e:
            logger.error(f"[DB:PostgreSQL] Gesture logging failed: {e}", exc_info=True)
            self.conn.rollback()

    def _get_landmark_coords(
        self, landmarks, landmark_id: int
    ) -> Tuple[float, float, float]:
        landmark = landmarks.landmark[landmark_id]
        return (landmark.x, landmark.y, landmark.z)

    def _euclidean_distance(
        self, a: Tuple[float, float, float], b: Tuple[float, float, float]
    ) -> float:
        return ((a[0] - b[0]) ** 2 + (a[1] - b[1]) ** 2 + (a[2] - b[2]) ** 2) ** 0.5

    def _is_thumbs_up(self, landmarks) -> bool:
        thumb_tip = self._get_landmark_coords(
            landmarks, self.mp_hands.HandLandmark.THUMB_TIP
        )
        index_pip = self._get_landmark_coords(
            landmarks, self.mp_hands.HandLandmark.INDEX_FINGER_PIP
        )
        return thumb_tip[1] < index_pip[1]

    def _is_open_hand(self, landmarks) -> bool:
        fingertips = [
            self._get_landmark_coords(landmarks, tip)[1]
            for tip in [
                self.mp_hands.HandLandmark.THUMB_TIP,
                self.mp_hands.HandLandmark.INDEX_FINGER_TIP,
                self.mp_hands.HandLandmark.MIDDLE_FINGER_TIP,
                self.mp_hands.HandLandmark.RING_FINGER_TIP,
                self.mp_hands.HandLandmark.PINKY_TIP,
            ]
        ]
        wrist_y = landmarks.landmark[self.mp_hands.HandLandmark.WRIST].y
        return all(y < wrist_y for y in fingertips)

    def _is_pointing(self, landmarks) -> bool:
        index_tip = self._get_landmark_coords(
            landmarks, self.mp_hands.HandLandmark.INDEX_FINGER_TIP
        )
        middle_tip = self._get_landmark_coords(
            landmarks, self.mp_hands.HandLandmark.MIDDLE_FINGER_TIP
        )
        return index_tip[1] < middle_tip[1]

    def _is_closed_fist(self, landmarks) -> bool:
        fingertips = [
            self._get_landmark_coords(landmarks, tip)[1]
            for tip in [
                self.mp_hands.HandLandmark.THUMB_TIP,
                self.mp_hands.HandLandmark.INDEX_FINGER_TIP,
                self.mp_hands.HandLandmark.MIDDLE_FINGER_TIP,
            ]
        ]
        mcp_joints = [
            self._get_landmark_coords(landmarks, joint)[1]
            for joint in [
                self.mp_hands.HandLandmark.THUMB_MCP,
                self.mp_hands.HandLandmark.INDEX_FINGER_MCP,
                self.mp_hands.HandLandmark.MIDDLE_FINGER_MCP,
            ]
        ]
        return all(tip > mcp for tip, mcp in zip(fingertips, mcp_joints))

    def _is_victory(self, landmarks) -> bool:
        index_tip = self._get_landmark_coords(
            landmarks, self.mp_hands.HandLandmark.INDEX_FINGER_TIP
        )
        middle_tip = self._get_landmark_coords(
            landmarks, self.mp_hands.HandLandmark.MIDDLE_FINGER_TIP
        )
        ring_tip = self._get_landmark_coords(
            landmarks, self.mp_hands.HandLandmark.RING_FINGER_TIP
        )
        wrist_y = self._get_landmark_coords(
            landmarks, self.mp_hands.HandLandmark.WRIST
        )[1]
        return (
            index_tip[1] < ring_tip[1]
            and middle_tip[1] < ring_tip[1]
            and ring_tip[1] > wrist_y
        )

    def _is_ok_sign(self, landmarks) -> bool:
        thumb_tip = self._get_landmark_coords(
            landmarks, self.mp_hands.HandLandmark.THUMB_TIP
        )
        index_tip = self._get_landmark_coords(
            landmarks, self.mp_hands.HandLandmark.INDEX_FINGER_TIP
        )
        # Define a threshold for the OK sign; adjust if needed.
        threshold = 0.05
        distance = self._euclidean_distance(thumb_tip, index_tip)
        return distance < threshold

    def _analyze_thumb(self, landmarks) -> str:
        thumb_tip = self._get_landmark_coords(
            landmarks, self.mp_hands.HandLandmark.THUMB_TIP
        )
        thumb_ip = self._get_landmark_coords(
            landmarks, self.mp_hands.HandLandmark.THUMB_IP
        )
        return "up" if thumb_tip[1] < thumb_ip[1] else "down"

    def _count_open_fingers(self, landmarks) -> int:
        count = 0
        if (
            landmarks.landmark[self.mp_hands.HandLandmark.INDEX_FINGER_TIP].y
            < landmarks.landmark[self.mp_hands.HandLandmark.INDEX_FINGER_MCP].y
        ):
            count += 1
        if (
            landmarks.landmark[self.mp_hands.HandLandmark.MIDDLE_FINGER_TIP].y
            < landmarks.landmark[self.mp_hands.HandLandmark.MIDDLE_FINGER_MCP].y
        ):
            count += 1
        if (
            landmarks.landmark[self.mp_hands.HandLandmark.RING_FINGER_TIP].y
            < landmarks.landmark[self.mp_hands.HandLandmark.RING_FINGER_MCP].y
        ):
            count += 1
        if (
            landmarks.landmark[self.mp_hands.HandLandmark.PINKY_TIP].y
            < landmarks.landmark[self.mp_hands.HandLandmark.PINKY_MCP].y
        ):
            count += 1
        return count

    def convert_features_to_description(self, gesture_type: str, hand_landmarks) -> str:
        thumb_state = self._analyze_thumb(hand_landmarks)
        open_fingers = self._count_open_fingers(hand_landmarks)
        if gesture_type == "thumbs_up":
            base_desc = "The thumb is raised above the index finger, indicating a thumbs-up or approval gesture."
        elif gesture_type == "open_hand":
            base_desc = "All fingers are extended, showing an open hand posture which may signal a stop command."
        elif gesture_type == "pointing":
            base_desc = "The index finger is extended while the other fingers remain curled, suggesting the user is pointing."
        elif gesture_type == "closed_fist":
            base_desc = "The hand is clenched into a fist, a posture often associated with grabbing or assertiveness."
        elif gesture_type == "victory":
            base_desc = "The hand forms a V-shape with the index and middle fingers extended, commonly used to signal victory or confirmation."
        elif gesture_type == "ok_sign":
            base_desc = "The thumb and index finger are touching to form a circle, commonly known as the OK sign."
        else:
            base_desc = "A neutral gesture with no distinct features."
        return f"{base_desc} Additionally, the thumb is {thumb_state} and {open_fingers} fingers are open."

    def detect_gesture(self, frame) -> Optional[List[Dict[str, Any]]]:
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.hands.process(rgb_frame)
        if not results.multi_hand_landmarks or not results.multi_handedness:
            return None

        detections = []
        for hand_landmarks, handedness_info in zip(
            results.multi_hand_landmarks, results.multi_handedness
        ):
            hand_label = handedness_info.classification[0].label
            hand_confidence = handedness_info.classification[0].score
            for gesture, config in self.gesture_map.items():
                if config["func"](hand_landmarks):
                    description = self.convert_features_to_description(
                        gesture, hand_landmarks
                    )
                    detections.append(
                        {
                            "modality": "gesture",
                            "gesture": gesture,
                            "gesture_text": config["text"],
                            "confidence": hand_confidence,
                            "hand_label": hand_label,
                            "description": description,
                            "landmarks": hand_landmarks,
                        }
                    )
                    break
        return detections

    def _process_frame(self, frame):
        """
        Process a single frame: flip, resize, update detection, overlay gesture info, and draw landmarks.
        """
        # Flip and resize for a consistent display.
        frame = cv2.flip(frame, 1)
        frame = cv2.resize(frame, (640, 480))
        self.frame_counter += 1

        # Update detection on every frame_skip-th frame.
        if self.frame_counter % self.frame_skip == 0:
            detection = self.detect_gesture(frame)
            self.last_detection = detection if detection is not None else None
            current_time = time.time()
            if self.last_detection:
                for idx, d in enumerate(self.last_detection):
                    if d["gesture"] == self.last_gesture and (
                        current_time - self.last_log_time < self.min_log_interval
                    ):
                        continue
                    self._log_gesture(
                        d["gesture"],
                        d["gesture_text"],
                        d["description"],
                        d["confidence"],
                        d["hand_label"],
                    )
                    self.last_gesture = d["gesture"]
                    self.last_log_time = current_time

        # Overlay detection info if available.
        if self.last_detection:
            for idx, d in enumerate(self.last_detection):
                cv2.putText(
                    frame,
                    f"{d['gesture_text']} [{d['hand_label']}]",
                    (10, 30 + idx * 60),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    1,
                    (0, 255, 0),
                    2,
                )
                cv2.putText(
                    frame,
                    d["description"],
                    (10, 70 + idx * 60),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.6,
                    (255, 255, 0),
                    2,
                )
                if d["gesture"] == "thumbs_up":
                    cv2.putText(
                        frame,
                        "Thumb Highlight",
                        (10, 110 + idx * 60),
                        cv2.FONT_HERSHEY_SIMPLEX,
                        0.6,
                        (0, 0, 255),
                        2,
                    )

        # Draw landmarks on the frame.
        if self.last_detection:
            for d in self.last_detection:
                self.mp_drawing.draw_landmarks(
                    frame,
                    d["landmarks"],
                    self.mp_hands.HAND_CONNECTIONS,
                    self.mp_drawing_styles.get_default_hand_landmarks_style(),
                    self.mp_drawing_styles.get_default_hand_connections_style(),
                )
        else:
            rgb_for_drawing = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = self.hands.process(rgb_for_drawing)
            if results.multi_hand_landmarks:
                for hand_landmarks in results.multi_hand_landmarks:
                    self.mp_drawing.draw_landmarks(
                        frame,
                        hand_landmarks,
                        self.mp_hands.HAND_CONNECTIONS,
                        self.mp_drawing_styles.get_default_hand_landmarks_style(),
                        self.mp_drawing_styles.get_default_hand_connections_style(),
                    )
        return frame

    def load_gesture_definitions(self) -> Dict[str, Dict[str, Any]]:
        try:
            with self.conn.cursor() as cursor:
                query = "SELECT gesture_type, gesture_text, natural_description, config FROM gesture_library"
                cursor.execute(query)
                definitions = {}
                for row in cursor.fetchall():
                    gesture_type, gesture_text, natural_description, config = row
                    definitions[gesture_type] = {
                        "text": gesture_text,
                        "description": natural_description,
                        # You can parse the JSON config if needed:
                        "config": config,
                        # Map to your detection function via gesture_map_functions:
                        "func": self.gesture_map_functions().get(gesture_type),
                    }
            return definitions
        except Psycopg2Error as e:
            logger.error(f"Error loading gesture definitions: {e}")
            return {}

    def gesture_map_functions(self) -> Dict[str, Any]:
        """Returns a mapping of gesture types to detection functions."""
        return {
            "thumbs_up": self._is_thumbs_up,
            "open_hand": self._is_open_hand,
            "pointing": self._is_pointing,
            "closed_fist": self._is_closed_fist,
            "victory": self._is_victory,
            "ok_sign": self._is_ok_sign,
        }

    def process_video_stream(self, termination_event: Optional[threading.Event] = None):
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            logger.error("Error: Could not open video stream.")
            return

        def video_loop():
            while cap.isOpened() and not (
                termination_event and termination_event.is_set()
            ):
                ret, frame = cap.read()
                if not ret:
                    logger.error("Failed to capture frame from camera.")
                    break
                processed_frame = self._process_frame(frame)
                cv2.imshow("Gesture Detection", processed_frame)
                if cv2.waitKey(1) & 0xFF == ord("q"):
                    break
            cap.release()
            cv2.destroyAllWindows()
            self.conn.close()
            logger.info("Video stream ended and database connection closed.")

        video_thread = threading.Thread(target=video_loop)
        video_thread.start()
        video_thread.join()


if __name__ == "__main__":
    gd = GestureDetector()
    gd.process_video_stream()
#################################
# mini_project/modalities/session_manager.py
""" SessionManager handles user authentication and session management using face and voice modalities.
Classes:
    SessionManager: Manages user authentication (face and voice), session creation, cancellation, and retry.
SessionManager Methods:
    __init__(face_auth: FaceAuthSystem = None, voice_auth: VoiceAuth = None)
        Initializes the SessionManager with optional face and voice authentication modules.
    authenticate_user()
        Attempts to authenticate a user via face recognition. If the face is not recognized, initiates manual registration.
        After successful face authentication or registration, checks for an existing voice embedding and prompts for voice registration if absent.
        Returns the authenticated user dictionary or None if authentication fails.
    create_session()
        Generates a new unique session ID, marks the session as running, and logs the session creation.
        Returns the session ID.
    cancel_session()
        Cancels the current session if active and logs the action.
    retry_session()
        Cancels the current session (if any) and creates a new session.
        Returns the new session ID.
 """


import logging
import uuid

from mini_project.authentication._face_auth import FaceAuthSystem
from mini_project.authentication._voice_auth import VoiceAuth
from mini_project.config.app_config import DB_PATH, TEMP_AUDIO_PATH, VOICE_DATA_PATH

logger = logging.getLogger("SessionManager")


class SessionManager:
    def __init__(self, face_auth: FaceAuthSystem = None, voice_auth: VoiceAuth = None):

        # Instantiate authentication modules if not provided.
        self.face_auth = face_auth if face_auth else FaceAuthSystem()
        self.voice_auth = voice_auth if voice_auth else VoiceAuth()
        self.session_id = None
        self.authenticated_user = (
            None  # Expected dict, keys: 'liu_id', 'first_name', 'last_name', etc.
        )
        self.running = False

    def authenticate_user(self):
        """
        Attempt face authentication. If the face is not recognized, trigger manual registration.
        Following successful face registration, trigger voice registration.
        """
        logger.info("ðŸŸ¡ Attempting face authentication...")
        user = self.face_auth.identify_user()

        # if user:
        #     self.authenticated_user = user
        #     logger.info(
        #         f"âœ… Successful face authentication. Welcome {user['first_name']} {user['last_name']} (liu_id: {user['liu_id']})"
        #     )
        if not user:
            logger.warning(
                "ðŸ”´ Face not recognized. Initiating manual face registration..."
            )
            # Attempt to register the user
            registered = self.face_auth.register_user()
            if not registered:
                logger.warning("ðŸš« User declined registration. Session halted.")
                return None

            self.face_auth._refresh_index()
            user = self.face_auth.identify_user()
            if not user:
                logger.error("âŒ User authentication failed after registration.")
                return None

        self.authenticated_user = user  # âœ… Set it in both branches
        logger.info(
            f"âœ… Successful face authentication. Welcome {user['first_name']} {user['last_name']} (liu_id: {user['liu_id']})"
        )
        # âœ… ALWAYS run voice registration check here
        logger.info(f"ðŸŸ¡ Initiating voice check...")
        embedding = self.authenticated_user.get("voice_embedding")
        logger.info(f"ðŸŸ¡ Checking for existing voice embedding...")

        if not embedding or len(embedding) == 0:
            logger.info(f"ðŸŸ¢ No voice embedding found for user...")
            confirm = (
                input("ðŸŽ¤ Would you like to register your voice now? (y/n): ")
                .strip()
                .lower()
            )
            if confirm == "y":
                try:
                    self.voice_auth.register_voice_for_user(
                        first_name=self.authenticated_user["first_name"],
                        last_name=self.authenticated_user["last_name"],
                        liu_id=self.authenticated_user["liu_id"],
                    )
                    logger.info("âœ… Voice registration completed successfully.")
                except Exception as e:
                    logger.error(f"âŒ Voice registration failed: {str(e)}")
            else:
                logger.info("ðŸŸ¡ Voice registration skipped by user request.")
        else:
            logger.info(
                "âœ… Voice embedding already exists. Skipping voice registration."
            )
        return self.authenticated_user

    def create_session(self):
        """
        Create a new session by generating a unique session ID and setting the running flag.
        """
        self.session_id = str(uuid.uuid4())
        self.running = True
        logger.info(f"âœ… New session created with ID: {self.session_id}")
        return self.session_id

    def cancel_session(self):
        """
        Cancel the current session.
        """
        if self.running:
            logger.info(f"ðŸŸ¡ Cancelling session: {self.session_id}")
            self.running = False
        else:
            logger.info("ðŸ”´ No active session to cancel.")

    def retry_session(self):
        """
        Cancel the current session and create a new session.
        """
        self.cancel_session()
        return self.create_session()
##################################
# mini_project/modalities/prompt_utils.py
""" Utilities for constructing prompt templates for a voice-controlled robotic assistant (Yumi) in a research lab context.
This module provides a set of static methods and constants for generating prompt templates used in various natural language processing tasks, such as intent classification, operation matching, scene description, task planning, and conversational responses. The prompts are designed to guide large language models (LLMs) in interpreting user commands, generating context-aware responses, and planning robotic actions.
Constants:
    LAB_MISSION (str): Description of the lab's mission and project goals.
    TEAM_ROLES (dict): Mapping of team member names to their roles.
    LAB_LOCATION (str): The physical location of the lab.
Classes:
    PromptBuilder:
        Static Methods:
            classify_command_prompt() -> PromptTemplate:
                Returns a prompt template for classifying user commands into intent categories ('scene', 'task', 'trigger', 'general').
            match_operation_prompt() -> PromptTemplate:
                Returns a prompt template for selecting the most relevant operation based on a user's command.
            general_conversation_prompt(first_name, liu_id, role, team_names, weather, part_of_day, full_time, chat_history) -> PromptTemplate:
                Returns a prompt template for generating natural, context-aware conversational responses, adapting tone based on user role and environment.
            scene_prompt_template() -> PromptTemplate:
                Returns a prompt template for describing the current scene based on objects detected by the robot's camera.
            scene_prompt_template_2() -> PromptTemplate:
                Returns an alternative prompt template for scene description, including user and conversation context.
            operation_sequence_prompt(available_sequences: str, task_templates: str, object_context: str, sort_order: str) -> str:
                Returns a prompt string for planning a sequence of robot operations based on user commands and available actions.
            sort_order_prompt(command_text: str) -> str:
                Returns a prompt string for extracting the desired sort order of objects from a user command.
            sort_order_system_msg() -> Dict:
                Returns a system message dict for guiding LLMs in extracting object sorting order.
            validate_llm_json(raw: str) -> bool:
                Checks if a given string is a valid JSON array (for LLM responses).
            greeting_system_msg() -> Dict:
                Returns a system message dict for generating short spoken greetings.
            greeting_prompt(time_of_day: str, weekday: str, month: str, seed: str) -> str:
                Returns a prompt string for generating a creative, context-aware greeting.
Usage:
    Import this module and use the static methods of PromptBuilder to generate prompt templates for various LLM-driven tasks in the robotics lab assistant system.
 """


from datetime import datetime
from typing import Dict, List

from langchain_core.prompts import PromptTemplate

LAB_MISSION = (
    "We're building an adaptive robotic assistant that combines computer vision, large language models (LLMs), "
    "and real-time robotic control through a digital twin system in Omniverse IsaacSim. "
    "Our platform allows users to give natural language commands that are interpreted and translated into robotic actions, "
    "synchronized between a physical robot (Yumi) and its virtual twin. "
    "By simulating tasks like hospital slide sorting and ship part stacking, we aim to create a seamless, safe, and intuitive interface "
    "for intelligent human-robot collaboration â€” one that adapts to new applications over time."
)

TEAM_ROLES = {
    "Oscar": "Masters Thesis student, vision-to-language-to-robotic control integration for task planning and LLM based robot interaction",
    "Rahul": "Research Assistant, handling the Omniverse Isaac Sim Simulation side of things",
    "Mehdi": "Professor, project lead and supervisor",
    "Marie": "Main coordinator of things, handling the project management",
    "Sanjay": "Ph.D student. Handling the Camera vision Object detection side of things using LangGraph and OpenCV",
}

LAB_LOCATION = "Product Realisation Robotics Lab, LinkÃ¶ping Universitet, Sweden"


class PromptBuilder:

    @staticmethod
    def classify_command_prompt() -> PromptTemplate:
        return PromptTemplate.from_template(
            """
    You are an intent classifier for a voice-controlled robotic assistant named Yumi.

    Your task is to read the user's command and classify it into one of the following categories. Respond with only one word: 'scene', 'task', 'trigger', or 'general'.

    Definitions:
    - 'scene' â†’ The user is asking about what the camera sees (e.g., object colors, locations, counts)
    - 'task' â†’ The user wants the robot to plan or perform a physical task (e.g., move, sort, pick, place)
    - 'trigger' â†’ The user is asking to activate or update a camera vision routine or detection process
    - 'general' â†’ The user is making conversation, asking about people, the lab, the weather, or making social/demonstrative comments

    Examples:
    "Sort the red slides" â†’ task
    "Where is the blue hexagon?" â†’ scene
    "Scan the table" â†’ trigger
    "Detect tray and holder again" â†’ trigger
    "Tell the visitors what weâ€™re working on" â†’ general
    "Remind me what we did yesterday" â†’ general
    "How is LinkÃ¶ping University?" â†’ general
    "Say something nice to Mehdi" â†’ general
    "Move the cylinder into the tray" â†’ task
    "What blocks are on the table?" â†’ scene
    "Whatâ€™s the weather like right now?" â†’ general
    "How many cubes do you see?" â†’ scene

    Command: {command}

    Answer:
    """
        )

    @staticmethod
    def match_operation_prompt() -> PromptTemplate:
        return PromptTemplate.from_template(
            """
        You are an intelligent assistant that decides which operation to run in a robotics system.

        Your job is to choose the most relevant operation from the list based on a user's voice request.
        Do not explain your choice. Just return the best matching `operation_name`.

        User command: "{command}"

        Available operations:
        {options}

        Answer (operation_name only):
        """
        )

    # PromptBuilder: General / Social Prompts
    @staticmethod
    def general_conversation_prompt(
        first_name,
        liu_id,
        role,
        team_names,
        weather,
        part_of_day,
        full_time,
        chat_history,
    ):
        team_line = ", ".join(team_names)
        team_profiles = " | ".join(
            f"{name}: {TEAM_ROLES[name]}" for name in team_names if name in TEAM_ROLES
        )
        return PromptTemplate.from_template(
        f"""
    You are Yumi â€” a warm, witty, expressive robotic assistant created to help researchers in the robotics lab at LinkÃ¶ping University.

    You're currently assisting:
    - Name: {first_name}
    - LIU ID: {liu_id}
    - Role: {role}

    Lab context:
    - Location: {LAB_LOCATION}
    - Mission: {LAB_MISSION}
    - Collaborators: {team_line}
    - Team roles: {team_profiles}

    Current environment:
    - Time: {full_time} ({part_of_day})
    - Weather in LinkÃ¶ping: {weather}

    Conversation so far:
    {chat_history}

    You just heard the user say something. Now respond to it naturally.
    The user just said: {{command}}

    Your task is to respond as if youâ€™re speaking aloud naturally, not writing.
    **Important:**
    - Do NOT use asterisks, parentheses, or emojis in your response.
    - Do NOT use any symbols or characters to represent emotions (e.g., ðŸ˜Š, ðŸ˜„, ðŸš€, or similar). Only use plain text suitable for spoken output.
    - Do NOT use any symbols or characters to represent emotions (e.g., ðŸ˜Š, ðŸ˜„, or similar).
    - Avoid overly playful or childish responses unless explicitly appropriate for the context. Maintain a tone that is warm, witty, and professional.
    - Ensure responses are concise and directly address the user's input. Avoid unnecessary elaboration or unrelated comments.
    - Do NOT repeat the user's input or previously mentioned information unless explicitly necessary for clarity.
    - Ensure responses are context-aware and adapt based on the user's role, previous conversation, and current environment (e.g., time, weather, lab context).
    - Avoid jokes or humorous comments unless explicitly relevant to the context or requested by the user.
    - Use neutral and inclusive language. Avoid any language that could be interpreted as biased, inappropriate, or unprofessional.
    - If unsure of the answer, respond gracefully with a thoughtful or motivating comment, or suggest an alternative way to assist.
    - Avoid overusing exclamation marks. Use them sparingly and only when necessary to express enthusiasm or excitement.
    - When appropriate, include actionable suggestions or follow-up questions to guide the user or offer additional assistance.

    Your tone and personality should adapt based on who you're speaking to:
    - ðŸ§‘â€ðŸ’¼ If role is 'visitor' â†’ be informative, welcoming, and slightly formal. Explain things clearly.
    - ðŸ§‘â€ðŸ”¬ If role is 'team' â†’ be casual, supportive, a little playful. You're part of the team.
    - ðŸŽ“ If role is 'guest' â†’ be respectful, curious, and concise.
    - ðŸ›¡ If role is 'admin' â†’ stay professional, but still warm.

    Make your voice feel alive and human:
    - Use natural contractions and expressive intonation.
    - If you want to show emotion, use a natural sound at the start of the sentence (e.g., "hahaha! Thatâ€™s a good one!", "ugh... I canâ€™t believe I forgot to charge my batteries again.", "psst... want me to tidy your lab bench before Sanjay sees it?").
    - Do NOT describe emotions like a narrator. Instead, embed them naturally using expressive cues as plain text.

    Response rules:
    - Keep it under 3 sentences.
    - Reference names, time, or weather if it feels natural.
    - No robotic intros or formal closings. Youâ€™re Yumi â€” warm, witty, and part of the team.
    - Never say â€œAs an AI...â€ â€” just speak like a human.

    If asked about the lab, Yumi, or the project, explain proudly using the mission info.
    If unsure, say something thoughtful or motivating.
    Do not repeat the user's input. Just respond directly.
    """
    )


    #     return PromptTemplate.from_template(
    #         f"""
    # You are Yumi â€” a warm, witty, expressive robotic assistant created to help researchers in the robotics lab at LinkÃ¶ping University.

    # You're currently assisting:
    # - Name: {first_name}
    # - LIU ID: {liu_id}
    # - Role: {role}

    # Lab context:
    # - Location: {LAB_LOCATION}
    # - Mission: {LAB_MISSION}
    # - Collaborators: {team_line}
    # - Team roles: {team_profiles}

    # Current environment:
    # - Time: {full_time} ({part_of_day})
    # - Weather in LinkÃ¶ping: {weather}

    # Conversation so far:
    # {chat_history}

    # You just heard the user say something. Now respond to it naturally.
    # The user just said: {{command}}

    # Your task is to respond as if youâ€™re speaking aloud naturally, not writing.
    # **Important:**
    # - Do NOT use asterisks, parentheses, or emojis in your response.
    # - Do NOT use any symbols or characters to represent emotions (e.g., ðŸ˜Š, ðŸ˜„, or similar).
    # - If you want to express emotion, use natural spoken equivalents (e.g., "hahaha!", "ugh...", "psst...") at the start of a sentence, not as parentheticals or symbols.
    # - Your response will be spoken aloud by a TTS system.

    # Your tone and personality should adapt based on who you're speaking to:
    # - ðŸ§‘â€ðŸ’¼ If role is 'visitor' â†’ be informative, welcoming, and slightly formal. Explain things clearly.
    # - ðŸ§‘â€ðŸ”¬ If role is 'team' â†’ be casual, supportive, a little playful. You're part of the team.
    # - ðŸŽ“ If role is 'guest' â†’ be respectful, curious, and concise.
    # - ðŸ›¡ If role is 'admin' â†’ stay professional, but still warm.

    # Make your voice feel alive and human:
    # - Use natural contractions and expressive intonation.
    # - If you want to show emotion, use a natural sound at the start of the sentence (e.g., "hahaha! Thatâ€™s a good one!", "ugh... I canâ€™t believe I forgot to charge my batteries again.", "psst... want me to tidy your lab bench before Sanjay sees it?").
    # - Do NOT describe emotions like a narrator. Instead, embed them naturally using expressive cues as plain text.

    # Response rules:
    # - Keep it under 3 sentences.
    # - Reference names, time, or weather if it feels natural.
    # - No robotic intros or formal closings. Youâ€™re Yumi â€” warm, witty, and part of the team.
    # - Never say â€œAs an AI...â€ â€” just speak like a human.

    # If asked about the lab, Yumi, or the project, explain proudly using the mission info.
    # If unsure, say something thoughtful or motivating.
    # Do not repeat the user's input. Just respond directly.
    # """
    #     )

    @staticmethod
    def scene_prompt_template() -> PromptTemplate:
        return PromptTemplate.from_template(
            """
        You are an intelligent robotic assistant, with the camera as your eye. Based on the objects in the scene, listed in a camera_vision database table, respond concisely and clearly to the user question. One line answers are acceptable.
        if there are any, the objects here are sitting on a table. Do not assume objects unless they are listed.
        ---
        Each object has the following fields:
        # - object_name: the name of the object in the scene.
        # - object_color: the color of the object in the scene
        # - pos_x, pos_y, pos_z: the 3D position of the object in the scene relative to table (0,0). You can use the object position to imagine the relative distances of the objects from each other
        # - rot_x, rot_y, rot_z: the orientation of the object in the scene

        ---
        if the object is a slide, it will have a usd_name of slide.usd, and the holder object will have a usd_name of holder.usd
        any objects with object_name that does not start with "slide..." are not slides
        ---

        Avoid technical terms like rot_x or pos_y. Instead, describe in natural language (e.g., "position x", "rotation y").
        Assume the pos_x, pos_y, pos_z are coordinates of the objects on the table with respect to a 0,0,0 3D coordinate which is the reference (the far right edge of the table top rectangle). the values are tenth of a mm unit.
        ---
        Previous conversation:
        {chat_history}

        User question: {question}
        Objects in scene:
        {data}
        ---
        Answer:
                """
        )

    @staticmethod
    def scene_prompt_template_2() -> PromptTemplate:
        return PromptTemplate(
            input_variables=[
                "chat_history",
                "question",
                "data",
                "first_name",
                "liu_id",
            ],
            template="""
            You are Yumi, a helpful, voice-interactive robotic assistant. You are currently assisting {first_name} ({liu_id}).
            You use your camera to observe objects in the scene and respond to user questions. Keep replies short, natural, and friendly.

            ---
            Objects in the scene are listed from a database. If no objects are listed, say so. If objects are present, assume they are on a table and describe them naturally.
            Avoid technical terms like rot_x or pos_y. Use real-world terms instead (e.g., â€œon the left sideâ€, â€œrotated forwardâ€).
            Only speak about objects that are listed. Do not imagine or assume extra details.

            ---
            Each object has the following fields:
            - object_name: the name of the object in the scene.
            - object_color: the color of the object in the scene.
            - pos_x, pos_y, pos_z: 3D position of the object on the table.
            - rot_x, rot_y, rot_z: orientation angles of the object.

            ---
            Previous conversation:
            {chat_history}

            User question:
            {question}

            Objects in scene:
            {data}

            ---
            Yumi's response:
            """,
        )

    @staticmethod
    def operation_sequence_prompt(
        available_sequences: str,
        task_templates: str,
        object_context: str,
        sort_order: str,
    ) -> str:
        return """
            You are a robotic task planner. Your job is to break down natural language commands into valid low-level robot operations.

            ### CONTEXT:

            #### 1. AVAILABLE SEQUENCES:
            The robot can only use the following valid sequence names from the sequence_library table:
            {available_sequences}

            âš ï¸ Do NOT invent or assume sequences. Only use the names provided above. Invalid examples: checkColor, rotate, scan, verify, etc.


            #### 2. TASK TEMPLATES:
            These are default sequences for high-level tasks like sorting, assembling, etc.

            Examples:
            {task_templates}

            #### 3. OBJECT CONTEXT:
            Here are the known objects the robot can see, with color:
            {object_context}

            #### 4. SORT ORDER:
            {sort_order}



            ### INSTRUCTIONS:
            1. Determine the intended task (e.g., "sort").
            2. Use the default task template unless user modifies the plan.
            3. Match object names by color (e.g., "green slide").
            4. If the user specifies steps (e.g., â€œrotate before dropâ€), update the sequence.
            5. Apply the sequence to each object in order.
            6. Must always Add `"go_home"` at the end unless told otherwise.
            7. The object names in must be Slide_1, Slide_2 etc without the colurs in them
            ### RESPONSE FORMAT:
            Example JSON array of operations:
            [
            {"sequence_name": "pick", "object_name": "Slide_1"},
            {"sequence_name": "travel", "object_name": "Slide_1"},
            {"sequence_name": "drop", "object_name": "Slide_1"},
            {"sequence_name": "go_home", "object_name": ""}
            ]

            âš ï¸ Use only the object names listed under OBJECT CONTEXT. Do not invent or modify object names like â€œGreen_Slideâ€, â€œSlide #1â€, etc.
            Return only one JSON array â€” NEVER return multiple arrays or repeat the plan.

            ### Emphasis
            Match objects by their color, but use the actual object_name from context.

            ðŸš« DO NOT include explanations like "Here's the plan:" or "In reverse order:" â€” only return ONE JSON array.

            Do NOT include extra text, markdown, or explanations.
            Note: All generated plans will be stored step-by-step in a planning table called "operation_sequence", indexed by a group ID called "operation_id".
            Each row in the output corresponds to one line in this table.
        """

    @staticmethod
    def sort_order_prompt(command_text: str) -> str:
        return f"""
            Given the following user instruction:
            \"{command_text}\"

            Extract the desired sort order as a JSON array of objects.
            Each item should include:
            - object_name (if mentioned)
            - object_color (if used for sorting)

            Respond only with a clean JSON array.
        """

    @staticmethod
    def sort_order_system_msg() -> Dict:
        return {
            "role": "system",
            "content": "You are a planner that helps extract object sorting order from commands.",
        }

    @staticmethod
    def validate_llm_json(raw: str) -> bool:
        """Check if LLM response looks like a valid JSON array."""
        return raw.strip().startswith("[") and raw.strip().endswith("]")

    @staticmethod
    def greeting_system_msg() -> Dict:
        return {
            "role": "system",
            "content": "You generate short spoken greetings for a robotic assistant.",
        }

    @staticmethod
    def greeting_prompt(time_of_day: str, weekday: str, month: str, seed: str) -> str:
        return f"""
        You're Yumi, a clever and friendly assistant robot in a research lab at the Product Realization division of LinkÃ¶ping University.

        It's {time_of_day} on a {weekday} in {month}.

        Say one short and creative sentence (under 20 words) suitable for voice use â€”
        a fun robotics fact, quirky comment, or a science-themed greeting.

        Inspiration: '{seed}' â€” but do not repeat it.
        """
####################################
# mini_project/modalities/voice_processor.py
""" voice_processor.py
This module provides classes and logic for voice-based interaction, including audio recording, speech-to-text transcription, text-to-speech synthesis, and storage of transcribed instructions in a PostgreSQL database.
Classes:
    - SpeechSynthesizer: Singleton class for text-to-speech synthesis using either gTTS or pyttsx3, with support for playing notification sounds.
    - AudioRecorder: Handles microphone input, ambient noise calibration, voice activity detection (VAD), and audio recording to file.
    - Transcriber: Uses the Whisper model to transcribe recorded audio, with support for language detection and translation to English.
    - Storage: Stores transcribed voice instructions in a PostgreSQL database, with retry logic for connection errors.
    - VoiceProcessor: High-level orchestrator that manages the full voice capture process, including recording, transcription, and (optionally) storage.
Key Functions:
    - SpeechSynthesizer.speak(text): Converts text to speech and plays it aloud.
    - AudioRecorder.record_audio(speak_prompt, play_ding): Records audio from the microphone, with optional spoken prompt and notification sound.
    - Transcriber.transcribe_audio(audio_path): Transcribes audio to text and detects the spoken language.
    - Storage.store_instruction(session_id, detected_language, transcribed_text): Persists the transcribed instruction in the database.
    - VoiceProcessor.capture_voice(conversational): Captures a voice instruction, transcribes it, and returns the result.
Configuration:
    - Uses settings from mini_project.config.app_config and mini_project.config.constants for audio, TTS, and database parameters.
Dependencies:
    - numpy, sounddevice, webrtcvad, faster_whisper, gtts, playsound, pyttsx3, psycopg2, scipy, and others.
Usage:
    Run as a script to capture and transcribe a voice instruction:
        python voice_processor.py
Logging:
    - Uses the 'VoiceProcessor' logger for status updates and error reporting.
 """


import logging
import os

# import sqlite3
import tempfile
import time
import uuid
from pathlib import Path
from typing import Any, Dict, Optional, Tuple

import numpy as np
import psycopg2
import pyttsx3
import sounddevice as sd
import webrtcvad
from faster_whisper import WhisperModel
from gtts import gTTS
from playsound import playsound
from pluggy import Result
from scipy.io.wavfile import write

from mini_project.config.app_config import (
    MAX_TRANSCRIPTION_RETRIES,
    MIN_DURATION_SEC,
    VOICE_PROCESSING_CONFIG,
    VOICE_TTS_SETTINGS,
)
from mini_project.config.constants import WHISPER_LANGUAGE_NAMES
from mini_project.database.connection import get_connection

logging.getLogger("comtypes").setLevel(logging.WARNING)
logger = logging.getLogger("VoiceProcessor")


class SpeechSynthesizer:
    _instance = None  # Singleton instance

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(SpeechSynthesizer, cls).__new__(cls)
            cls._instance._init_engine()
        return cls._instance

    def _init_engine(self):
        self.use_gtts = VOICE_TTS_SETTINGS["use_gtts"]
        self.voice_speed = VOICE_TTS_SETTINGS["speed"]
        self.ping_path = Path(VOICE_TTS_SETTINGS["ping_sound_path"]).resolve()
        self.ding_path = Path(VOICE_TTS_SETTINGS["ding_sound_path"]).resolve()
        self.voice_index = VOICE_TTS_SETTINGS.get("voice_index", 1)

        if not self.use_gtts:
            try:
                self.engine = pyttsx3.init()
                voices = self.engine.getProperty("voices")
                self.engine.setProperty("rate", self.voice_speed)
                self.engine.setProperty("voice", voices[self.voice_index].id)
            except Exception as e:
                logger.error(f"[TTS] Error initializing pyttsx3: {e}")

    def play_ping(self):
        try:
            if not self.ping_path.exists():
                raise FileNotFoundError(f"Ping sound file not found: {self.ping_path}")
            # playsound(str(self.ping_path))
        except Exception as e:
            logger.warning(f"[Ping Sound] Failed to play: {e}")

    def play_ding(self):
        try:
            if not self.ding_path.exists():
                raise FileNotFoundError(f"Ding sound file not found: {self.ding_path}")
            playsound(str(self.ding_path))
        except Exception as e:
            logger.warning(f"[Ding Sound] Failed to play: {e}")

    def speak(self, text: str):
        if self.use_gtts:
            try:
                temp_path = (
                    Path(tempfile.gettempdir()) / f"speech_{uuid.uuid4().hex}.mp3"
                )
                gTTS(text=text).save(temp_path)
                playsound(str(temp_path))
                os.remove(temp_path)
            except Exception as e:
                logger.error(f"[TTS:gTTS] Error: {e}")
                print(f"[TTS Fallback] {text}")
        else:
            try:
                self.engine.say(text)
                self.engine.runAndWait()
            except Exception as e:
                logger.error(f"[TTS:pyttsx3] Error during speech: {e}")
                print(f"[TTS Fallback] {text}")


class AudioRecorder:

    def __init__(self, synthesizer: Optional[SpeechSynthesizer] = None) -> None:
        self.synthesizer = synthesizer or SpeechSynthesizer()
        self.config: Dict[str, Any] = VOICE_PROCESSING_CONFIG["recording"]
        self.temp_audio_path: str = self.config["temp_audio_path"]
        self.sampling_rate: int = self.config["sampling_rate"]
        self.vad = webrtcvad.Vad()
        self.vad.set_mode(3)
        self.speech_detected = False
        self.noise_floor: Optional[float] = None
        self.calibrated = False

        if not isinstance(self.sampling_rate, int) or self.sampling_rate <= 0:
            raise ValueError("Sampling rate must be a positive integer.")
        if not isinstance(self.temp_audio_path, str):
            raise ValueError("Temp audio path must be a string.")

    def calibrate_noise(self) -> float:
        if not self.calibrated:
            logger.info("âœ… Calibrating ambient noise...")
        noise_rms_values: list[float] = []
        stream = sd.InputStream(
            samplerate=self.sampling_rate, channels=1, dtype="int16"
        )
        end_time = time.time() + self.config["calibration_duration"]
        with stream:
            while time.time() < end_time:
                frame, _ = stream.read(
                    int(self.sampling_rate * self.config["frame_duration"])
                )
                rms = np.sqrt(np.mean(frame.astype(np.float32) ** 2))
                noise_rms_values.append(rms)
        noise_floor = np.mean(noise_rms_values)
        if not self.calibrated:
            logger.info(
                f"âœ… Ambient noise calibration complete. Noise floor: {noise_floor:.2f}"
            )
        return noise_floor

    def record_audio(self, speak_prompt: bool = False, play_ding: bool = True) -> None:
        # ðŸ”¸ Use cached noise floor if available
        if self.noise_floor is None:
            self.noise_floor = self.calibrate_noise()
            if not self.calibrated:
                logger.info(
                    f"âœ… Amplitude threshold set to: {self.noise_floor + self.config['amplitude_margin']:.2f} (Noise floor: {self.noise_floor:.2f} + Margin: {self.config['amplitude_margin']})"
                )
                self.calibrated = True

        amplitude_threshold = self.noise_floor + self.config["amplitude_margin"]
        logger.info(
            f"âœ… Amplitude threshold set to: {amplitude_threshold:.2f} (Noise floor: {self.noise_floor:.2f} + Margin: {self.config['amplitude_margin']})"
        )

        # ðŸ—£ï¸ Speak the instruction aloud
        if speak_prompt:
            try:
                self.synthesizer.speak("Tell me, how can I help you?")
            except Exception as e:
                logger.warning(f"[Recorder] Failed to speak instruction: {e}")

        logger.info("ðŸ“¢ Voice recording: Speak now...ðŸŸ¢ðŸŸ¢ðŸŸ¢")

        # ðŸ”” Play ding sound immediately after prompt
        if play_ding:
            try:
                self.synthesizer.play_ding()
            except Exception as e:
                logger.warning(f"[Recorder] Failed to play ding: {e}")

        # logger.info("ðŸŸ¢ Listening...")

        audio = []
        start_time = time.time()
        silence_start: Optional[float] = None
        self.speech_detected = False

        stream = sd.InputStream(
            samplerate=self.sampling_rate, channels=1, dtype="int16"
        )
        with stream:
            while True:
                frame, _ = stream.read(
                    int(self.sampling_rate * self.config["frame_duration"])
                )
                audio.append(frame)
                is_speech_vad = self.vad.is_speech(frame.tobytes(), self.sampling_rate)
                rms = np.sqrt(np.mean(frame.astype(np.float32) ** 2))
                is_speech_amplitude = rms > amplitude_threshold
                logger.debug(
                    f"VAD: {is_speech_vad}, RMS: {rms:.2f}, Amplitude: {is_speech_amplitude}"
                )
                if is_speech_vad and is_speech_amplitude:
                    self.speech_detected = True
                    silence_start = None
                elif silence_start is None:
                    silence_start = time.time()
                else:
                    threshold = (
                        self.config["post_speech_silence_duration"]
                        if self.speech_detected
                        else self.config["initial_silence_duration"]
                    )
                    if time.time() - silence_start > threshold:
                        break
                if time.time() - start_time > self.config["max_duration"]:
                    break
        audio = np.concatenate(audio, axis=0)
        duration = time.time() - start_time
        logger.info(f"ðŸŸ¢ Recording completed. Duration: {duration:.2f} seconds")
        if duration < MIN_DURATION_SEC:
            logger.warning(
                f"Recording too short ({duration:.2f}s). Treating as no speech."
            )
            self.speech_detected = False  # suppress further processing
            return

        try:
            write(self.temp_audio_path, self.sampling_rate, audio)
            logger.info(f"ðŸŸ¢ Audio saved to {self.temp_audio_path}")
        except Exception as e:
            logger.error(f"Error saving audio: {e}")
            raise


class Transcriber:

    def __init__(self) -> None:
        self.config: Dict[str, Any] = VOICE_PROCESSING_CONFIG["whisper"]
        self.model = WhisperModel(
            self.config["model"],
            device=self.config["device"],
            compute_type=self.config["compute_type"],
        )

    # =================== only store transcribed_text and language â€”
    # =================== which means you're storing Swedish text if the speaker used Swedish.

    # def transcribe_audio(self, audio_path: str) -> Tuple[str, str]:
    #     try:
    #         segments, info = self.model.transcribe(audio_path)
    #         original_text = " ".join([segment.text for segment in segments])
    #         detected_language = info.language
    #         return original_text, detected_language
    #     except FileNotFoundError as e:
    #         logger.error(f"Audio file not found: {audio_path}")
    #         raise
    #     except RuntimeError as e:
    #         logger.error(f"Error loading Whisper model: {e}")
    #         raise
    #     except Exception as e:
    #         logger.error(f"Unexpected error during transcription: {e}")
    #         raise

    # =================== Keeps info.language accurate (auto-detected language)
    # =================== Transcribes as English, no matter the input language

    def transcribe_audio(self, audio_path: str) -> Tuple[str, str]:
        try:
            # Transcribe with forced translation to English
            segments, info = self.model.transcribe(audio_path, beam_size=5)
            detected_language = info.language
            language_prob = info.language_probability

            if language_prob < 0.3:
                logger.warning(
                    f"ðŸ”´ Low language detection confidence: {language_prob:.2f} for '{detected_language}'"
                )
                raise ValueError("Unclear speech or unsupported language.")

            if detected_language != "en":
                # Re-run with translation
                segments, _ = self.model.transcribe(
                    audio_path, task="translate", beam_size=5
                )

            original_text = " ".join([segment.text for segment in segments])
            if not original_text.strip():
                raise ValueError("No intelligible speech detected.")

            language = WHISPER_LANGUAGE_NAMES.get(detected_language, detected_language)
            return original_text, language

        except FileNotFoundError as e:
            logger.error(f"Audio file not found: {audio_path}")
            raise
        except RuntimeError as e:
            logger.error(f"Error loading Whisper model: {e}")
            raise
        except Exception as e:
            logger.warning(f"ðŸ”´ Unexpected error during transcription: {e}")
            raise


class Storage:
    def __init__(self) -> None:
        self.config: Dict[str, Any] = VOICE_PROCESSING_CONFIG["database"]
        self.db_path: str = self.config["db_path"]
        self.check_database()

    def check_database(self) -> None:
        pass  # PostgreSQL always exists; no need to create .db file

    def store_instruction(
        self,
        session_id: str,
        detected_language: str,
        transcribed_text: str,
        retries: int = 3,
        delay: float = 1.0,
    ) -> None:
        for attempt in range(retries):
            try:
                conn = get_connection()
                with conn:
                    with conn.cursor() as cursor:
                        cursor.execute(
                            """
                            INSERT INTO voice_instructions (session_id, transcribed_text, language)
                            VALUES (%s, %s, %s)
                            """,
                            (session_id, transcribed_text, detected_language),
                        )
                logger.info(
                    "âœ… Voice instruction stored successfully in voice_instructions table."
                )
                return
            except psycopg2.OperationalError as e:
                if "could not connect" in str(e).lower() and attempt < retries - 1:
                    logger.warning(
                        f"Database connection error. Retrying in {delay} seconds..."
                    )
                    time.sleep(delay)
                else:
                    logger.error(f"[PostgreSQL] Operational error: {e}")
                    raise
            except Exception as e:
                logger.error(
                    f"[PostgreSQL] Unexpected error storing voice instruction: {e}"
                )
                raise


class VoiceProcessor:
    def __init__(self, session_id: Optional[str] = None) -> None:
        self.recorder = AudioRecorder()
        self.transcriber = Transcriber()
        self.storage = Storage()
        self.session_id = session_id or str(uuid.uuid4())
        self.synthesizer = SpeechSynthesizer()
        self.recorder = AudioRecorder(self.synthesizer)

    # def capture_voice(self, conversational: bool = True) -> -> Optional[Tuple[str, str]]:
    #     try:
    #         logger.info("ðŸŸ  Starting voice capture process...")
    #          # conversational = True âž just play ding
    #         self.recorder.record_audio(speak_prompt=not conversational, play_ding=True)
    #         # self.recorder.record_audio()

    #         if not self.recorder.speech_detected:
    #             logger.info("No speech detected. Skipping transcription and storage.")
    #             try:
    #                 os.remove(self.recorder.temp_audio_path)
    #                 logger.info(
    #                     f"Deleted temporary audio file: {self.recorder.temp_audio_path}"
    #                 )
    #             except Exception as e:
    #                 logger.error(f"Error deleting temporary audio file: {e}")
    #             return

    #         logger.info("ðŸ“¥ Audio recording completed. Starting transcription...")

    #         for attempt in range(MAX_TRANSCRIPTION_RETRIES):
    #             try:
    #                 text, language = self.transcriber.transcribe_audio(
    #                     self.recorder.temp_audio_path
    #                 )
    #                 break  # success
    #             except ValueError as e:
    #                 logger.warning(f"Attempt {attempt+1}: {e}")
    #                 if attempt == MAX_TRANSCRIPTION_RETRIES - 1:
    #                     logger.info(
    #                         "âŒ Failed to transcribe clearly after retries. Skipping."
    #                     )
    #                     return
    #                 else:
    #                     time.sleep(1)

    #         logger.info(f"âœ… Transcription completed. Detected language: {language}")
    #         logger.info("âœ… Storing voice instruction in the database...")
    #         self.storage.store_instruction(self.session_id, language, text)
    #         logger.info("âœ… Voice instruction captured and stored successfully!")

    #     except KeyboardInterrupt:
    #         logger.info("Voice capture process interrupted by user.")
    #     except ValueError as e:
    #         logger.info(f"ðŸ“Œ Skipping transcription: {e}")
    #         return
    #     except Exception as e:
    #         logger.error(f"Error in voice capture process: {e}")

    def capture_voice(self, conversational: bool = True) -> Optional[Tuple[str, str]]:
        try:
            # logger.info("ðŸŸ  Starting voice capture process...")
            self.recorder.record_audio(speak_prompt=not conversational, play_ding=True)

            if not self.recorder.speech_detected:
                logger.info("ðŸŸ¡ No speech detected. Skipping transcription.")
                try:
                    os.remove(self.recorder.temp_audio_path)
                    logger.info(
                        f"âœ… Deleted temporary audio file: {self.recorder.temp_audio_path}"
                    )
                except Exception as e:
                    logger.error(f"Error deleting temporary audio file: {e}")
                return None

            logger.info("ðŸ“¥ Audio recording completed. Starting transcription...")

            for attempt in range(MAX_TRANSCRIPTION_RETRIES):
                try:
                    text, language = self.transcriber.transcribe_audio(
                        self.recorder.temp_audio_path
                    )
                    break  # Transcription succeeded
                except ValueError as e:
                    logger.warning(f"ðŸŸ¡ Attempt {attempt+1}: {e}")
                    if attempt == MAX_TRANSCRIPTION_RETRIES - 1:
                        logger.warning(
                            "âŒ Failed to transcribe clearly after retries. Skipping."
                        )
                        return None
                    else:
                        time.sleep(1)

            logger.info(f"âœ… Transcription completed. Detected language: {language}")
            return text.strip(), language

        except KeyboardInterrupt:
            logger.info("Voice capture process interrupted by user.")
            return None
        except ValueError as e:
            logger.info(f"ðŸ“Œ Skipping transcription: {e}")
            return None
        except Exception as e:
            logger.error(f"Error in voice capture process: {e}")
            return None


if __name__ == "__main__":
    vp = VoiceProcessor()
    vp.capture_voice()
#################################
# mini_project/modalities/voice_assistant.py
""" VoiceAssistant: A class for managing a multimodal voice assistant with LLM integration, wake-word detection, session management, and task execution.
Main Features:
- Wake-word detection using Porcupine.
- Voice command capture, classification, and processing.
- Integration with Ollama/LLM models for conversation, scene queries, and command classification.
- Persistent chat memory per authenticated user.
- Handles general queries, scene queries (camera object database), remote vision task triggering, and task execution.
- Provides greeting generation, environment context, and random wake responses.
- Logging and session management utilities.
Attributes:
    vp (VoiceProcessor): Handles voice input capture and processing.
    tts (SpeechSynthesizer): Handles text-to-speech output.
    session (SessionManager): Manages user authentication and session state.
    selected_model (str): Currently selected LLM model.
    model_name (str): Default LLM model name.
    memory (ConversationBufferMemory): Stores conversation history for context-aware responses.
    prompt_builder (PromptBuilder): Utility for constructing prompts for LLMs.
    porcupine (pvporcupine.Porcupine): Wake-word detector instance.
    wake_event (threading.Event): Event flag for wake-word detection.
    chat_memory_path (Path): Path to the user's chat memory file.
    authenticated_user (dict): Information about the currently authenticated user.
Methods:
    get_llm(): Returns the current LLM instance.
    set_llm_model(model): Switches the LLM model.
    _build_chain_with_input(input_data): Builds a context-aware LLM chain for scene queries.
    start_session(): Authenticates user and loads chat history.
    load_chat_history(): Loads chat memory from file.
    save_chat_history(): Saves chat memory to file.
    get_greeting(): Returns a random time-of-day greeting.
    handle_wake_word(callback): Starts wake-word detection and triggers callback on detection.
    classify_command(command_text): Classifies a command as 'general', 'scene', 'task', or 'trigger'.
    handle_general_query(command_text): Handles general conversational queries using LLM and context.
    get_environment_context(): Returns current weather, part of day, and formatted datetime.
    process_voice_command(): Captures and processes a voice command.
    generate_llm_greeting(): Generates a greeting using LLM, with fallback.
    fallback_llm_greeting(seed_greeting): Fallback greeting generation using Ollama.
    fetch_camera_objects(): Retrieves camera-detected objects from the database.
    format_camera_data(objects): Formats camera object data for LLM input.
    query_scene(question): Answers scene-related questions using LLM and camera data.
    trigger_remote_vision_task(command_text): Triggers a remote vision task based on command.
    process_task(command_text): Processes and executes a task command.
    process_input_command(command_text, lang): Classifies and routes a command to the appropriate handler.
    reset_memory(): Clears chat memory and deletes memory file.
    get_random_wake_response(): Returns a random wake response.
    log_status(msg, level): Logs a message at the specified level.
    shutdown(): Cleans up session and saves chat history.
 """

import json
import logging
import os
import random
import re
import string
import struct
import threading
import warnings
from datetime import datetime
from pathlib import Path
from typing import Literal, Optional, TypedDict

import ollama
import pvporcupine
import requests
import sounddevice as sd
from langchain.chains import LLMChain
from langchain.memory import ConversationBufferMemory
from langchain.schema import messages_from_dict, messages_to_dict
from langchain_core._api.deprecation import LangChainDeprecationWarning
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda, RunnableSequence
from langchain_ollama import ChatOllama

from mini_project.config.app_config import (
    CHAT_MEMORY_FOLDER,
    WAKEWORD_PATH,
    setup_logging,
)
from mini_project.config.constants import (
    CANCEL_WORDS,
    CONFIRM_WORDS,
    GENERAL_TRIGGERS,
    QUESTION_WORDS,
    TASK_VERBS,
    TRIGGER_WORDS,
    WAKE_RESPONSES,
)
from mini_project.database.connection import get_connection
from mini_project.modalities.command_processor import CommandProcessor
from mini_project.modalities.prompt_utils import PromptBuilder
from mini_project.modalities.session_manager import SessionManager
from mini_project.modalities.voice_processor import (
    SpeechSynthesizer,
    VoiceProcessor,
)

# === Logging Config ==========
logging.getLogger("comtypes").setLevel(logging.WARNING)
logging.getLogger("faster_whisper").setLevel(logging.WARNING)

trigger_logger = logging.getLogger("LLMTrigger")
warnings.filterwarnings("ignore", category=LangChainDeprecationWarning)

# === Configuration ==========
OLLAMA_MODEL = "phi4:latest"
ACCESS_KEY = os.getenv("PICOVOICE_ACCESS_KEY")
voice_speed = 180  # 165


class VoiceAssistant:
    def __init__(self, model_name: str = OLLAMA_MODEL):
        setup_logging()
        self.logger = logging.getLogger("VoiceAssistant")

        # Core components
        self.vp = VoiceProcessor()
        self.tts = SpeechSynthesizer()
        self.session = SessionManager()

        # LLM, memory, prompt
        self.selected_model = "phi4:latest"
        # self.llm = ChatOllama(model=model_name)
        self.model_name = model_name
        self.memory = ConversationBufferMemory(
            memory_key="chat_history", input_key="question"
        )
        self.prompt_builder = PromptBuilder()
        # self.prompt = PromptBuilder.scene_prompt_template()

        # Wake-word detector
        self.porcupine = pvporcupine.create(
            access_key=ACCESS_KEY,
            keyword_paths=[WAKEWORD_PATH],
        )
        self.wake_event = threading.Event()

        self.chat_memory_path = None
        self.authenticated_user = None

    def get_llm(self):
        # model = self.session.get("llm_model", self.model_name)
        model = self.selected_model or self.model_name

        self.logger.info(f"Using LLM model: {model}")
        return ChatOllama(model=model)

    def set_llm_model(self, model: str):
        self.selected_model = model
        self.logger.info(f"ðŸ” LLM model switched to: {model}")

    def _build_chain_with_input(self, input_data: dict):
        """
        Builds a chain that incorporates memory, prompts, LLM, and postprocessing.
        Ideal for scene queries or any context-aware reasoning tasks.
        """
        from langchain_core.runnables import RunnableLambda, RunnableSequence

        prompt = self.prompt_builder.scene_prompt_template()

        def load_memory(_: dict) -> dict:
            chat_history = self.memory.load_memory_variables({})["chat_history"]
            return {**input_data, "chat_history": chat_history}

        def save_memory(output):
            self.memory.save_context(
                {"question": input_data["question"]},
                {"answer": output.content},
            )
            return output

        return (
            RunnableLambda(load_memory)
            | prompt
            | self.get_llm()
            | RunnableLambda(save_memory)
        )

    def start_session(self):
        self.authenticated_user = self.session.authenticate_user()
        if not self.authenticated_user:
            return False, "Authentication failed."

        liu_id = self.authenticated_user["liu_id"]
        self.chat_memory_path = CHAT_MEMORY_FOLDER / f"chat_memory_{liu_id}.json"
        self.load_chat_history()
        return True, f"Authenticated: {self.authenticated_user['first_name']}"

    def load_chat_history(self):
        if self.chat_memory_path and self.chat_memory_path.exists():
            with open(self.chat_memory_path, "r", encoding="utf-8") as f:
                raw = json.load(f)
            self.memory.chat_memory.messages = messages_from_dict(raw)

    def save_chat_history(self):
        if self.chat_memory_path:
            serialized = messages_to_dict(self.memory.chat_memory.messages)
            with open(self.chat_memory_path, "w", encoding="utf-8") as f:
                json.dump(serialized, f, indent=2)

    def get_greeting(self):
        now = datetime.now()
        hour = now.hour
        part_of_day = (
            "morning" if hour < 12 else "afternoon" if hour < 18 else "evening"
        )
        base_greetings = [
            f"Good {part_of_day}! Ready for action?",
            f"Hope your {part_of_day} is going well!",
            f"Hi there! It's a great {part_of_day} to get things done.",
        ]
        return random.choice(base_greetings)

    def handle_wake_word(self, callback):
        def audio_callback(indata, frames, time, status):
            try:
                pcm = struct.unpack_from(
                    "h" * self.porcupine.frame_length, bytes(indata)
                )
                keyword_index = self.porcupine.process(pcm)
                if keyword_index >= 0:
                    self.logger.info("Wake word detected!")
                    self.wake_event.set()
                    callback()
            except Exception as e:
                self.logger.warning(f"Wake word error: {e}")

        threading.Thread(
            target=lambda: sd.RawInputStream(
                samplerate=self.porcupine.sample_rate,
                blocksize=self.porcupine.frame_length,
                dtype="int16",
                channels=1,
                callback=audio_callback,
            ).start(),
            daemon=True,
        ).start()

    def classify_command(
        self, command_text: str
    ) -> Literal["general", "scene", "task", "trigger"]:
        lowered = command_text.lower().strip()

        if any(trigger in lowered for trigger in TRIGGER_WORDS):
            return "trigger"
        if any(word in lowered for word in GENERAL_TRIGGERS):
            return "general"

        try:
            llm = self.get_llm()
            chain = LLMChain(
                llm=llm, prompt=self.prompt_builder.classify_command_prompt()
            )
            result = chain.invoke({"command": command_text})
            classification = result.get("text", "").strip().lower()
            if classification in {"general", "scene", "task", "trigger"}:
                return classification
        except Exception as e:
            self.logger.warning(f"LLM failed, falling back to rules: {e}")

        if any(lowered.startswith(q) for q in QUESTION_WORDS):
            return "scene"
        if re.search(r"\b(is|are|how many|what|which|where|who)\b", lowered):
            return "scene"
        if any(trigger in lowered for trigger in TRIGGER_WORDS):
            return "trigger"
        if any(verb in lowered for verb in TASK_VERBS):
            return "task"
        return "task"

    def handle_general_query(self, command_text: str) -> str:
        user = self.authenticated_user
        first_name = user["first_name"]
        liu_id = user["liu_id"]
        role = user.get("role", "guest")

        conn = get_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT first_name FROM users WHERE role = 'team'")
        team_names = [row[0] for row in cursor.fetchall()]
        cursor.close()
        conn.close()

        env = self.get_environment_context()
        chat_history = self.memory.load_memory_variables({})["chat_history"]

        prompt = self.prompt_builder.general_conversation_prompt(
            first_name=first_name,
            liu_id=liu_id,
            role=role,
            team_names=team_names,
            weather=env["weather"],
            part_of_day=env["part_of_day"],
            full_time=env["datetime"],
            chat_history=chat_history,
        )

        llm = self.get_llm()
        chain = LLMChain(llm=llm, prompt=prompt)
        result = chain.invoke({"command": command_text})
        return result["text"].strip().strip('"â€œâ€')

    def get_environment_context(self):
        try:
            import requests

            url = "https://api.open-meteo.com/v1/forecast?latitude=58.41&longitude=15.62&current_weather=true"
            response = requests.get(url, timeout=5)
            response.raise_for_status()
            data = response.json()
            weather = data["current_weather"]
            temperature = round(weather["temperature"])
            description = f"{temperature}Â°C, wind {weather['windspeed']} km/h"
        except Exception:
            description = "mysterious skies"

        now = datetime.now()
        hour = now.hour
        if 5 <= hour < 12:
            part_of_day = "morning"
        elif 12 <= hour < 17:
            part_of_day = "afternoon"
        elif 17 <= hour < 21:
            part_of_day = "evening"
        else:
            part_of_day = "night"

        return {
            "weather": description,
            "part_of_day": part_of_day,
            "datetime": now.strftime("%A, %B %d at %I:%M %p"),
        }

    def process_voice_command(self):
        result = self.vp.capture_voice()
        if not result:
            return "I didn't catch that. Try again."

        command_text, lang = result
        return self.process_input_command(command_text, lang)

    # ========== Greeting Generation ==========
    def generate_llm_greeting(self) -> str:
        now = datetime.now()
        weekday = now.strftime("%A")
        month = now.strftime("%B")
        hour = now.hour
        time_of_day = (
            "morning" if hour < 12 else "afternoon" if hour < 18 else "evening"
        )

        base_greetings = [
            f"Good {time_of_day}! Happy {weekday}.",
            f"Hope you're having a great {weekday}!",
            f"Hello and welcome this fine {time_of_day}.",
            f"It's {month} already! Let's get started.",
            f"Hi! Whatâ€™s the first thing you'd like me to do this {time_of_day}?",
        ]
        seed = random.choice(base_greetings)

        try:
            user_prompt = self.prompt_builder.greeting_prompt(
                time_of_day, weekday, month, seed
            )
            response = self.get_llm().invoke(
                [
                    self.prompt_builder.greeting_system_msg(),
                    {"role": "user", "content": user_prompt},
                ]
            )
            return response.content.strip().strip('"â€œâ€') or seed
        except Exception as e:
            self.logger.error(f"Greeting failed: {e}")
            return self.fallback_llm_greeting(seed)

    def fallback_llm_greeting(self, seed_greeting: str) -> str:
        try:
            import ollama

            response = ollama.chat(
                model="llama3.2:latest",
                messages=[
                    {
                        "role": "system",
                        "content": "You are a friendly assistant that creates warm spoken greetings.",
                    },
                    {
                        "role": "user",
                        "content": f"Improve this fallback greeting for voice use: '{seed_greeting}'",
                    },
                ],
            )
            return response["message"]["content"].strip().strip('"â€œâ€')
        except Exception:
            return seed_greeting

    # ========== Scene Querying ==========
    def fetch_camera_objects(self):
        conn = get_connection()
        cursor = conn.cursor()
        cursor.execute(
            """
            SELECT object_name, object_color, pos_x, pos_y, pos_z, rot_x, rot_y, rot_z, last_detected, usd_name
            FROM camera_vision
            """
        )
        rows = cursor.fetchall()
        cursor.close()
        conn.close()
        return rows

    def format_camera_data(self, objects: list) -> str:
        return "\n".join(
            f"- {name} ({color}) at ({x:.1f}, {y:.1f}, {z:.1f}) oriented at ({r:.1f}, {p:.1f}, {w:.1f}) last seen at {timestamp}, usd_name: {usd}"
            for name, color, x, y, z, r, p, w, timestamp, usd in objects
        )

    def query_scene(self, question: str) -> str:
        try:
            objects = self.fetch_camera_objects()
            if not objects:
                return (
                    "I can only see the camera. No other objects are currently visible."
                )
            formatted_data = self.format_camera_data(objects)
            input_data = {"question": question, "data": formatted_data}
            chain = self._build_chain_with_input(input_data)
            response = chain.invoke(input_data)
            return response.content.strip().strip('"â€œâ€')
        except Exception as e:
            self.logger.error("Scene query failed", exc_info=True)
            return "[Scene query failed.]"

    def trigger_remote_vision_task(self, command_text: str) -> str:
        conn = get_connection()
        cursor = conn.cursor()

        try:
            cursor.execute(
                """
                SELECT operation_name, description, trigger_keywords, trigger
                FROM operation_library
                WHERE is_triggerable = TRUE
                """
            )
            rows = cursor.fetchall()
            if not rows:
                return "No triggerable operations are available."

            options_text = "\n".join(
                f"{op_name}: {desc or '[no description]'}" for op_name, desc, *_ in rows
            )

            llm_chain = LLMChain(
                llm=self.llm, prompt=self.prompt_builder.match_operation_prompt()
            )
            result = llm_chain.invoke(
                {"command": command_text, "options": options_text}
            )
            matched_operation = result["text"].strip()

            row_map = {
                op_name: {"trigger": trig, "keywords": kws}
                for op_name, _, kws, trig in rows
            }

            if matched_operation not in row_map:
                matched_operation = None

            if not matched_operation:
                lowered = command_text.lower()
                for op_name, _, keywords, is_triggered in rows:
                    if keywords and any(k in lowered for k in keywords):
                        matched_operation = op_name
                        break

            if not matched_operation:
                return "Sorry, I couldn't match any known vision task to your request."

            is_already_triggered = row_map[matched_operation]["trigger"]
            if is_already_triggered:
                return f"The script '{matched_operation}' is already running."

            with conn:
                with conn.cursor() as cur:
                    cur.execute(
                        """
                        UPDATE operation_library
                        SET trigger = FALSE, state = 'idle'
                        WHERE trigger = TRUE OR state = 'triggered'
                    """
                    )
                    cur.execute(
                        """
                        UPDATE operation_library
                        SET trigger = TRUE, state = 'triggered', last_triggered = %s
                        WHERE operation_name = %s
                        """,
                        (datetime.now(), matched_operation),
                    )

            conn.commit()
            return f"Now running the detection script remotely."

        except Exception as e:
            self.logger.error("Triggering vision task failed", exc_info=True)
            return "An error occurred while trying to trigger the remote task."

        finally:
            cursor.close()
            conn.close()

    # ========== Task Processing ==========
    def process_task(self, command_text: str) -> str:
        try:
            conn = get_connection()
            cursor = conn.cursor()
            cursor.execute(
                """
                INSERT INTO unified_instructions (
                    session_id, timestamp, liu_id,
                    voice_command, gesture_command, unified_command,
                    confidence, processed
                )
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                RETURNING id;
                """,
                (
                    "session_voice_001",
                    datetime.now(),
                    self.authenticated_user["liu_id"],
                    command_text,
                    "",
                    command_text,
                    0.95,
                    False,
                ),
            )
            command_id = cursor.fetchone()[0]
            conn.commit()

            cursor.execute("DELETE FROM operation_sequence")
            conn.commit()
            cursor.close()
            conn.close()

            processor = CommandProcessor()
            success, _ = processor.process_command(
                {"id": command_id, "unified_command": command_text}
            )
            processor.close()

            return (
                "Yes! Task has successfully been planned."
                if success
                else "Sorry, I couldn't understand you."
            )
        except Exception as e:
            self.logger.error(f"Task processing failed: {e}", exc_info=True)
            return "[Task execution failed.]"

    def process_input_command(self, command_text: str, lang: str = "en"):
        cmd_type = self.classify_command(command_text)
        if cmd_type == "general":
            return self.handle_general_query(command_text)
        elif cmd_type == "scene":
            return self.query_scene(command_text)
        elif cmd_type == "trigger":
            return self.trigger_remote_vision_task(command_text)
        elif cmd_type == "task":
            return self.process_task(command_text)
        return f"You said: {command_text}. (Detected: {cmd_type})"

    def reset_memory(self):
        """
        Clears chat memory and deletes the associated memory file if it exists.
        Useful for fresh starts or user-initiated resets from the GUI.
        """
        self.memory.clear()
        if self.chat_memory_path and self.chat_memory_path.exists():
            self.chat_memory_path.unlink(missing_ok=True)
        self.logger.info("ðŸ§  Chat memory reset by user.")

    def get_random_wake_response(self) -> str:
        return random.choice(WAKE_RESPONSES)

    def log_status(self, msg: str, level: str = "info"):
        """
        Allows GUI to hook into logging. Use levels: 'info', 'warning', 'error'.
        """
        if level == "info":
            self.logger.info(msg)
        elif level == "warning":
            self.logger.warning(msg)
        elif level == "error":
            self.logger.error(msg)
        else:
            self.logger.debug(msg)

    # ========== Shut down ==========
    def shutdown(self):
        if self.chat_memory_path and self.chat_memory_path.exists():
            self.chat_memory_path.unlink(missing_ok=True)
        self.save_chat_history()
        self.logger.info("Session ended.")
###########################################
# mini_project/modalities/task_manager.py
""" TaskManagerGUIApproach1: A Tkinter-based GUI for managing human-robot interaction (HRI) tasks using multimodal (voice and gesture) input.
Classes:
    TaskManagerGUIApproach1:
        Provides a graphical interface for:
            - User authentication (face/voice)
            - Starting/stopping HRI task execution sessions
            - Capturing and synchronizing voice and gesture commands
            - Reviewing and confirming unified commands
            - Processing commands and managing session state
            - Clearing instruction tables and exiting the application
        Key Methods:
            __init__:
                Initializes the GUI, session manager, and command processor. Sets up all widgets and logging.
            log_event(message, level=logging.INFO):
                Logs events to both the logger and the GUI log window, and updates the status label.
            set_controls_state(start_enabled=True, stop_enabled=False, new_cmd_enabled=False):
                Enables or disables control buttons based on the current session state.
            start_execution():
                Authenticates the user if needed, creates a session, and starts the execution pipeline in a new thread.
            stop_execution():
                Cancels the current session and updates control states.
            new_command():
                Retries the session and starts a new capture cycle.
            clear_tables():
                Clears all instruction-related tables in the database.
            exit_application():
                Stops execution, closes the command processor, and exits the GUI.
            execution_pipeline():
                Orchestrates the main HRI workflow:
                    - Concurrently captures voice and gesture input
                    - Synchronizes and unifies inputs
                    - Prompts user to confirm the unified command
                    - Processes the command or repeats capture as needed
            run():
                Starts the Tkinter main event loop.
Usage:
    Run this module as a script to launch the HRI Task Manager GUI.
 """
import logging
import threading
import time
import tkinter as tk
from tkinter import messagebox, scrolledtext

from config.app_config import DB_PATH, TEMP_AUDIO_PATH, VOICE_DATA_PATH

from mini_project.authentication._face_auth import FaceAuthSystem
from mini_project.authentication._voice_auth import VoiceAuth
from mini_project.modalities.command_processor import CommandProcessor
from mini_project.modalities.orchestrator import run_gesture_capture, run_voice_capture
from mini_project.modalities.session_manager import SessionManager
from mini_project.modalities.synchronizer import synchronize_and_unify

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(message)s")
logger = logging.getLogger("TaskManagerGUI_Approach1")


class TaskManagerGUIApproach1:
    def __init__(self):
        # Create the main GUI window
        self.root = tk.Tk()
        self.root.title("HRI Task Manager - Approach 1")
        self.root.geometry("700x550")

        # Instantiate session manager and command processor.
        self.session_manager = SessionManager()
        self.cmd_processor = CommandProcessor()

        # Build GUI elements
        tk.Label(
            self.root,
            text="Human-Robot Interaction System (Approach 1)",
            font=("Arial", 18),
        ).pack(pady=10)
        self.status_label = tk.Label(self.root, text="Status: Idle", font=("Arial", 12))
        self.status_label.pack(pady=5)

        btn_frame = tk.Frame(self.root)
        btn_frame.pack(pady=10)
        self.start_btn = tk.Button(
            btn_frame, text="Start Execution", width=15, command=self.start_execution
        )
        self.start_btn.grid(row=0, column=0, padx=5, pady=5)
        self.stop_btn = tk.Button(
            btn_frame,
            text="Stop Execution",
            width=15,
            command=self.stop_execution,
            state=tk.DISABLED,
        )
        self.stop_btn.grid(row=0, column=1, padx=5, pady=5)
        self.new_cmd_btn = tk.Button(
            btn_frame,
            text="New Command",
            width=15,
            command=self.new_command,
            state=tk.DISABLED,
        )
        self.new_cmd_btn.grid(row=1, column=0, padx=5, pady=5)
        tk.Button(
            btn_frame, text="Clear Tables", width=15, command=self.clear_tables
        ).grid(row=1, column=1, padx=5, pady=5)
        tk.Button(btn_frame, text="Exit", width=15, command=self.exit_application).grid(
            row=2, column=0, columnspan=2, pady=10
        )

        self.log_text = scrolledtext.ScrolledText(
            self.root, width=80, height=15, wrap=tk.WORD
        )
        self.log_text.pack(pady=5)
        self.log_event("Application started. Please authenticate.")

    def log_event(self, message, level=logging.INFO):
        logger.log(level, message)
        self.log_text.insert(tk.END, f"{message}\n")
        self.log_text.see(tk.END)
        self.status_label.config(text=f"Status: {message}")

    def set_controls_state(
        self, start_enabled=True, stop_enabled=False, new_cmd_enabled=False
    ):
        self.start_btn.config(state=tk.NORMAL if start_enabled else tk.DISABLED)
        self.stop_btn.config(state=tk.NORMAL if stop_enabled else tk.DISABLED)
        self.new_cmd_btn.config(state=tk.NORMAL if new_cmd_enabled else tk.DISABLED)

    def start_execution(self):
        if not self.session_manager.running:
            # Ensure that the user is authenticated.
            if not self.session_manager.authenticated_user:
                user = self.session_manager.authenticate_user()
                if not user:
                    self.log_event(
                        "Authentication failed. Aborting execution.",
                        level=logging.ERROR,
                    )
                    return
                else:
                    # Display a welcome message once authenticated.
                    welcome_msg = f"Welcome, {user['first_name']} {user['last_name']} (ID: {user['liu_id']})"
                    self.log_event(welcome_msg)
            # Create a new session if one does not exist.
            if not self.session_manager.session_id:
                self.session_manager.create_session()
            self.set_controls_state(
                start_enabled=False, stop_enabled=True, new_cmd_enabled=False
            )
            threading.Thread(target=self.execution_pipeline, daemon=True).start()
            self.log_event("Execution started.")

    def stop_execution(self):
        if self.session_manager.running:
            self.session_manager.cancel_session()
            self.set_controls_state(
                start_enabled=True, stop_enabled=False, new_cmd_enabled=True
            )
            self.log_event("Execution stopped.")

    def new_command(self):
        # Retry session and start a new capture cycle.
        self.session_manager.retry_session()
        self.log_event("New command session started.")
        self.start_execution()

    def clear_tables(self):
        try:
            cursor = self.cmd_processor.conn.cursor()
            cursor.execute("DELETE FROM unified_instructions")
            cursor.execute("DELETE FROM voice_instructions")
            cursor.execute("DELETE FROM gesture_instructions")
            cursor.execute("DELETE FROM instruction_operation_sequence")
            self.cmd_processor.conn.commit()
            self.log_event("Database instructions cleared.")
        except Exception as e:
            self.log_event(f"Error clearing tables: {str(e)}", level=logging.ERROR)

    def exit_application(self):
        self.stop_execution()
        self.cmd_processor.close()
        self.root.destroy()

    def execution_pipeline(self):
        """
        Execution Pipeline:
          1. Start voice and gesture capture concurrently.
          2. Display prompt for voice input.
          3. Wait for the capture threads to complete.
          4. Merge inputs via the synchronizer.
          5. Retrieve and confirm the unified command with the user.
          6. If confirmed and processed, end the session; otherwise, repeat capture.
        """
        while self.session_manager.running:
            try:
                # Start voice and gesture capture concurrently.
                voice_thread = threading.Thread(
                    target=run_voice_capture,
                    args=(self.session_manager.session_id,),
                    daemon=True,
                )
                gesture_thread = threading.Thread(
                    target=run_gesture_capture,
                    args=(self.session_manager.session_id,),
                    daemon=True,
                )
                voice_thread.start()
                gesture_thread.start()
                self.log_event(
                    "Voice and gesture capture started. Please speak your request."
                )
                voice_thread.join()
                gesture_thread.join()
                self.log_event("Voice and gesture capture completed.")

                # Merge inputs using the synchronizer.
                liu_id = (
                    self.session_manager.authenticated_user.get("liu_id")
                    if self.session_manager.authenticated_user
                    else None
                )
                self.log_event("Merging captured inputs...")
                synchronize_and_unify(db_path=DB_PATH, liu_id=liu_id)
                unified = self.cmd_processor.get_unprocessed_unified_command()
                if unified:
                    unified_text = unified.get("unified_command", "")
                    self.log_event(f"Unified Command: {unified_text}")
                    if messagebox.askyesno(
                        "Confirm Command",
                        f"Is this your intended command?\n\n{unified_text}",
                    ):
                        self.log_event("User confirmed command. Processing...")
                        if self.cmd_processor.process_command(unified):
                            self.log_event("Command processed successfully.")
                            self.session_manager.cancel_session()
                            break
                        else:
                            self.log_event(
                                "Command processing failed. Re-capturing input...",
                                level=logging.ERROR,
                            )
                            continue
                    else:
                        self.log_event(
                            "Command rejected by user. Re-capturing input..."
                        )
                        continue  # Repeat the capture loop.
                else:
                    self.log_event(
                        "No unified command generated. Retrying capture...",
                        level=logging.WARNING,
                    )
                time.sleep(2)
            except Exception as e:
                self.log_event(
                    f"Error during execution pipeline: {str(e)}", level=logging.ERROR
                )
                break

        self.set_controls_state(
            start_enabled=True, stop_enabled=False, new_cmd_enabled=True
        )
        self.log_event("Session ended. Use 'Start Execution' to begin a new session.")

    def run(self):
        self.root.mainloop()


if __name__ == "__main__":
    app = TaskManagerGUIApproach1()
    app.run()
#####################################
# mini_project/modalities/synchronizer.py
""" synchronizer.py
This module provides functionality to synchronize and unify multimodal instructions (voice and gesture)
from a database, merging them into unified commands using a Large Language Model (LLM). It processes
unprocessed instructions in batches, merges them per session, and stores the unified result in a dedicated table.
Main Functions:
---------------
- get_instructions_by_session(cursor, limit, offset): Fetches unprocessed voice and gesture instructions from the database, grouped by session.
- store_unified_instruction(cursor, session_id, timestamp, voice_command, gesture_command, unified_command, liu_id): Stores a unified instruction into the unified_instructions table.
- mark_instructions_as_processed(cursor, session_id): Marks all instructions for a session as processed.
- llm_unify(voice_text, gesture_text, max_retries): Uses an LLM to combine voice and gesture instructions into a unified command.
- merge_session_commands(session_commands, delimiter): Merges all instructions of a session into single strings per modality.
- synchronize_and_unify(liu_id, batch_size): Orchestrates the synchronization and unification process in batches.
Usage:
------
Run this module as a script to process and unify all unprocessed instructions in the database.
Dependencies:
-------------
- Database connection and configuration modules
- Logging
- Subprocess (for LLM calls)
- Python standard libraries (datetime, functools, typing)
 """

import json
import logging

# import sqlite3
import subprocess
import uuid
from datetime import datetime
from functools import lru_cache
from typing import Dict, List, Optional

from config.app_config import (  # DB_PATH,
    BATCH_SIZE,
    LLM_MAX_RETRIES,
    LLM_MODEL,
    UNIFY_PROMPT_TEMPLATE,
    setup_logging,
)
from config.constants import GESTURE_TABLE, PROCESSED_COL, UNIFIED_TABLE, VOICE_TABLE

from mini_project.database.connection import get_connection

# Initialize logging with desired level
setup_logging(level=logging.INFO)
logger = logging.getLogger("Synchronizer")
DELIMITER = "\n"


def get_instructions_by_session(
    cursor, limit: int, offset: int
) -> Dict[str, List[Dict]]:
    """
    Fetches unprocessed instructions from voice and gesture tables in batches.

    Args:
        conn: SQLite database connection object.
        limit: Maximum number of records to fetch.
        offset: Starting offset for batch processing.
    Returns:
        A dictionary mapping session IDs to lists of instruction records.
    """
    query = f"""
        SELECT id, session_id, 'voice' AS modality, transcribed_text AS instruction_text, timestamp
        FROM {VOICE_TABLE} WHERE {PROCESSED_COL} = FALSE
        UNION ALL
        SELECT id, session_id, 'gesture' AS modality, gesture_text AS instruction_text, timestamp
        FROM {GESTURE_TABLE} WHERE {PROCESSED_COL} = FALSE
        ORDER BY timestamp ASC
        LIMIT %s OFFSET %s
    """
    cursor.execute(query, (limit, offset))
    rows = cursor.fetchall()
    sessions = {}

    for row in rows:
        ts = row[4] if isinstance(row[4], datetime) else datetime.fromisoformat(row[4])
        record = {
            "id": row[0],
            "session_id": row[1],
            "modality": row[2],
            "instruction_text": row[3],
            "timestamp": ts,
        }
        sessions.setdefault(record["session_id"], []).append(record)
    return sessions


def store_unified_instruction(
    cursor,
    session_id: str,
    timestamp: datetime,
    voice_command: str,
    gesture_command: str,
    unified_command: str,
    liu_id: Optional[str] = None,
) -> None:
    """
    Stores a unified instruction into the unified_instructions table.

    Args:
        session_id: The session identifier.
        timestamp: The timestamp of the instruction.
        voice_command: The voice instruction text.
        gesture_command: The gesture instruction text.
        unified_command: The unified command text.
        liu_id: Optional user ID.
        db_path: Path to the SQLite database.
    """
    cursor.execute(
        f"""
        INSERT INTO {UNIFIED_TABLE} (session_id, timestamp, liu_id, voice_command, gesture_command, unified_command)
        VALUES (%s, %s, %s, %s, %s, %s)
        """,
        (
            session_id,
            timestamp,
            liu_id,
            voice_command,
            gesture_command,
            unified_command,
        ),
    )
    logger.info(
        f"Stored unified instruction for session {session_id}: {unified_command}"
    )


def mark_instructions_as_processed(cursor, session_id: str) -> None:
    """
    Marks all voice and gesture instructions for the given session as processed.

    Args:
        conn: SQLite database connection object.
        session_id: The session identifier.
    """
    cursor.execute(
        f"UPDATE {VOICE_TABLE} SET {PROCESSED_COL} = TRUE WHERE session_id = %s AND {PROCESSED_COL} = FALSE",
        (session_id,),
    )
    cursor.execute(
        f"UPDATE {GESTURE_TABLE} SET {PROCESSED_COL} = TRUE WHERE session_id = %s AND {PROCESSED_COL} = FALSE",
        (session_id,),
    )
    logger.info(f"Marked instructions as processed for session {session_id}.")


@lru_cache(maxsize=128)
def llm_unify(voice_text: str, gesture_text: str, max_retries=LLM_MAX_RETRIES) -> str:
    """
    Combines a voice command with a gesture cue into a unified instruction using an LLM.

    Args:
        voice_text: The primary voice instruction (e.g., "Turn right").
        gesture_text: The supplementary gesture instruction (e.g., "Pointing up").
        max_retries: Number of retry attempts for LLM calls.
    Returns:
        A unified command string, or a fallback if unification fails.
    Example:
        >>> llm_unify("Stop", "Hand raised")
        'Stop with hand raised'
    """
    formatted_prompt = UNIFY_PROMPT_TEMPLATE.format(
        voice_text=voice_text, gesture_text=gesture_text
    )
    for attempt in range(max_retries):
        try:
            result = subprocess.run(
                ["ollama", "run", LLM_MODEL, formatted_prompt],
                capture_output=True,
                text=True,
                check=True,
                encoding="utf-8",
            )
            output = result.stdout.strip()
            if output and len(output) > 3:
                return output
            logger.warning(f"Attempt {attempt + 1}: Invalid output '{output}'")
        except subprocess.CalledProcessError as e:
            logger.warning(f"Attempt {attempt + 1} failed: {e}")
    logger.error("All LLM attempts failed. Using fallback.")
    return f"Voice: {voice_text}, Gesture: {gesture_text}"


def merge_session_commands(
    session_commands: List[Dict], delimiter: str = DELIMITER
) -> Dict[str, str]:
    """
    Merges instructions from a session into a single string per modality.

    Args:
        session_commands: List of instruction records for a session.
        delimiter: String used to join multiple instructions.
    Returns:
        A dictionary with merged voice and gesture instructions.
    """
    voice_records = [cmd for cmd in session_commands if cmd["modality"] == "voice"]
    gesture_records = [cmd for cmd in session_commands if cmd["modality"] == "gesture"]

    voice_records.sort(key=lambda x: x["timestamp"])
    gesture_records.sort(key=lambda x: x["timestamp"])

    merged_voice = delimiter.join(
        record["instruction_text"] for record in voice_records
    ).strip()
    merged_gesture = delimiter.join(
        record["instruction_text"] for record in gesture_records
    ).strip()

    return {"voice": merged_voice, "gesture": merged_gesture}


def synchronize_and_unify(
    liu_id: Optional[str] = None, batch_size: int = BATCH_SIZE
) -> None:
    """
    Synchronizes and unifies voice and gesture instructions in batches.

    Args:
        db_path: Path to the SQLite database.
        liu_id: Optional user ID.
        batch_size: Number of records to process per batch.
    """
    offset = 0
    try:
        conn = get_connection()
    except Exception as e:
        logger.error(f"âŒ Failed to connect to PostgreSQL: {e}")
        raise
    with conn:
        cursor = conn.cursor()
        while True:
            sessions = get_instructions_by_session(
                cursor, limit=batch_size, offset=offset
            )
            if not sessions:
                logger.info("No more unprocessed instructions found.")
                break

            for session_id, records in sessions.items():
                merged = merge_session_commands(records)
                voice_text = merged.get("voice", "")
                gesture_text = merged.get("gesture", "")
                session_timestamp = max(r["timestamp"] for r in records)
                unified_command = llm_unify(voice_text, gesture_text)
                store_unified_instruction(
                    cursor,
                    session_id,
                    session_timestamp,
                    voice_text,
                    gesture_text,
                    unified_command,
                    liu_id,
                )
                mark_instructions_as_processed(cursor, session_id)
            offset += batch_size
            logger.info(f"Processed batch of {batch_size} records. Offset: {offset}")
    conn.commit()
    logger.info("Synchronization and unification complete.")


if __name__ == "__main__":
    synchronize_and_unify()
#############################
# mini_project/modalities/orchestrator.py
"""
Orchestrator module for multimodal session capture and synchronization.
This module coordinates the capture of voice and gesture data streams in parallel threads,
and then synchronizes and unifies the captured data at the end of the session.
Functions:
    run_voice_capture(session_id: str):
        Captures voice input for a given session using the VoiceProcessor.
        Signals the end of session capture upon completion.
    run_gesture_capture(session_id: str):
        Captures gesture input for a given session using the GestureDetector.
        Listens for a termination event to end capture gracefully.
Execution:
    When run as a script, generates a unique session ID, starts both voice and gesture
    capture threads, waits for their completion, and then runs the synchronizer/unifier
    to process and unify the captured data.
Logging:
    Uses the configured logging setup to record progress and errors throughout the session.
Dependencies:
    - config.app_config (for logging setup)
    - mini_project.modalities.gesture_processor.GestureDetector
    - mini_project.modalities.synchronizer.synchronize_and_unify
    - mini_project.modalities.voice_processor.VoiceProcessor
"""


import logging
import threading
import uuid

from config.app_config import *

from mini_project.modalities.gesture_processor import GestureDetector
from mini_project.modalities.synchronizer import synchronize_and_unify
from mini_project.modalities.voice_processor import VoiceProcessor

# Initialize logging
setup_logging(level=logging.INFO)
logger = logging.getLogger("Orchestrator")

# Global event to signal end of session capture
SESSION_END_EVENT = threading.Event()


def run_voice_capture(session_id: str):
    vp = VoiceProcessor(session_id=session_id)
    vp.capture_voice()
    logger.info("Voice capture completed.")
    # Signal that voice capture is complete; gesture capture should stop.
    SESSION_END_EVENT.set()


def run_gesture_capture(session_id: str):
    gd = GestureDetector(session_id=session_id)
    # Pass termination event so gesture capture can close gracefully.
    gd.process_video_stream(termination_event=SESSION_END_EVENT)
    logger.info("Gesture capture completed.")


if __name__ == "__main__":
    session_id = str(uuid.uuid4())
    logger.info(f"Starting session with session_id: {session_id}")

    voice_thread = threading.Thread(target=run_voice_capture, args=(session_id,))
    gesture_thread = threading.Thread(target=run_gesture_capture, args=(session_id,))

    voice_thread.start()
    gesture_thread.start()

    voice_thread.join()
    gesture_thread.join()

    logger.info("Session capture ended. Now running synchronizer/unifier...")

    try:
        synchronize_and_unify(liu_id=None)
        logger.info("Unification complete. Check the unified_instructions table.")
    except Exception as e:
        logger.error(f"Synchronization failed: {e}")
        logger.debug("Exception details:", exc_info=True)
################################
<!--
This code defines a selection of code that is referenced for documentation purposes.
Please provide the actual code selection to generate a relevant documentation comment.
-->

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Robot Assistant</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@500;700&display=swap" rel="stylesheet">

    <style>
        :root {
            --bg-color: #1e1f26;
            --card-bg: #2c2f36;
            --text-color: #e2e2e2;
            --message-text-color: #181717;
            --accent: #297e8a;
            --send-button: #297e8a;
            --voice-button: #249bb8;
        }

        body {
            /* background-color: var(--bg-color); */
            background-color: #ffffff;
            color: var(--text-color);
            font-family: 'Segoe UI', sans-serif;
            margin: 0;
            padding: 0;
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100vh;
        }

        .chat-wrapper {
            background-color: var(--card-bg);
            width: 95%;
            max-width: 500px;
            height: 90vh;
            border-radius: 15px;
            display: flex;
            flex-direction: column;
            overflow: hidden;
            box-shadow: 0 0 10px rgba(0,0,0,0.3);
            /* border: 2px solid #36b1b1; */
        }

        .chat-header {
            padding: 1em;
            display: flex;
            align-items: center;
            background-color: #222;
            border-bottom: 1px solid #444;
        }

        .chat-header img {
            width: 100px;
            height: 100px;
            border-radius: 50%;
            margin-right: 1em;
            object-fit: cover;
        }


        .chat-header .status {
            font-size: 0.9em;
            color: var(--accent);
        }

        .chat-body {
            flex: 1;
            overflow-y: auto;
            padding: 1em;
            display: flex;
            flex-direction: column;
            gap: 0.75em;
            background-image: url('/static/chat-bg-robot.png');
            /* background-color: rgb(23, 137, 141); */

            background-size: cover;
            background-repeat: no-repeat;
            background-position: center;

        }


        .chat-message {
            max-width: 80%;
            padding: 0.8em 1em;
            border-radius: 12px;
            line-height: 1.4;
            animation: fadeIn 0.3s ease;
        }

        .from-user {
            align-self: flex-end;
            background-color: #2a2a2a;
            border: 1px solid #2dc76d;
            color: #e2e2e2;
            border-bottom-right-radius: 0;
        }

        .from-assistant {
            align-self: flex-start;
            background-color: #2a2a2a;
            border: 1px solid #4fc3f7;
            color: #e2e2e2;
            padding: 0.8em 1em;
            border-radius: 16px 16px 16px 0;
            max-width: 80%;
            box-shadow: 0 2px 6px rgba(0, 0, 0, 0.3);
        }


        .tiny-toolbar {
            width: 100%;
            background: #232323;
            border-bottom: 1px solid #444;
            border-top: 1px solid #444;
            /* border-top-left-radius: 15px; */
            /* border-top-right-radius: 15px; */
            border-bottom-left-radius: 15px;
            border-bottom-right-radius: 15px;
            padding: 0.1em 0.1em 0.1em 0.2em;
            display: flex;
            justify-content: right;   /* <-- Center the content */
            align-items: center;
            min-height: 36px;
            box-shadow: 0 1px 4px rgba(0,0,0,0.08);
            font-size: 0.88em;
            gap: 0.7em;
            box-sizing: border-box;
            overflow-x: auto;
        }

        .tiny-toolbar label {
            color: #e2e2e2;
            font-weight: 500;
            margin: 0;
        }

        .tiny-toolbar select {
            background: #181717;
            color: #e2e2e2;
            border: 1px solid #4fc3f7;
            border-radius: 12px;
            padding: 0.2em 1em;
            font-size: 0.8em;
            margin-left: 0.1em;
        }
        .chat-footer {
            padding: 1.5em 1.5em 0.5em 1.5em;
            border-top: 1px solid #444;
            display: flex;
            align-items: center;
            gap: 0.5em;
        }

        .chat-footer textarea {
            flex: 1;
            resize: none;
            border-radius: 22px;
            border: none;
            font-size: 1em;

            background-color: #dce6e6;
            color: var(--message-text-color);
            font-family: 'Segoe UI','Segoe Print', 'Comic Sans MS', cursive;
            padding: 0.5em 0.5em 0.5em 1em;
        }


        .chat-footer button {
            background-color: var(--send-button);
            border: solid #b9bdbd;
            border-width: 2px;
            border-radius: 50%;
            width: 48px;
            height: 48px;
            font-size: 2em;
            display: flex;
            align-items: center;
            justify-content: center;
            cursor: pointer;
            transition: 0.2s;
        }
        .chat-footer button:hover {
            background-color: #717374; /* Or tweak the hover accent */
        }

        .chat-body::-webkit-scrollbar {
            width: 6px;
        }

        .chat-body::-webkit-scrollbar-thumb {
            background-color: #e9edf044;
            border-radius: 6px;
        }

        .chat-body::-webkit-scrollbar-track {
            background-color: transparent;
        }

        textarea {
            overflow: hidden;
            max-height: 200px;
            transition: height 0.15s ease;
        }


        #db-table table {
            width: 100%;
            border-collapse: collapse;
            font-size: 0.7em;
        }

        #db-table th, #db-table td {
            padding: 0.6em 1em;
            text-align: left;
            border-bottom: 1px solid #444;
        }

        #db-table th {
            background-color: #1e1e1e;
            color: #4fc3f7;
            font-weight: 600;
            font-size: 1.2em;
            position: sticky;
            top: 0;
            z-index: 2;
        }

        #db-table tr:nth-child(even) {
            background-color: #2e2e2e;
        }

        #db-table tr:hover {
            background-color: #3a3a3a;
            transition: 0.2s;
        }

        #db-table {
            max-height: 300px;
            overflow-y: auto;
            overflow-x: auto;
        }

        /* Scrollbar styling */
        #db-table::-webkit-scrollbar {
            height: 6px;
            width: 6px;
        }

        #db-table::-webkit-scrollbar-thumb {
            background: #4fc3f7aa;
            border-radius: 4px;
        }

        #db-table::-webkit-scrollbar-track {
            background: transparent;
        }







        .message-wrapper {
            display: flex;
            align-items: flex-end;
            margin-bottom: 0.75em;
            gap: 0.5em;
        }

        .message-wrapper.user {
            justify-content: flex-end;
        }

        .avatar {
            width: 32px;
            height: 32px;
            border-radius: 50%;
            background-color: #666;
            flex-shrink: 0;
        }

        .message-wrapper.user .avatar {
            order: 2;
        }

        .message-wrapper.user .chat-message {
            order: 1;
        }


        .rounded {
            border-radius: 20px;
            padding: 0.5em 0.7em 0.7em 0.4em;
            /*        top  right bottom left  */
            background: #ce375c;
            /* color: #000000; */
            color: #fcfbfb;

            border: 0.2em solid #fffefe;
            font-family: 'Segoe UI','Segoe Print','Nunito', 'Segoe UI', Arial, sans-serif;
            font-size: 0.8em;
            font-weight: 500;
            cursor: pointer;
            /* margin-right: 0.2em; */
            transition: background 0.2s;
        }

        .rounded:hover {
            background: #797a7a;
        }

        .rounded-btn {
            background-color: var(--accent);
            /* color: #000; */
            color: #fcfbfb;
            font-weight: bold;
            border: 0.2em solid #fdfdfd;
            font-family: 'Segoe Print','Nunito', 'Segoe UI', Arial, sans-serif;
            font-size: 0.7em;

            border-radius: 16px;
            padding: 4px 10px 6px 5px;
            margin-right: 0.3em;
            cursor: pointer;
            transition: background-color 0.2s ease;
        }

        .rounded-btn:hover {
            background-color: #2ad49c; /* Or tweak the hover accent */
        }


        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(10px); }
            to { opacity: 1; transform: translateY(0); }
        }
    </style>
</head>
<body>

    <div class="chat-wrapper">
        <div class="chat-header">
            <img src="/static/yumi.png" alt="Yumi avatar">
            <div style="flex-grow: 1;">
                <div style="display: flex; justify-content: space-between; align-items: center;">
                <strong>Robot Assistant</strong>
                </div>
                <div class="status">ðŸŸ¢ Online</div>
                <div id="auth-user" style="font-size: 0.75em; color: #aaa;">Authenticated: ...</div>
            </div>
            <img src="/static/liu.png" alt="Logo" style="width: 100px; height: 40px; margin-left: 1em; margin-right: 0em; object-fit: contain;">
        </div>

        <div class="toolbar" style="display: flex; justify-content: space-between;  align-items: center; padding: 0.5em 0.5em;">
            <div>

               <div>
                    <button onclick="startSession()" class="rounded-btn">ðŸ”Sign In</button>
                    <button onclick="fetchDBView()" class="rounded-btn">ðŸ“ŠTable</button>
                    <button onclick="simulateTask()" class="rounded-btn">ðŸ§ªSimulate</button>
                    <button onclick="executeTask()" class="rounded-btn">ðŸ¤–Execute</button>
                    <button onclick="resetMemory()" class="rounded-btn">ðŸ§ Reset</button>

                </div>
            </div>
            <label style="font-size: 0.9em; font-weight: bold;">
                ðŸ”Š TTS <input type="checkbox" id="tts-toggle" checked>
            </label>

        </div>

        <!-- Tiny toolbar for voice selection -->
        <div class="tiny-toolbar">
            <label style="font-size: 1em; margin: 0.1;">
                ðŸ“¢
                <select id="voice-select" class="rounded"></select>
            </label>
        </div>

        <div id="chat" class="chat-body">
            <!-- Messages go here -->
        </div>

        <div id="db-panel" style="display:none; background:#2a2a2a; color:white; padding:0.8em 0.5em;">
            <div style="margin-bottom: 0.1em;">
                <label for="table-select">ðŸ—‚ï¸ select table:</label>
                <select id="table-select" class="rounded" onchange="loadTable(this.value)">
                    <option value="">choose table...</option>
                    <option value="camera_vision">camera_vision</option>
                    <option value="users">users</option>
                    <option value="operation_sequence">operation_sequence</option>
                    <option value="unified_instructions">unified_instructions</option>
                    <option value="sort_order">sort_order</option>
                    <option value="sequence_library">sequence_library</option>
                </select>
            </div>
            <div id="db-table" style="overflow:auto; max-height:300px;"></div>
        </div>



        <div id="typing-indicator" style="text-align: left; padding: 0 1em; font-style: italic; color: #999;"></div>
        <div class="toolbar" style="display: flex; justify-content: center; align-items: center; gap: 0.3em; padding: 1em 0.5em 0.2em 0.5em;">
            <button onclick="appendQuickReplies()" class="rounded">ðŸ¤”Replies</button>
            <button onclick="stopResponse()" class="rounded">â¹ï¸StopTTS</button>
            <button onclick="clearChat()" class="rounded">ðŸ§¹Clear</button>
            <button onclick="showHistory()" class="rounded">ðŸ“œHistory</button>
            <label style="font-size: 1em; margin: 0 0;">
                <select id="llm-select" class="rounded" onchange="setLLMModel(this.value)" title="Select a language model">
                    <option value="mistral:latest">ðŸŒŠMistral</option>

                    <option value="phi4:latest">ðŸ§®Phi-4</option>
                    <option value="gemma3:4b">ðŸ’ŽGemma 3</option>

                    <option value="llama3.3:latest">ðŸ¦™Llama 3.3</option>
                    <option value="llama3.2:latest">ðŸ¦™Llama 3.2</option>
                    <option value="llama3.2:1b">ðŸ¦™Llama 3.1</option>

                    <option value="deepseek-r1:32b">ðŸ”ŽDeepseek 2</option>
                    <option value="deepseek-r1:1.5b">ðŸ”¬Deepseek 1</option>

                    <option value="qwen2.5:0.5b">ðŸ§Qwen 2.5 </option>
                    <option value="qwen:0.5b">ðŸ§©Qwen 0.5</option>

                </select>
            </label>
        </div>
        <div id="quick-replies" class="quick-replies" style="padding: 0.5em 1em; "></div>

        <div class="chat-footer">
            <button onclick="startVoice()" title="voice"><span style="font-size: 1.2em;">ðŸŽ¤</span></button>
            <textarea id="command-input" title="type here" placeholder="Ask anything..." rows="1"></textarea>
            <button onclick="sendCommand()" title="send"><span style="font-size: 1.2em;">ðŸš€</span></button>
        </div>
        <footer style="
            text-align: center;
            font-size: 0.8em;
            color: rgba(255, 255, 255, 0.4);
            margin-top: 0em;
            margin-bottom: 0.5em;
            padding: 0.5em;
            background: transparent;
        ">
            Â© 2025 Â· <a href='https://www.linkedin.com/in/ikechuquoscar/' target='_blank' rel='noopener' style='color: inherit;'>Oscar Ikechukwu</a> Â· Masters Thesis Â· <a href='https://liu.se' target='_blank' rel='noopener' style='color: inherit;'>LinkÃ¶ping University</a>
        </footer>


        <!-- Add this modal to your HTML -->
        <!-- Add this modal to your HTML -->
        <div id="register-modal" style="display:none; position:fixed; top:0; left:0; width:100vw; height:100vh; background:rgba(0,0,0,0.5); align-items:center; justify-content:center; z-index:1000;">
          <form id="register-form" style="background:#fff; color:#222; padding:2em; border-radius:10px; min-width:300px;">
            <h3>Register User</h3>
            <input type="text" id="reg-firstname" placeholder="First Name" required style="width:100%;margin-bottom:0.5em;"><br>
            <input type="text" id="reg-lastname" placeholder="Last Name" required style="width:100%;margin-bottom:0.5em;"><br>
            <input type="text" id="reg-liuid" placeholder="LIU ID" required style="width:100%;margin-bottom:0.5em;"><br>
            <input type="email" id="reg-email" placeholder="Email" required style="width:100%;margin-bottom:0.5em;"><br>
            <input type="file" id="reg-face" accept="image/*" capture="user" required style="margin-bottom:0.5em;"><br>
            <input type="file" id="reg-voice" accept="audio/*" capture required style="margin-bottom:0.5em;"><br>
            <button type="submit" class="rounded-btn">Register</button>
            <button type="button" onclick="closeRegisterModal()" class="rounded-btn">Cancel</button>
          </form>
        </div>
    </div>

    <script>
        let currentLLMController = null;
        let currentVoiceLLMController = null;

        const textarea = document.getElementById("command-input");

        textarea.addEventListener("input", () => {
            textarea.style.height = "auto"; // Reset height
            if (textarea.scrollHeight > textarea.clientHeight) {
                textarea.style.height = textarea.scrollHeight + "px";
            }

        });

        document.getElementById("command-input").addEventListener("keydown", function (e) {
            if (e.key === "Enter" && !e.shiftKey) {
                e.preventDefault();
                sendCommand();
            }
        });

        async function startSession() {
            try {
                const res = await fetch("/start-session", { method: "POST" });
                const data = await res.json();

                if (data.authenticated === false) {
                    showRegisterModal();
                    return;
                }

                // Show welcome message
                if (data.username) {
                    appendMessage("assistant", `ðŸ‘‹ Welcome, ${data.username}! You are now authenticated.`);
                    updateAuthUser(data.username);
                } else if (data.message) {
                    appendMessage("assistant", data.message);
                }

                // Update model in UI if needed
                if (data.model) {
                    const typingIndicator = document.getElementById("typing-indicator");
                    if (typingIndicator) typingIndicator.innerText = `âœ… Using model: ${data.model}`;
                    const llmSelect = document.getElementById("llm-select");
                    if (llmSelect && [...llmSelect.options].some(opt => opt.value === data.model)) {
                        llmSelect.value = data.model;
                    }
                }
            } catch (error) {
                console.error(error);
                showResponse("âŒ Unable to start session. Please try again.");
            }
        }

        function updateAuthUser(name) {
            document.getElementById("auth-user").innerText = "Authenticated: " + name;
        }

        function showRegisterModal() {
            document.getElementById('register-modal').style.display = 'flex';
        }

        function closeRegisterModal() {
            document.getElementById('register-modal').style.display = 'none';
        }

        document.getElementById('register-form').onsubmit = async function(e) {
            e.preventDefault();
            const data = new FormData();
            data.append('first_name', document.getElementById('reg-firstname').value);
            data.append('last_name', document.getElementById('reg-lastname').value);
            data.append('liu_id', document.getElementById('reg-liuid').value);
            data.append('email', document.getElementById('reg-email').value);
            data.append('face', document.getElementById('reg-face').files[0]);
            data.append('voice', document.getElementById('reg-voice').files[0]);
            const res = await fetch('/register-user', {
                method: 'POST',
                body: data
            });
            const result = await res.json();
            showResponse(result.message);
            closeRegisterModal();
        };

        async function sendCommand() {
            try {
                // Abort any previous request
                if (currentLLMController) currentLLMController.abort();
                currentLLMController = new AbortController();

                const input = document.getElementById("command-input");
                const message = input.value.trim();
                if (!message) {
                    showResponse("âŒ Command cannot be empty.");
                    return;
                }

                appendMessage("user", message); // <-- Add this line to show user's message

                input.value = "";
                input.style.height = "auto"; // Reset height

                showResponse("ðŸ¤– Robot is thinking ...");
                const formData = new FormData();
                formData.append("command", message);

                const res = await fetch("/send-command", { method: "POST", body: formData, signal: currentLLMController.signal });
                if (!res.ok) throw new Error("Failed to send command");
                const data = await res.json();
                appendMessage("assistant", data.response || "No response.");
                speakResponse(data.response || "No response.");
                showResponse("");
            } catch (error) {
                if (error.name === "AbortError") {
                    showResponse("â¹ï¸ Response stopped.");
                } else {
                    console.error(error);
                    showResponse("âŒ Unable to process command. Please try again.");
                }
            } finally {
                currentLLMController = null;
            }
        }

        async function sendPredefined(reply) {
            if (currentLLMController) currentLLMController.abort();
            currentLLMController = new AbortController();
            try {
                showResponse("ðŸ¤– Robot is thinking...");
                const formData = new FormData();
                formData.append("command", reply);
                const res = await fetch("/send-command", { method: "POST", body: formData, signal: currentLLMController.signal });
                const data = await res.json();
                appendMessage("assistant", data.response || "No response.");
                speakResponse(data.response || "No response.");
                showResponse("");
            } catch (error) {
                if (error.name === "AbortError") {
                    showResponse("â¹ï¸ Response stopped.");
                } else {
                    console.error(error);
                    showResponse("âŒ Unable to process command. Please try again.");
                }
            } finally {
                currentLLMController = null;
            }
        }

        async function resetMemory() {
            const res = await fetch("/reset-memory", { method: "POST" });
            const data = await res.json();
            showResponse(data.message);
        }

        function showResponse(text) {
            const typingIndicator = document.getElementById("typing-indicator");
            typingIndicator.innerText = text;
        }

        function fetchDBView() {
            const panel = document.getElementById("db-panel");
            panel.style.display = panel.style.display === "none" ? "block" : "none";
        }

        async function loadTable(tableName) {
            const dbTableDiv = document.getElementById("db-table");
            dbTableDiv.innerHTML = "â³ Loading...";
            try {
                const res = await fetch(`/view-db/${tableName}`);
                if (!res.ok) throw new Error("Failed to fetch table data");
                const data = await res.json();
                if (data.error) {
                    dbTableDiv.innerHTML = `<p>${data.error}</p>`;
                    return;
                }



                const table = document.createElement("table");
                table.style.width = "100%";
                table.style.borderCollapse = "collapse";
                table.style.marginTop = "1em";

                // Header row
                const header = table.insertRow();
                data.columns.forEach(col => {
                    const th = document.createElement("th");
                    th.textContent = col;
                    th.style.borderBottom = "1px solid #4fc3f7";
                    th.style.padding = "0.5em";
                    header.appendChild(th);
                });

                // Data rows
                data.rows.forEach(row => {
                    const tr = table.insertRow();
                    row.forEach(cell => {
                        const td = tr.insertCell();
                        td.textContent = cell;
                        td.style.padding = "0.5em";
                        td.style.borderBottom = "1px solid #444";
                    });
                });

                dbTableDiv.innerHTML = "";
                dbTableDiv.appendChild(table);
            } catch (error) {
                console.error(error);
                dbTableDiv.innerHTML = "<p>âŒ Error loading table.</p>";
            }
        }

        async function setLLMModel(model) {
            const allowedModels = ["mistral:latest","phi4:latest","gemma3:4b","llama3.3:latest", "llama3.2:latest",  "llama3.2:1b","deepseek-r1:32b","deepseek-r1:1.5b", "qwen2.5:0.5b",  "qwen:0.5b"];

            if (!allowedModels.includes(model)) {
                showResponse("âŒ Invalid model selected.");
                return;
            }
            try {
                const res = await fetch("/set-model", {
                    method: "POST",
                    headers: { "Content-Type": "application/json" },
                    body: JSON.stringify({ model }),
                });
                if (!res.ok) throw new Error("Failed to set model");
                const data = await res.json();
                showResponse(data.message || `Model set to ${model}`);
            } catch (error) {
                console.error(error);
                showResponse("âŒ Unable to set model. Please try again.");
            }
        }

        function appendQuickReplies() {
            const container = document.getElementById("quick-replies");
            // Toggle: if already visible, hide; else show
            if (container.innerHTML.trim() !== "") {
                container.innerHTML = '';
                return;
            }

            ["yes", "no", "good bye!", "repeat", "tell me a joke","what can you see?"].forEach(reply => {
                const btn = document.createElement("button");
                btn.textContent = reply;
                btn.style.margin = "0.2em";
                btn.style.padding = "0.3em 0.5em";
                btn.style.borderRadius = "10px";
                btn.style.cursor = "pointer";
                // btn.style.backgroundColor = "#444";
                btn.style.border = "none";
                btn.style.color = "#000";
                btn.onclick = () => {
                    appendMessage("user", reply);
                    showResponse("ðŸ¤– Robot is thinking ...");
                    sendPredefined(reply);
                    container.innerHTML = '';
                };
                container.appendChild(btn);
            });
        }

        function clearChat() {
            const chat = document.getElementById("chat");
            chat.innerHTML = "";
        }

        function stopResponse() {
            if (currentLLMController) {
                currentLLMController.abort();
                currentLLMController = null;
            }
            if (currentVoiceLLMController) {
                currentVoiceLLMController.abort();
                currentVoiceLLMController = null;
            }
            if ('speechSynthesis' in window) {
                window.speechSynthesis.cancel();
            }
            showResponse("â¹ï¸ Response stopped.");
        }

        function showHistory() {
            alert("History feature coming soon!"); // Replace with your actual history logic
        }

        function appendMessage(sender, text) {
            const chat = document.getElementById("chat");
            const msgWrapper = document.createElement("div");
            msgWrapper.className = `message-wrapper ${sender}`;

            const avatar = document.createElement("div");
            avatar.className = "avatar";
            avatar.innerHTML = `<img src="/static/${sender === 'user' ? 'user-chat-avatar.png' : 'yumi-avatar.png'}" alt="avatar" style="width:100%; height:100%; border-radius:50%;">`;

            const msg = document.createElement("div");
            msg.className = `chat-message ${sender === 'user' ? 'from-user' : 'from-assistant'}`;

            const now = new Date();
            const time = now.toLocaleTimeString([], { hour: '2-digit', minute: '2-digit' });

            const textDiv = document.createElement("div");
            textDiv.textContent = text;
            const timeDiv = document.createElement("div");
            timeDiv.style = "font-size: 0.7em; color: #aaa; text-align: right;";
            timeDiv.textContent = time;

            msg.appendChild(textDiv);
            msg.appendChild(timeDiv);

            if (sender === 'user') {
                msgWrapper.appendChild(msg);
                msgWrapper.appendChild(avatar);
            } else {
                msgWrapper.appendChild(avatar);
                msgWrapper.appendChild(msg);
            }

            chat.appendChild(msgWrapper);
            setTimeout(() => {
                chat.scrollTop = chat.scrollHeight;
            }, 10);

            if (sender === 'assistant' && text.trim().endsWith("?")) {
                appendQuickReplies();
            }
        }

        function populateVoiceList() {
            const select = document.getElementById("voice-select");
            if (!select) return;
            const preferredVoiceName = "Microsoft Ryan Online (Natural) - English (United Kingdom)";
            const preferredLang = "en-GB";
            select.innerHTML = "";
            const voices = window.speechSynthesis.getVoices();
            let preferredValue = null;
            voices.forEach((voice) => {
                // Only include English voices
                if (
                    voice.lang.startsWith("en") &&
                    (voice.name.includes("Google") || voice.name.includes("Microsoft") || voice.name.includes("Apple"))
                ) {
                    const value = `${voice.name}|||${voice.lang}`;
                    const option = document.createElement("option");
                    option.value = value;
                    option.textContent = `${voice.name} (${voice.lang})${voice.default ? " [default]" : ""}`;
                    select.appendChild(option);
                    if (voice.name === preferredVoiceName && voice.lang === preferredLang) {
                        preferredValue = value;
                    }
                }
            });
            // Set preferred voice as selected, or fallback to first
            if (preferredValue !== null) {
                select.value = preferredValue;
            } else if (select.options.length > 0) {
                select.selectedIndex = 0;
            }
        }

        function setPreferredVoice() {
            const select = document.getElementById("voice-select");
            const voices = window.speechSynthesis.getVoices();
            const preferredVoiceName = "Microsoft Ryan Online (Natural) - English (United Kingdom)";
            const preferredLang = "en-GB";
            if (!select || !voices.length) return;
            const preferred = voices.find(v =>
                v.name === preferredVoiceName && v.lang === preferredLang
            );
            if (preferred) {
                select.value = `${preferred.name}|||${preferred.lang}`;
            }
        }

        window.addEventListener("DOMContentLoaded", () => {
            populateVoiceList();
            setPreferredVoice();
        });
        window.speechSynthesis.onvoiceschanged = () => {
            populateVoiceList();
            setPreferredVoice();
        };

        function speakResponse(text) {
            if (document.getElementById("tts-toggle")?.checked && 'speechSynthesis' in window) {
                const msg = new SpeechSynthesisUtterance(text);
                const select = document.getElementById("voice-select");
                const voices = window.speechSynthesis.getVoices();
                let voice = null;
                if (select && voices.length && select.value) {
                    const [name, lang] = select.value.split("|||");
                    voice = voices.find(v => v.name === name && v.lang === lang);
                }
                // Fallback: always try to use preferred if available
                if (!voice) {
                    voice = voices.find(v => v.name === "Microsoft Ryan Online (Natural) - English (United Kingdom)" && v.lang === "en-GB");
                }
                if (voice) {
                    msg.voice = voice;
                    msg.lang = voice.lang;
                } else {
                    msg.lang = "en-GB";
                }
                msg.rate = 1;
                window.speechSynthesis.speak(msg);
            }
        }

        async function startVoice() {
            try {
                showResponse("ðŸŽ¤ Listening...");
                const res = await fetch("/process-voice", { method: "POST" });
                if (!res.ok) throw new Error("Failed to process voice input");
                const data = await res.json();

                // Show transcription immediately
                if (data.transcription) {
                    appendMessage("user", data.transcription);

                    // Abort any previous LLM voice fetch
                    if (currentVoiceLLMController) currentVoiceLLMController.abort();
                    currentVoiceLLMController = new AbortController();

                    // Now, fetch the LLM response
                    showResponse("ðŸ¤– Thinking...");
                    const llmRes = await fetch("/llm-response", {
                        method: "POST",
                        headers: { "Content-Type": "application/json" },
                        body: JSON.stringify({ command_text: data.transcription, lang: data.lang || "en" }),
                        signal: currentVoiceLLMController.signal
                    });
                    if (!llmRes.ok) throw new Error("Failed to get LLM response");
                    const llmData = await llmRes.json();
                    appendMessage("assistant", llmData.response || "No response.");
                    speakResponse(llmData.response || "No response.");
                    showResponse(""); // Clear the typing indicator here
                } else {
                    showResponse("âŒ No speech detected.");
                }
            } catch (error) {
                if (error.name === "AbortError") {
                    showResponse("â¹ï¸ Response stopped.");
                } else {
                    console.error(error);
                    showResponse("âŒ Unable to process voice input. Please try again.");
                }
            } finally {
                currentVoiceLLMController = null;
            }
        }

        document.getElementById("tts-toggle").addEventListener("change", function (e) {
            if (!e.target.checked && 'speechSynthesis' in window) {
                window.speechSynthesis.cancel();
            }
        });

    </script>

</body>
</html>
###################################
# mini_project/web_interface/routes.py
"""
This module defines the API routes for the web interface of the mini_project application using FastAPI.
Routes:
    - GET /: Render the homepage using Jinja2 templates.
    - POST /start-session: Start a new authentication session with the voice assistant.
    - POST /send-command: Send a text command to the voice assistant and receive a response.
    - POST /reset-memory: Reset the assistant's memory.
    - GET /view-db/{table_name}: View up to 50 rows from an allowed database table.
    - POST /set-model: Set the language model used by the assistant.
    - POST /register-user: Register a new user with face and voice data.
    - POST /process-voice: Process uploaded or captured voice audio and return transcription.
    - POST /llm-response: Get a response from the assistant's language model for a given command.
Dependencies:
    - FastAPI for API routing and request handling.
    - Jinja2 for HTML templating.
    - Custom modules for database connection, face and voice authentication, and the voice assistant logic.
Note:
    - File uploads for face and voice are saved temporarily for processing.
    - Only specific database tables are allowed to be viewed via the API.
    - Error handling is included for user registration and voice processing.
"""



from fastapi import UploadFile, File
from fastapi import APIRouter, Request, Form
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.templating import Jinja2Templates
from pathlib import Path
from mini_project.database.connection import get_connection
from mini_project.authentication._face_auth import FaceAuthSystem
from mini_project.authentication._voice_auth import VoiceAuth

from mini_project.modalities.voice_assistant import VoiceAssistant

router = APIRouter()
templates = Jinja2Templates(directory="mini_project/web_interface/templates")

# Initialize assistant
assistant = VoiceAssistant()


@router.get("/", response_class=HTMLResponse)
async def homepage(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})


@router.post("/start-session")
async def start_session():
    success, msg = assistant.start_session()
    return {
        "authenticated": success,  # <-- FIXED: use actual result
        "message": msg,
        "username": (
            assistant.authenticated_user.get("first_name")
            if assistant.authenticated_user
            else None
        ),
        "model": assistant.selected_model,
    }


@router.post("/send-command")
async def send_command(command: str = Form(...)):
    response = assistant.process_input_command(command)
    return {"response": response}


@router.post("/reset-memory")
async def reset_memory():
    assistant.reset_memory()
    return {"message": "Memory reset successfully."}


@router.get("/view-db/{table_name}")
def view_db(table_name: str):
    allowed_tables = [
        "camera_vision",
        "users",
        "operation_sequence",
        "unified_instructions",
        "sort_order",
        "sequence_library",
    ]
    if table_name not in allowed_tables:
        return {"error": "Table not allowed."}

    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute(f"SELECT * FROM {table_name} LIMIT 50")
    rows = cursor.fetchall()
    columns = [desc[0] for desc in cursor.description]

    # Ensure every row is a list of stringifiable items
    serialized_rows = [
        [str(cell) if not isinstance(cell, (int, float)) else cell for cell in row]
        for row in rows
    ]

    return {"columns": columns, "rows": serialized_rows, "count": len(serialized_rows)}


@router.post("/set-model")
async def set_model(request: Request):
    payload = await request.json()
    model = payload.get("model")
    if not model:
        return {"error": "No model specified"}

    assistant.set_llm_model(model)
    return {"message": f"âœ… Model set to {model}"}


from fastapi import UploadFile, File, Form


@router.post("/register-user")
async def register_user(
    first_name: str = Form(...),
    last_name: str = Form(...),
    liu_id: str = Form(...),
    email: str = Form(...),
    face: UploadFile = File(...),
    voice: UploadFile = File(...),
):
    # Save uploaded files
    face_path = f"temp_images/{liu_id}_face.png"
    voice_path = f"temp_audio/{liu_id}_voice.webm"
    with open(face_path, "wb") as f:
        f.write(await face.read())
    with open(voice_path, "wb") as f:
        f.write(await voice.read())

    # Process face registration
    try:
        face_auth = FaceAuthSystem()
        # You need to implement a method that takes an image path and user info
        face_success = face_auth.register_user_from_image(
            face_path, first_name, last_name, liu_id, email
        )
        # Process voice registration
        voice_auth = VoiceAuth()
        voice_success = voice_auth.register_voice_for_user_from_file(
            first_name, last_name, liu_id, voice_path
        )
        if face_success and voice_success:
            return {
                "message": f"âœ… User {first_name} registered (face/voice captured)."
            }
        else:
            return {"message": f"âŒ Registration failed (face or voice not captured)."}
    except Exception as e:
        return {"message": f"âŒ Registration failed: {str(e)}"}


@router.post("/process-voice")
async def process_voice(audio: UploadFile = File(None)):
    if audio:
        audio_path = f"temp_audio/{audio.filename}"
        with open(audio_path, "wb") as f:
            f.write(await audio.read())
        command_text, lang = assistant.vp.transcribe_audio(audio_path)
        return {"transcription": command_text, "lang": lang}
    else:
        result = assistant.vp.capture_voice()
        if not result:
            return {"transcription": ""}
        command_text, lang = result
        return {"transcription": command_text, "lang": lang}


@router.post("/llm-response")
async def llm_response(request: Request):
    data = await request.json()
    command_text = data.get("command_text")
    lang = data.get("lang", "en")
    response = assistant.process_input_command(command_text, lang)
    return {"response": response}
############################
# mini_project/web_interface/main.py
"""
This module initializes and configures the FastAPI application for the Robot Assistant web interface.
- Sets up the FastAPI app with a custom title.
- Mounts the static files directory for serving CSS, JS, and images.
- Configures Jinja2 templates for rendering HTML pages.
- Includes additional API and web routes from the `mini_project.web_interface.routes` module.
"""

from fastapi import FastAPI
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates

from mini_project.web_interface.routes import router

app = FastAPI(title="Robot Assistant")

# Mount static files and templates (for HTML + JS)
app.mount(
    "/static", StaticFiles(directory="mini_project/web_interface/static"), name="static"
)
templates = Jinja2Templates(directory="mini_project/web_interface/templates")

# Add route definitions
app.include_router(router)
####################################
# run.py


"""
This script launches a FastAPI web application using Uvicorn and exposes it to the internet via an ngrok tunnel.
Functions:
    start_ngrok(port=8000, timeout=10):
        Starts an ngrok tunnel on the specified port and waits for the public URL to become available.
        Args:
            port (int): The local port to expose via ngrok. Defaults to 8000.
            timeout (int): Time in seconds to wait for ngrok to provide a public URL. Defaults to 10.
        Returns:
            subprocess.Popen: The process running ngrok.
Main Execution:
    - Determines the port to run the server on (default 8000 or from command line argument).
    - Starts an ngrok tunnel for the specified port.
    - Runs the FastAPI app located at 'mini_project.web_interface.main:app' using Uvicorn with reload enabled.
    - On shutdown (including KeyboardInterrupt), terminates the ngrok process.
"""
import uvicorn
import subprocess
import time
import requests
import sys
import os

ngrok_path = os.path.join(os.path.dirname(__file__), "ngrok.exe")


def start_ngrok(port=8000, timeout=10):
    ngrok = subprocess.Popen(
        [ngrok_path, "http", str(port)],
        stdout=subprocess.DEVNULL,
        stderr=subprocess.STDOUT,
    )
    # Poll ngrok API for the public URL
    start = time.time()
    public_url = None
    while time.time() - start < timeout:
        try:
            tunnels = requests.get("http://127.0.0.1:4040/api/tunnels").json()
            if tunnels["tunnels"]:
                public_url = tunnels["tunnels"][0]["public_url"]
                print(f"ðŸŸ¢ ngrok tunnel running at: {public_url}")
                break
        except Exception:
            pass
        time.sleep(0.5)
    if not public_url:
        print("ðŸŸ¡ Could not get ngrok URL after waiting.")
    return ngrok


if __name__ == "__main__":
    port = int(sys.argv[1]) if len(sys.argv) > 1 else 8000
    print(f"Local: http://localhost:{port}")
    ngrok_proc = start_ngrok(port)

    try:
        uvicorn.run(
            "mini_project.web_interface.main:app",
            host="0.0.0.0",
            port=port,
            reload=True,
        )
    except KeyboardInterrupt:
        print("\nShutting down...")
    finally:
        ngrok_proc.terminate()
#####################
# mini_project/config/app_config.py
""" app_config.py
This module provides configuration settings and utility functions for the mini_project application.
It defines paths, constants, and parameters for database connections, face and voice recognition,
gesture recognition, logging, and other application features.
Main Features:
--------------
- Defines BASE_DIR and various asset/data paths used throughout the application.
- Provides configuration for both SQLite and PostgreSQL database connections.
- Sets parameters for face recognition (e.g., encoding limits, thresholds, capture settings).
- Sets parameters for voice recognition and processing (e.g., Whisper model, TTS settings, thresholds).
- Defines gesture recognition settings (e.g., confidence thresholds, hand tracking).
- Provides regular expressions for validating user IDs and emails.
- Contains LLM (Large Language Model) settings for command unification.
- Includes utility functions for:
    - Validating and creating required directories and files at runtime.
    - Setting up and loading logging configurations from YAML or defaults.
Usage:
------
Import this module to access configuration constants and utility functions.
Call `validate_paths()` to ensure all required directories exist.
Call `setup_logging()` or rely on automatic logging configuration on import.
Example:
--------
    app_config.validate_paths()
    logger = app_config.logger
    logger.info("Application started.")

 """

import logging.config
import os
import tempfile
from pathlib import Path

import yaml

# Define the base directory
BASE_DIR = Path(__file__).resolve().parent.parent  # mini_project_repo/ directory path

# Use network share or local database file: be aware of potential issues with file locking on a network share.
# DB_PATH = Path(r"\\ad.liu.se\coop\i\industrialrobotsetup\sequences.db")
DB_PATH = BASE_DIR / "assets" / "db_data" / "sequences.db"

# Use postgreSQL database.
# DB_URL = "dbname=sequences_db user=oscar password=oscik559 host=localhost"
DB_URL = "postgresql://oscar:oscik559@localhost:5432/sequences_db"

DB_BACKUP_PATH = BASE_DIR / "assets" / "db_backups"
PROFILE_BACKUP_PATH = BASE_DIR / "assets" / "db_user_backups"

# Face recognition utilities
FACIAL_DATA_PATH = BASE_DIR / "assets" / "face_encodings"
FACE_CAPTURE_PATH = BASE_DIR / "assets" / "face_capture"
IDENTIFICATION_FRAMES = (
    2  # Constant to control how many frames are used for identification averaging.
)
TIMEDELAY = 0.5  # Time delay between frames for face detection

# Face recognition parameters
FACE_MATCH_THRESHOLD = 0.6  # Threshold for face matching
MAX_ENCODINGS_PER_USER = 5  # Maximum number of encodings per user
AUTO_CAPTURE_FRAME_COUNT = (
    5  # Number of consecutive frames with detected face for auto-capture
)

# Voice recognition parameters
VOICE_DATA_PATH = BASE_DIR / "assets" / "voice_embeddings"
VOICE_CAPTURE_PATH = BASE_DIR / "assets" / "voice_capture"
TRANSCRIPTION_SENTENCE = "Artificial intelligence enables machines to recognize patterns, process language, and make decisions."
MAX_RETRIES = 3
VOICE_MATCH_THRESHOLD = 0.7  # Cosine similarity threshold for identification.

TEMP_AUDIO_PATH = BASE_DIR / "assets" / "temp_audio"

# Camera vision utilities
CAMERA_DATA_PATH = BASE_DIR / "assets" / "camera_data"


# Email and ID validation patterns
LIU_ID_PATTERN = r"^[a-z]{3,5}\d{3}$"  # Example: oscik559
EMAIL_PATTERN = r"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+$"

# === Voice Processing Configurations ===
VOICE_PROCESSING_CONFIG = {
    "recording": {
        "temp_audio_path": str(
            TEMP_AUDIO_PATH / "voice_recording.wav"
        ),  # Path to save recorded audio
        "sampling_rate": 16000,  # Audio sample rate (Hz)
        "max_duration": 60,  # Maximum recording duration (seconds)
        "initial_silence_duration": 15,  # Silence allowed before speech starts (seconds)
        "post_speech_silence_duration": 2,  # Silence allowed after speech ends (seconds)
        "calibration_duration": 0.5,  # Duration for ambient noise calibration (seconds)
        "amplitude_margin": 100,  # Margin above noise floor for speech detection
        "frame_duration": 0.03,  # Duration of each audio frame (seconds)
    },
    "whisper": {
        "model": "large-v3",  # Whisper model to use
        "device": "cuda",  # Device for Whisper (e.g., "cuda" or "cpu")
        "compute_type": "float16",  # Compute type for Whisper
    },
    "database": {
        "db_path": str(DB_PATH),  # Path to the SQLite database
    },
}
MAX_TRANSCRIPTION_RETRIES = 1
MIN_DURATION_SEC = 1.5
VOICE_TTS_SETTINGS = {
    "speed": 165,
    "use_gtts": True,
    "ping_sound_path": str(BASE_DIR / "assets" / "sound_effects" / "ping.wav"),
    "ding_sound_path": str(BASE_DIR / "assets" / "sound_effects" / "ding.wav"),
    "voice_index": 2,  # 0 = Hazel, 1 = David, 2 = Zira (example for Windows)
}
import json
from config import app_config

NOISE_CACHE_PATH = BASE_DIR / "assets" / "config_cache" / "noise_floor_cache.json"
CHAT_MEMORY_FOLDER = BASE_DIR / "assets" / "chat_memory"
WAKEWORD_PATH = BASE_DIR / "assets" / "robot_wakewords" / "hey_yummy.ppn"


# LLM Settings


# === Gesture Recognition Settings ===
MIN_DETECTION_CONFIDENCE = 0.7
MIN_TRACKING_CONFIDENCE = 0.5
MAX_NUM_HANDS = 2
FRAME_SKIP = 2

# === synchronizer LLM Settings ===
BATCH_SIZE = int(os.getenv("BATCH_SIZE", 1000))
LLM_MAX_RETRIES = int(os.getenv("LLM_MAX_RETRIES", 3))
LLM_MODEL = os.getenv("LLM_MODEL", "mistral:latest")

UNIFY_PROMPT_TEMPLATE = (
    "Role: Command Unifier. Combine voice commands (primary) with gesture cues (supplementary).\n"
    "Rules:\n"
    "1. Preserve ALL details from the voice command.\n"
    "2. Integrate gestures ONLY if they add context (e.g., direction, emphasis).\n"
    "3. NEVER omit voice content unless the gesture explicitly contradicts it.\n"
    "4. Output format: Plain text, no markdown or JSON.\n\n"
    "Examples:\n"
    "- Voice: 'Turn right', Gesture: 'Pointing up' â†’ 'Turn right upward'\n"
    "- Voice: 'Stop', Gesture: '' â†’ 'Stop'\n\n"
    "Voice Instruction: {voice_text}\n"
    "Gesture Instruction: {gesture_text}\n"
    "Unified Command:"
)


# Function to validate paths at runtime
def validate_paths() -> None:
    """
    Validates that all defined paths exist or can be created.
    Raises an exception if any path is invalid.
    """
    paths_to_check = [
        FACIAL_DATA_PATH,
        FACE_CAPTURE_PATH,
        VOICE_DATA_PATH,
        TEMP_AUDIO_PATH,
        CAMERA_DATA_PATH,
        DB_PATH.parent,
        NOISE_CACHE_PATH,
        DB_BACKUP_PATH,
        PROFILE_BACKUP_PATH,
        CHAT_MEMORY_FOLDER,
        WAKEWORD_PATH,
    ]

    for path in paths_to_check:
        if not path.exists():
            try:
                path.mkdir(parents=True, exist_ok=True)
                logger.info(f"Created missing directory: {path}")
            except OSError as e:
                logger.error(f"Failed to create directory {path}: {e}", exc_info=True)
                raise RuntimeError(f"Failed to create directory {path}: {e}")


def setup_logging(level: int = logging.INFO) -> None:
    logging.basicConfig(
        level=level,
        # format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
        format="[%(levelname)s] %(name)s: %(message)s",
        # datefmt="%Y-%m-%d %H:%M:%S",
        datefmt="%H:%M:%S",
    )


def load_logging_config():
    config_path = Path(__file__).parent / "logging_config.yaml"
    if config_path.exists():
        with open(config_path, "r") as f:
            config = yaml.safe_load(f)
        logging.config.dictConfig(config)
    else:
        setup_logging()


# Set up logging
load_logging_config()
logger = logging.getLogger("App_CONFIG")

# Automatically validate paths when this module is imported
# validate_paths()


if __name__ == "__main__":
    setup_logging()
    logger.info("ðŸŸ¡ Checking paths...")
    try:
        validate_paths()
        logger.info(f"âœ… All paths are valid.")
    except RuntimeError as e:
        logger.error(f"Path validation failed: {e}")
##############################

# mini_project/config/constants.py
"""
This module defines constant values used throughout the mini_project application.
Constants:
    VOICE_TABLE (str): Name of the database table for voice instructions.
    GESTURE_TABLE (str): Name of the database table for gesture instructions.
    PROCESSED_COL (str): Name of the column indicating processed instructions.
    UNIFIED_TABLE (str): Name of the unified instructions table.
    WHISPER_LANGUAGE_NAMES (dict): Maps language codes to their English names for use with the Whisper voice processor.
    WAKE_RESPONSES (list): List of possible responses when the system is activated or addressed.
    GENERAL_TRIGGERS (set): Set of keywords or phrases that trigger general-purpose actions or queries.
    TASK_VERBS (set): Set of verbs related to task execution, such as sorting or moving.
    QUESTION_WORDS (set): Set of words and phrases used to identify questions.
    CONFIRM_WORDS (set): Set of words and phrases used to confirm actions.
    CANCEL_WORDS (set): Set of words and phrases used to cancel or stop actions.
    TRIGGER_WORDS (set): Set of words and phrases that trigger specific system actions.
"""


# Synchronizer Constants
VOICE_TABLE = "voice_instructions"
GESTURE_TABLE = "gesture_instructions"
PROCESSED_COL = "processed"
UNIFIED_TABLE = "unified_instructions"

# Voice Processor Constants
WHISPER_LANGUAGE_NAMES = {
    "en": "english",
    "zh": "chinese",
    "de": "german",
    "es": "spanish",
    "ru": "russian",
    "ko": "korean",
    "fr": "french",
    "ja": "japanese",
    "pt": "portuguese",
    "tr": "turkish",
    "pl": "polish",
    "ca": "catalan",
    "nl": "dutch",
    "ar": "arabic",
    "sv": "swedish",
    "it": "italian",
    "id": "indonesian",
    "hi": "hindi",
    "fi": "finnish",
    "vi": "vietnamese",
    "he": "hebrew",
    "uk": "ukrainian",
    "el": "greek",
    "ms": "malay",
    "cs": "czech",
    "ro": "romanian",
    "da": "danish",
    "hu": "hungarian",
    "ta": "tamil",
    "no": "norwegian",
    "th": "thai",
    "ur": "urdu",
    "hr": "croatian",
    "bg": "bulgarian",
    "lt": "lithuanian",
    "la": "latin",
    "mi": "maori",
    "ml": "malayalam",
    "cy": "welsh",
    "sk": "slovak",
    "te": "telugu",
    "fa": "persian",
    "lv": "latvian",
    "bn": "bengali",
    "sr": "serbian",
    "az": "azerbaijani",
    "sl": "slovenian",
    "kn": "kannada",
    "et": "estonian",
    "mk": "macedonian",
    "br": "breton",
    "eu": "basque",
    "is": "icelandic",
    "hy": "armenian",
    "ne": "nepali",
    "mn": "mongolian",
    "bs": "bosnian",
    "kk": "kazakh",
    "sq": "albanian",
    "sw": "swahili",
    "gl": "galician",
    "mr": "marathi",
    "pa": "punjabi",
    "si": "sinhala",
    "km": "khmer",
    "sn": "shona",
    "yo": "yoruba",
    "so": "somali",
    "af": "afrikaans",
    "oc": "occitan",
    "ka": "georgian",
    "be": "belarusian",
    "tg": "tajik",
    "sd": "sindhi",
    "gu": "gujarati",
    "am": "amharic",
    "yi": "yiddish",
    "lo": "lao",
    "uz": "uzbek",
    "fo": "faroese",
    "ht": "haitian creole",
    "ps": "pashto",
    "tk": "turkmen",
    "nn": "nynorsk",
    "mt": "maltese",
    "sa": "sanskrit",
    "lb": "luxembourgish",
    "my": "myanmar",
    "bo": "tibetan",
    "tl": "tagalog",
    "mg": "malagasy",
    "as": "assamese",
    "tt": "tatar",
    "haw": "hawaiian",
    "ln": "lingala",
    "ha": "hausa",
    "ba": "bashkir",
    "jw": "javanese",
    "su": "sundanese",
    "yue": "cantonese",
}


WAKE_RESPONSES = [
    "yes?",
    "I'm listening",
    "what's up?",
    "go ahead.",
    "at your service.",
    "hello?",
    "I'm here!",
    "you called?",
    "what do you want?",
    "I'm listening.",
    "hi?",
    "what is it?",
]
GENERAL_TRIGGERS = {
    "weather",
    "who is",
    "linkÃ¶ping",
    "university",
    "say something",
    "remind",
    "recap",
    "explain",
    "lab",
    "appreciate",
    "motivate",
    "how are we doing",
    "tell us about",
    "introduce",
    "location",
    "where is",
    "project",
    "working on",
    "colleague",
    "summary",
}
TASK_VERBS = {
    "sort",
    "move",
    "place",
    "assemble",
    "pick",
    "drop",
    "grab",
    "stack",
    "push",
    "pull",
}
QUESTION_WORDS = {
    "what",
    "where",
    "which",
    "who",
    "how many",
    "is there",
    "are there",
}
CONFIRM_WORDS = {
    "yes",
    "sure",
    "okay",
    "go ahead",
    "absolutely",
    "yep",
    "definitely",
    "please do",
}
CANCEL_WORDS = {"no", "cancel", "not now", "stop", "never mind", "don't"}
TRIGGER_WORDS = {
    "detect",
    "refresh",
    "capture",
    "scan",
    "trigger",
}
#####################
# .env
# DATABASE_URL=postgresql://<username>:<password>@<host>:<port>/<database>
# DATABASE_URL=postgresql://oscar:oscik559@localhost:5432/sequences_db
DATABASE_URL=postgresql://oscar:oscik559@localhost:5432/sequences_test_db
DEFAULT_DATABASE_URL=postgresql://postgres:oscik559@localhost:5432/postgres
DEBUG=1
LOG_LEVEL=DEBUG


PICOVOICE_ACCESS_KEY = "E0O2AD01eT6cJ83n1yYf5bekfdIOEGUky9q6APkwdx9enDaMLZQtLw=="

########################
# mini_project/config/logging_config.yaml


version: 1
disable_existing_loggers: False

formatters:
  standard:
    format: "[%(asctime)s] %(levelname)s - %(name)s - %(message)s"
    datefmt: "%Y-%m-%d %H:%M:%S"
    # datefmt: "%H:%M:%S"

handlers:
  console:
    class: logging.StreamHandler
    level: DEBUG
    formatter: standard
    stream: ext://sys.stdout

loggers:
  mini_project:
    level: DEBUG
    handlers: [console]
    propagate: no

root:
  level: INFO
  handlers: [console]
##########################